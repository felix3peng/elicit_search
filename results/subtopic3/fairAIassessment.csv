Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Fairness Assessment for Artificial Intelligence in Financial Industry,Fairness evaluation is a key component of AI governance.,Search,2019,7,"Yukun  Zhang, Longsheng  Zhou",ArXiv,,,,https://semanticscholar.org/paper/9b0f1ec29596a1e5d41366235322f7d1d68be9e4,,"Artificial Intelligence (AI) is an important driving force for the development and transformation of the financial industry. However, with the fast-evolving AI technology and application, unintentional bias, insufficient model validation, immature contingency plan and other underestimated threats may expose the company to operational and reputational risks. In this paper, we focus on fairness evaluation, one of the key components of AI Governance, through a quantitative lens. Statistical methods are reviewed for imbalanced data treatment and bias mitigation. These methods and fairness evaluation metrics are then applied to a credit card default payment example.",,
Fairlearn: A toolkit for assessing and improving fairness in AI,Fairlearn is a toolkit that helps assess and improve the fairness of AI systems.,Search,2020,48,"Sarah  Bird, Miro  Dudík, Richard  Edgar, Brandon  Horn, Roman  Lutz, Vanessa  Milan, Mehrnoosh  Sameki, Hanna  Wallach, Kathleen  Walker",,,,,https://semanticscholar.org/paper/5894d57ea49bd5c136ebefb1e6c3986555908ea0,,"We introduce Fairlearn, an open source toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems. Fairlearn has two components: an interactive visualization dashboard and unfairness mitigation algorithms. These components are designed to help with navigating trade-offs between fairness and model performance. We emphasize that prioritizing fairness in AI systems is a sociotechnical challenge. Because there are many complex sources of unfairness—some societal and some technical—it is not possible to fully “debias” a system or to guarantee fairness; the goal is to mitigate fairness-related harms as much as possible. As Fairlearn grows to include additional fairness metrics, unfairness mitigation algorithms, and visualization capabilities, we hope that it will be shaped by a diverse community of stakeholders, ranging from data scientists, developers, and business decision makers to the people whose lives may be affected by the predictions of AI systems.",,
Assessing the Fairness of AI Recruitment systems,There is limited scientific research on the fairness of AI recruitment systems.,Search,2019,5,Akhil  Krishnakumar,,,,,https://semanticscholar.org/paper/eb58d3ef9847a73015b229dc7c4e414ded1a9c9e,,"Businesses have leveraged Artificial Intelligence (AI) into many of their operational activities such as marketing, sales, and finance for its speed and cost-effectiveness. Lately, AI has also found applications in organizational recruitment processes. Unlike the conventional rule-based systems, present-day AI systems learn from data patterns—supported by the growing volumes of (big) data and increasing computing capacity—and make decisions independently without any human interventions. Thus, the perception that AI is fact-oriented and unbiased has led to this change in organizational recruitment practices. Though recent studies have shown that AI decisions could be unfair, scientific research on the fairness of AI recruitment systems is limited. This research fills this gap by designing a conceptual model to assist top-level HR managers in assessing the fairness of AI recruitment tools while drawing from information systems and responsible innovation literature. Guided by Design Science Research (DSR), the development of the model entailed three cycles of research, i.e., relevance cycle (which focused on design environment), rigor cycle (which focused on the existing knowledge base), and design cycle (which focused on development and evaluation). The design environment was explored by reviewing the literature on fairness in recruitment and algorithmic biases. Understanding both the recruitment fairness and potential causes of unfairness in AI helped to define the goal of the conceptual model. The design cycle was informed by the design principles for responsible AI, namely Accountability, Responsibility, and Transparency (ART), and General Data Protection Regulation (GDPR). The model presents seven dimensions which translate the principles to design requirements to assess the fairness of AI recruitment system. They are: (1)Justification; (2)Explanation; (3)Anticipation; (4)Reflexiveness; (5)Inclusion; (6)Responsiveness; and (7)Auditablity. The model also ties these concepts with specific criteria of conventional recruitment fairness such as consistency, interpersonal fairness, job-relatedness, and statistical parity. Finally, the completeness of the model was evaluated by discussing its alignment with other frameworks that had similar objective and utility of the model was validated by collecting feedback from the intended users. This thesis project makes several scientific and practical contributions. The research discusses the potential risks of using AI in the context of HR recruitment systems thereby contributes to the limited literature available in this respect. By using the DSR methodology for building the assessment model, this research serves as a case for DSR methodology in designing a non-IS artifact. Furthermore, the thesis has unified scattered studies in recruitment justice to provide a comprehensive overview of the characteristics of a fair recruitment system. Building on the theoretical contributions, the study has developed an assessment model to assist top-level HR managers in assessing the fairness of an AI recruitment tool. Employing this assessment tool can have positive effects on a business organization and society by eradicating the unfairness or bias that AI recruitment tools can bring into the organization. It would also raise awareness regarding the risks of AI. Given that the GDPR (article 35) mandate organizations to take responsibility in assessing the impact while introducing automated processing in new contexts or purposes, the assessment model designed in this study supports these regulations.",,Review
Fairness in AI applications,AI fairness problems can be resolved by public consultation.,Search,2021,,Cameron  Shelley,2021 IEEE International Symposium on Technology and Society (ISTAS),,10.1109/istas52410.2021.9629140,https://doi.org/10.1109/istas52410.2021.9629140,https://semanticscholar.org/paper/048a7b203bfa22550ddd01af10f40a538a1bb27d,,"Applications of Artificial Intelligence (AI) that have broad, social impact for many people have recently increased greatly in number. They will continue to increase in ubiquity and impact for some time to come. In conjunction with this increase, many scholars have studied the nature of these impacts, including problems of fairness. Here, fairness refers to conflicts of interest between social groups that result from the configuration of these AI systems. One focus of research has been to define these fairness problems and to quantify them in a way that lends itself to calculation of fair outcomes. The purpose of this presentation is to show that this issue of fairness in AI is consistent with fairness problems posed by technological design in general and that addressing these problems goes beyond what can be readily quantified and calculated. For example, many such problems may be best resolved by forms of public consultation. This point is clarified by presenting an analytical tool, the Fairness Impact Assessment, and examples from AI and elsewhere.",,
Toward fairness in AI for people with disabilities SBG@a research roadmap,"AI technologies may not work properly for people with disabilities, or may actively discriminate against them.",Search,2020,39,"Anhong  Guo, Ece  Kamar, Jennifer Wortman Vaughan, Hanna  Wallach, Meredith Ringel Morris",ACM SIGACCESS Access. Comput.,,10.1145/3386296.3386298,https://doi.org/10.1145/3386296.3386298,https://semanticscholar.org/paper/e74eb6147977d94ac5db4ff779e6d4e53feeed75,,"AI technologies have the potential to dramatically impact the lives of people with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for many state-of-the-art AI systems, such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing, or language prediction algorithms that can augment communication for people with speech or cognitive disabilities. However, widely deployed AI systems may not work properly for PWD, or worse, may actively discriminate against them. These considerations regarding fairness in AI for PWD have thus far received little attention. In this position paper, we identify potential areas of concern regarding how several AI technology categories may impact particular disability constituencies if care is not taken in their design, development, and testing. We intend for this risk assessment of how various classes of AI might interact with various classes of disability to provide a roadmap for future research that is needed to gather data, test these hypotheses, and build more inclusive algorithms.",,
AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias,Fairness is an increasingly important concern as machine learning models are used to support decision making.,Search,2019,113,"Rachel K. E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Jacquelyn A. Martino, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",IBM J. Res. Dev.,,10.1147/jrd.2019.2942287,https://doi.org/10.1147/jrd.2019.2942287,https://semanticscholar.org/paper/333671a5fbbf726f8819138f3670524ec0405726,,"Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This article introduces a new open-source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license (

https://github.com/ibm/aif360

). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms for use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience that provides a gentle introduction to the concepts and capabilities for line-of-business users, researchers, and developers to extend the toolkit with their new algorithms and improvements and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.",,
"AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias",AI Fairness 360 is a toolkit to facilitate the transition of fairness research algorithms to use in an industrial setting.,Search,2018,316,"Rachel K. E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Jacquelyn  Martino, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",ArXiv,,,,https://semanticscholar.org/paper/c8541b1dc813f3a638d7acc79e5f972e77f3c5a7,,"Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license {this https URL). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms.

The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (this https URL) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.",,
FAIR AI: A Conceptual Framework for Democratisation of 21st Century AI,The lack of interpretability is a main inhibitor of broader use of 21st century AI.,Search,2021,,Saman  Halgamuge,"2021 International Conference on Instrumentation, Control, and Automation (ICA)",,10.1109/ICA52848.2021.9625672,https://doi.org/10.1109/ICA52848.2021.9625672,https://semanticscholar.org/paper/cae45a30452713cacad1e5ec04adfb4a065f6a8e,,"Popular models of AI have two significant deficiencies: 1) they are mostly manually designed using the experience of AI-experts 2) they lack human interpretability, i.e., users cannot make sense of the functionality of neural network architectures either semantically/linguistically or mathematically. This lack of interpretability is a main inhibitor of broader use of 21st century AI, e.g., Deep Neural Networks (DNN). The dependence on AI experts to create AI hinders the democratisation of AI and therefore the accessibility to AI. Addressing these deficiencies would provide answers to some of the valid questions about traceability, accountability and the ability to integrate existing knowledge (scientific or linguistically articulated human experience) into the model. This keynote abstract addresses these two significant deficiencies that inhibit the democratisation of AI by developing new methods that can automatically create interpretable neural network models without the help of AI-experts. The proposed cross-fertilised innovation will have a profound impact on the society through the increased accessibility and trustworthiness of AI beneficial to almost all areas of sciences, engineering, and humanities.",,
Fairness Score and Process Standardization: Framework for Fairness Certification in Artificial Intelligence Systems,A Fairness Score and a Standard Operating Procedure (SOP) for issuing Fairness Certification for AI systems would improve the trustworthiness of AI systems.,Search,2022,,"Avinash  Agarwal, Harsh  Agarwal, Nihaarika  Agarwal",ArXiv,,,,https://semanticscholar.org/paper/64c2fa6b3c161fe2a5ae20f03ff77c3250027e10,,"Decisions made by various Artificial Intelligence (AI) systems greatly influence our day-to-day lives. With the increasing use of AI systems, it becomes crucial to know that they are fair, identify the underlying biases in their decision-making, and create a standardized framework to ascertain their fairness. In this paper, we propose a novel Fairness Score to measure the fairness of a data-driven AI system and a Standard Operating Procedure (SOP) for issuing Fairness Certification for such systems. Fairness Score and audit process standardization will ensure quality, reduce ambiguity, enable comparison and improve the trustworthiness of the AI systems. It will also provide a framework to operationalise the concept of fairness and facilitate the commercial deployment of such systems. Furthermore, a Fairness Certificate issued by a designated third-party auditing agency following the standardized process would boost the conviction of the organizations in the AI systems that they intend to deploy. The Bias Index proposed in this paper also reveals comparative bias amongst the various protected attributes within the dataset. To substantiate the proposed framework, we iteratively train a model on biased and unbiased data using multiple datasets and check that the Fairness Score and the proposed process correctly identify the biases and judge the fairness.",,
Casual Conversations: A dataset for measuring fairness in AI,"A key feature of the ""fairness"" dataset is that it contains videos of various adults in various age, gender and apparent skin tone groups.",Search,2021,3,"Caner  Hazirbas, Joanna  Bitton, Brian  Dolhansky, Jacqueline  Pan, Albert  Gordo, Cristian  Canton-Ferrer",2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW53098.2021.00258,https://doi.org/10.1109/CVPRW53098.2021.00258,https://semanticscholar.org/paper/b894f25d1887896ca31061f28de70b4d4020e00b,,"This paper introduces a novel ""fairness"" dataset to measure the robustness of AI models to a diverse set of age, genders, apparent skin tones and ambient lighting conditions. Our dataset is composed of 3,011 subjects and contains over 45,000 videos, with an average of 15 videos per person. The videos were recorded in multiple U.S. states with a diverse set of adults in various age, gender and apparent skin tone groups. A key feature is that each subject agreed to participate for their likenesses to be used. Additionally, our age and gender annotations are provided by the subjects themselves. A group of trained annotators labeled the subjects’ apparent skin tone using the Fitzpatrick skin type scale [6]. Moreover, annotations for videos recorded in low ambient lighting are also provided. As an application to measure robustness of predictions across certain attributes, we evaluate the state-of-the-art apparent age and gender classification methods. Our experiments provides a through analysis on these models in terms of fair treatment of people from various backgrounds.",,
Design Methods for Artificial Intelligence Fairness and Transparency,"Fairness and transparency in artificial intelligence continue to become more prevalent as topics for research, design and development.",Search,2021,,"Simone  Stumpf, Lorenzo  Strappelli, Subeida  Ahmed, Yuri  Nakao, Aisha  Naseer, Giulia Del Gamba, Daniele  Regoli",IUI Workshops,,,,https://semanticscholar.org/paper/baf8cb94908738c3b2d040a6b44d812e5ca80396,,"Fairness and transparency in artificial intelligence (AI) continue to become more prevalent as topics for research, design and development. General principles and guidelines for designing ethical and responsible AI systems have been proposed, yet there is a lack of design methods for these kinds of systems. In this paper, we present CoFAIR, a novel method to design user interfaces for exploring fairness, consisting of series of co-design workshops, and wider evaluation. This method can be readily applied in practice by researchers, designers and developers to create responsible and ethical AI systems.",,
Requirements for AI Assessment,"The development of AI systems represents a significant investment and to realize the promise of that investment, performance assessment is necessary.",Search,2021,,"Gary Klein Mohammad Jalaeian Macrocognition, Robert R. Hoffman",,,,,https://semanticscholar.org/paper/5cfa64c13d88cf571a2a395862664b4cd7948207,,"The development of AI systems represents a significant investment. But to realize the promise of that investment, performance assessment is necessary. Empirical evaluation of Human-AI work systems must adduce convincing empirical evidence that the work method and its AI technology are learnable, usable, and useful. The theme to this Report is the notion that AI assessment must be effective but must also be efficient. Bench testing of a prototype of an AI system cannot require extensive series of experiments with complex designs. Thus, the empirical requirements that are presented in this Report involve escaping some of the constraints that are imposed in traditional laboratory research. Also, there is a recognition of new constraints that are unique to AI evaluation contexts. Empirical requirements are presented covering study design, research methods, statistical analyses, and online experimentation. The 15 requirements presented in this Report should be applicable to all research intended to evaluate the effectivity of AI systems.",,
Who Decides if AI is Fair? The Labels Problem in Algorithmic Auditing,The fidelity of ground truth data can lead to spurious differences in performance of ASRs between urban and rural populations.,Search,2021,,"Abhilash  Mishra, Yash  Gorana",ArXiv,,,,https://semanticscholar.org/paper/5875c09dfd840ff8cda6b4b022df89f188b1355c,,"Labelled ""ground truth"" datasets are routinely used to evaluate and audit AI algorithms applied in high-stakes settings. However, there do not exist widely accepted benchmarks for the quality of labels in these datasets. We provide empirical evidence that quality of labels can significantly distort the results of algorithmic audits in real-world settings. Using data annotators typically hired by AI firms in India, we show that fidelity of the ground truth data can lead to spurious differences in performance of ASRs between urban and rural populations. After a rigorous, albeit expensive, label cleaning process, these disparities between groups disappear. Our findings highlight how trade-offs between label quality and data annotation costs can complicate algorithmic audits in practice. They also emphasize the need for development of consensus-driven, widely accepted benchmarks for label quality.",,
Cognitive and Emotional Response to Fairness in AI – A Systematic Review,AI applications can disadvantage certain groups of individuals.,Search,2019,3,"Janine  Baleis, Birte  Keller, Christopher  Starke, Frank  Marcinkowski",,,,,https://semanticscholar.org/paper/b5a92d0fda53c4720f00c3d75682f665c6ceb6a4,,"Artificial intelligence is increasingly used to make decisions that can have a significant impact on people's lives. These decisions can disadvantage certain groups of individuals. A central question that follows is the feasibility of justice in AI applications. Therefore, it should be considered which demands such applications have to meet and where the transfer of social order to algorithmic contexts still needs to be overhauled. Previous research efforts in the context of discrimination come from different disciplines and shed light on problems from specific perspectives on the basis of various definitions. An interdisciplinary approach to this topic is still lacking, which is why it is considered sensible to systematically summarise research findings across disciplines in order to find parallels and combine common fairness requirements. This endeavour is the aim of this paper. As a result of the systematic review, it can be stated that the individual perception of fairness in AI applications is strongly context-dependent. In addition, transparency, trust and individual moral concepts demonstrably have an influence on the individual perception of fairness in AI applications. Within the interdisciplinary scientific discourse, fairness is conceptualized by various definitions, which is why there is no consensus on a uniform definition of fairness in the scientific literature to date.",,Systematic Review
Towards Fairness Certification in Artificial Intelligence,Machine learning models incur the risk of amplifying prejudices and societal stereotypes by over associating protected attributes.,Search,2021,1,"Tatiana  Tommasi, Silvia  Bucci, Barbara  Caputo, Pietro  Asinari",ArXiv,,,,https://semanticscholar.org/paper/157246efaa0a667383cd78da0599231687368e0e,,"Thanks to the great progress of machine learning in the last years, several Artificial Intelligence (AI) techniques have been increasingly moving from the controlled research laboratory settings to our everyday life. The most simple examples are the spam filters that keep our email account in order, face detectors that help us when taking a portrait picture, online recommender systems that suggest which movie and clothing we might like, or interactive maps that navigate us towards our vacation home. Artificial intelligence is clearly supportive in many decision-making scenarios, but when it comes to sensitive areas such as health care, hiring policies, education, banking or justice, with major impact on individuals and society, it becomes crucial to establish guidelines on how to design, develop, deploy and monitor this technology. Indeed the decision rules elaborated by machine learning models are data-driven and there are multiple ways in which discriminatory biases can seep into data. Algorithms trained on those data incur the risk of amplifying prejudices and societal stereotypes by over associating protected attributes such as gender, ethnicity or disabilities with the prediction task.",,
Introduction to AI Fairness,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern.",Search,2020,2,"Yunfeng  Zhang, Rachel K. E. Bellamy, Moninder  Singh, Q. Vera Liao",CHI Extended Abstracts,,10.1145/3334480.3375059,https://doi.org/10.1145/3334480.3375059,https://semanticscholar.org/paper/d49f8d57240cd04dd66d91bf53c639497139ab3a,,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. Recently, the AI research community has proposed many methods to measure and mitigate unwanted biases, and developed open-source toolkits for developers to make fair AI. This course will cover the recent development in algorithmic fairness, including the many different definitions of fairness, their corresponding quantitative measurements, and ways to mitigate biases. This course is open to beginners and is designed for anyone interested in the topic of AI fairness.",,