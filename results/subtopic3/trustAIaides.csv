Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Zombies in the Loop? Humans Trust Untrustworthy AI-Advisors for Ethical Decisions,AI is overtrusted rather than distrusted.,Search,2021,,"Sebastian  Krugel, Andreas  Ostermaier, Matthias  Uhl",,,,,https://semanticscholar.org/paper/5fa1ca52f54ec170d42f899ff918ec3609895d42,,"Departing from the claim that AI needs to be trustworthy, we find that ethical advice from an AI-powered algorithm is trusted even when its users know nothing about its training data and when they learn information about it that warrants distrust. We conducted online experiments where the subjects took the role of decision-makers who received advice from an algorithm on how to deal with an ethical dilemma. We manipulated the information about the algorithm and studied its influence. Our findings suggest that AI is overtrusted rather than distrusted. We suggest digital literacy as a potential remedy to ensure the responsible use of AI.",,
Applied artificial intelligence and trust—The case of autonomous vehicles and medical assistance devices,Firms must foster trust in applied AI in order to increase acceptance of automated devices.,Search,2016,271,"Monika  Hengstler, Ellen  Enkel, Selina  Duelli",,,10.1016/J.TECHFORE.2015.12.014,https://doi.org/10.1016/J.TECHFORE.2015.12.014,https://semanticscholar.org/paper/203b30269d27e158cd26bd1f47c3207f52e6aec8,,"Automation with inherent artificial intelligence (AI) is increasingly emerging in diverse applications, for instance, autonomous vehicles and medical assistance devices. However, despite their growing use, there is still noticeable skepticism in society regarding these applications. Drawing an analogy from human social interaction, the concept of trust provides a valid foundation for describing the relationship between humans and automation. Accordingly, this paper explores how firms systematically foster trust regarding applied AI. Based on empirical analysis using nine case studies in the transportation and medical technology industries, our study illustrates the dichotomous constitution of trust in applied AI. Concretely, we emphasize the symbiosis of trust in the technology as well as in the innovating firm and its communication about the technology. In doing so, we provide tangible approaches to increase trust in the technology and illustrate the necessity of a democratic development process for applied AI.",,
Attachment and trust in artificial intelligence,People with a secure attachment style have a higher trust in AI.,Search,2021,21,"Omri  Gillath, Ting  Ai, Michael  Branicky, Shawn  Keshmiri, Rob  Davison, Ryan  Spaulding",Comput. Hum. Behav.,,10.1016/j.chb.2020.106607,https://doi.org/10.1016/j.chb.2020.106607,https://semanticscholar.org/paper/d7e73699049a73cf90c82a7b5aef589d84dbe794,,"Abstract Lack of trust is one of the main obstacles standing in the way of taking full advantage of the benefits artificial intelligence (AI) has to offer. Most research on trust in AI focuses on cognitive ways to boost trust. Here, instead, we focus on boosting trust in AI via affective means. Specifically, we tested and found associations between one's attachment style—an individual difference representing the way people feel, think, and behave in relationships—and trust in AI. In Study 1 we found that attachment anxiety predicted less trust. In Study 2, we found that enhancing attachment anxiety reduced trust, whereas enhancing attachment security increased trust in AI. In Study 3, we found that exposure to attachment security cues (but not positive affect cues) resulted in increased trust as compared with exposure to neutral cues. Overall, our findings demonstrate an association between attachment security and trust in AI, and support the ability to increase trust in AI via attachment security priming.",,
The relationship between trust in AI and trustworthy machine learning technologies,Trust can be impacted throughout the life cycle of AI-based systems.,Search,2020,50,"Ehsan  Toreini, Mhairi  Aitken, Kovila  Coopamootoo, Karen  Elliott, Carlos Gonzalez Zelaya, Aad van Moorsel",FAT*,,10.1145/3351095.3372834,https://doi.org/10.1145/3351095.3372834,https://semanticscholar.org/paper/bd4cf5a6987ef0f39b7e4e88d59b3a3365abcc70,http://arxiv.org/pdf/1912.00782,"To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.",,
Trust in Artificial Intelligence,"Artificial intelligence (AI) is being used for scheduling appointments, powering smart homes, recognizing people’s faces in photos, making health diagnoses, providing investment advice, and several other things.",Search,2018,14,Arathi  Sethumadhavan,Ergonomics in Design: The Quarterly of Human Factors Applications,,10.1177/1064804618818592,https://doi.org/10.1177/1064804618818592,https://semanticscholar.org/paper/94a43052c729c8e7e3f28c50c76e46d09ec95f62,https://journals.sagepub.com/doi/pdf/10.1177/1064804618818592,"Trust is fundamental to creating a lasting relationship with another human being. In our daily lives, we encounter several situations where we place trust on other humans such as bus drivers, colleagues, and even strangers. Trust in human-human teams is initially founded on the predictability of the trustee, and as the relationship between the trustor and the trustee progresses, dependability or integrity replaces predictability as the basis of trust (Hoff & Bashir, 2015). As artificial intelligence (AI) gets smarter and smarter, it is becoming an integral part of human lives. For example, AI is being used for scheduling appointments, powering smart homes, recognizing people’s faces in photos, making health diagnoses, providing investment advice, and several other things. As with a humanhuman team, trust is a necessary ingredient for human-AI partnerships. However, trust in human-human teams progresses in the reverse order from human-human teams (e.g., Hoff & Bashir, 2015). This is because humans initially assume that AI is near perfect. Therefore, in the initial stages, faith forms the essential constituent of trust, and as the number of exchanges between the human and the AI increases, faith is replaced by dependability and predictability. Trust in AI is considered a twodimensional construct comprising trust and distrust, where trust is associated with feelings of calmness and security and distrust involves fear and worry (Lyons, Stokes, Eschleman, Alarcon, & Barelka, 2011). Unarguably, trust is a complex social process with a variety of factors determining the extent to which humans trust AI agents (Hoff & Bashir, 2015; Lee & See, 2004). Specifically, in order to calibrate the right level of trust in AI, consider dispositional (i.e., user characteristics such as age, culture, gender, and personality), internal (e.g., user characteristics such as workload, mood, self-confidence, and working memory capacity), environmental (e.g., task difficulty, perceived risks and benefits, organizational setting), and learned factors (e.g., reputation of the AI, reliability, consistency, type and timing of errors made by the AI, and users’ experience with similar agents). Design factors (e.g., appearance, ease of use, communication style, and transparency of the AI) also affect perceptions of trust. For example, anthrophomorphizing is an effective way of establishing a long-term social bond between humans and AI agents, which is driven by the neurotransmitter and hormone, oxytocin (e.g., de Visser et al., 2017). Further, anthropomorphic agents also resist breakdown in trust compared to their counterpart non-anthropomorphic agents, presumably because anthropomorphic agents remind users of humans who are forgiven more easily for being imperfect in comparison to machine-like agents. Interestingly, there is also evidence that humans tend to disclose more information to AI therapists than human therapists. Transparency of the AI also helps to calibrate the right level of trust by enabling users to develop accurate mental models of the AI underpinnings (e.g., Balfe, Sharples, & Wilson, 2018). 818592 ERGXXX10.1177/1064804618818592ergonomics in designergonomics in design research-article2018",,
Limits of trust in medical AI,"AI systems can be relied on but cannot be trusted, which may cause a trust deficit in medical relationships.",Search,2020,10,Joshua James Hatherley,Journal of Medical Ethics,,10.1136/medethics-2019-105935,https://doi.org/10.1136/medethics-2019-105935,https://semanticscholar.org/paper/8fd2b272d824d5322928bc40b55c047f35717f8a,,"Artificial intelligence (AI) is expected to revolutionise the practice of medicine. Recent advancements in the field of deep learning have demonstrated success in variety of clinical tasks: detecting diabetic retinopathy from images, predicting hospital readmissions, aiding in the discovery of new drugs, etc. AI’s progress in medicine, however, has led to concerns regarding the potential effects of this technology on relationships of trust in clinical practice. In this paper, I will argue that there is merit to these concerns, since AI systems can be relied on, and are capable of reliability, but cannot be trusted, and are not capable of trustworthiness. Insofar as patients are required to rely on AI systems for their medical decision-making, there is potential for this to produce a deficit of trust in relationships in clinical practice.",,
Trust in Artificial Intelligence: Australian Insights,Artificial Intelligence (AI) is the cornerstone technology of the Fourth Industrial Revolution and is enabling rapid innovation with many potential benefits for Australian society.,Search,2020,5,"Steve  Lockey, Nicole  Gillespie, Caitlin  Curtis",,,10.14264/b32f129,https://doi.org/10.14264/b32f129,https://semanticscholar.org/paper/6c2714f81bc323851052920d240bc21b98e2a9f8,https://espace.library.uq.edu.au/view/UQ:b32f129/LockeyGillespieCurtis_Trust_In_AI.pdf,"Artificial Intelligence (AI) is the cornerstone technology of the Fourth Industrial Revolution and is enabling rapid innovation with many potential benefits for Australian society (e.g. enhanced healthcare diagnostics, transportation optimisation) and business (e.g. enhanced efficiency and competitiveness). The COVID-19 pandemic has accelerated the uptake of advanced technology, and investment in AI continues to grow exponentially.AI also poses considerable risks and challenges to society which raises concerns about whether AI systems are worthy of trust. These concerns have been fuelled by high profile cases of AI use that were biased, discriminatory, manipulative, unlawful, or violated privacy or other human rights. Without public confidence that AI is being developed and used in an ethical and trustworthy manner, it will not be trusted and its full potential will not be realised. To echo the sentiment of Dr Alan Finkel AO, Australia’s Chief Scientist, acceptance of AI rests on “the essential foundation of trust”. Are we capable of extending our trust to AI? This national survey is the first to take a deep dive into answering this question and understanding community trust and expectations in relation to AI. To do this, we surveyed a nationally representative sample of over 2,500 Australian citizens in June to July 2020. Our findings provide important and timely research insights into the public’s trust and attitudes towards AI and lay out a pathway for strengthening trust and acceptance of AI systems.Key findings include:              - Trust is central to the acceptance of AI, and is influenced by four key drivers;              - Australians have low trust in AI systems but generally ‘accept’ or ‘tolerate’ AI;              - Australians expect AI to be regulated and carefully managed;              - Australians expect organisations to uphold the principles of trustworthy AI;              - Australians feel comfortable with some but not all uses of AI at work;              - Australians want to know more about AI but currently have low awareness and understanding of AI and its uses.We draw out the implications of the findings for government, business and NGOs and provide a roadmap to enhancing public trust in AI highlighting three key actions:              - Live up to Australian’s expectations of trustworthy AI              - Strengthen the regulatory framework for governing AI              - Strengthen Australia’s AI literacy",,
Impacts on Trust of Healthcare AI,"Artificial Intelligence and robotics are rapidly moving into healthcare, playing key roles in specific medical functions.",Search,2018,24,"Emily  LaRosa, David  Danks",AIES,,10.1145/3278721.3278771,https://doi.org/10.1145/3278721.3278771,https://semanticscholar.org/paper/41326f8a19b7a20983d4aa092fb0318dc683404d,,"Artificial Intelligence and robotics are rapidly moving into healthcare, playing key roles in specific medical functions, including diagnosis and clinical treatment. Much of the focus in the technology development has been on human-machine interactions, leading to a host of related technology-centric questions. In this paper, we focus instead on the impact of these technologies on human-human interactions and relationships within the healthcare domain. In particular, we argue that trust plays a central role for relationships in the healthcare domain, and the introduction of healthcare AI can potentially have significant impacts on those relations of trust. We contend that healthcare AI systems ought to be treated as assistive technologies that go beyond the usual functions of medical devices. As a result, we need to rethink regulation of healthcare AI systems to ensure they advance relevant values. We propose three distinct guidelines that can be universalized across federal regulatory boards to ensure that patient-doctor trust is not detrimentally affected by the deployment and widespread adoption of healthcare AI technologies.",,
Transparency and trust in artificial intelligence systems,Transparency can have a negative impact on trust in AI systems.,Search,2020,20,"Philipp  Schmidt, Felix  Bießmann, Timm  Teubner",J. Decis. Syst.,,10.1080/12460125.2020.1819094,https://doi.org/10.1080/12460125.2020.1819094,https://semanticscholar.org/paper/7a19f30e02c34c4eb7b197d3bcd4fbcb8a4e1602,,"ABSTRACT Assistive technology featuring artificial intelligence (AI) to support human decision-making has become ubiquitous. Assistive AI achieves accuracy comparable to or even surpassing that of human experts. However, often the adoption of assistive AI systems is limited by a lack of trust of humans into an AI’s prediction. This is why the AI research community has been focusing on rendering AI decisions more transparent by providing explanations of an AIs decision. To what extent these explanations really help to foster trust into an AI system remains an open question. In this paper, we report the results of a behavioural experiment in which subjects were able to draw on the support of an ML-based decision support tool for text classification. We experimentally varied the information subjects received and show that transparency can actually have a negative impact on trust. We discuss implications for decision makers employing assistive AI technology.",,
Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making,"Confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making.",Search,2020,98,"Yunfeng  Zhang, Q. Vera Liao, Rachel K. E. Bellamy",FAT*,,10.1145/3351095.3372852,https://doi.org/10.1145/3351095.3372852,https://semanticscholar.org/paper/5cc4100a67fd6f2ce3c760655ba7a12f358c7950,http://arxiv.org/pdf/2001.02114,"Today, AI is being increasingly used to help human experts make decisions in high-stakes scenarios. In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success. We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome. A key to their success is to appropriately calibrate human trust in the AI on a case-by-case basis; knowing when to trust or distrust the AI allows the human expert to appropriately apply their knowledge, improving decision outcomes in cases where the model is likely to perform poorly. This research conducts a case study of AI-assisted decision making in which humans and AI have comparable performance alone, and explores whether features that reveal case-specific model information can calibrate trust and improve the joint performance of the human and AI. Specifically, we study the effect of showing confidence score and local explanation for a particular prediction. Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors. We also highlight the problems in using local explanation for AI-assisted decision making scenarios and invite the research community to explore new approaches to explainability for calibrating human trust in AI.",,
Trust in Smart Personal Assistants: A Systematic Literature Review and Development of a Research Agenda,"Smart Personal Assistants fundamentally influence the way individuals perform tasks, use services and interact with organizations.",Search,2020,10,"Naim  Zierau, Christian  Engel, Matthias  Söllner, Jan Marco Leimeister",Wirtschaftsinformatik,,10.30844/wi_2020_a7-zierau,https://doi.org/10.30844/wi_2020_a7-zierau,https://semanticscholar.org/paper/d4fb227f7401844b4516906c5ea9dadfdf0665f3,https://kobra.uni-kassel.de/bitstream/123456789/11902/1/ZierauEngelSoellnerLeimeisterTrustInSmartPersonalAssistants.pdf,"Smart Personal Assistants (SPA) fundamentally influence the way individuals perform tasks, use services and interact with organizations. They thus bear an immense economic and societal potential. However, a lack of trust rooted in perceptions of uncertainty and risk when interacting with intelligent computer agents can inhibit their adoption. In this paper, we conduct a systematic literature review to investigate the state of knowledge on trust in SPAs. Based on a concept-centric analysis of 50 papers, we derive three distinct research perspectives that constitute this nascent field: user interface-driven, interaction-driven, and explanation-driven trust in SPAs. Building on the results of our analysis, we develop a research agenda to spark and guide future research surrounding trust in SPAs. Ultimately, this paper intends to contribute to the body of knowledge of trust in artificial intelligence-based systems, specifically SPAs. It does so by proposing a novel framework mapping out their",,Review
"A Review of Trust in Artificial Intelligence: Challenges, Vulnerabilities and Future Directions","Artificial Intelligence (AI) can benefit society, but it is also fraught with risks.",Search,2021,4,"Steven  Lockey, Nicole M. Gillespie, Daniel  Holm, Ida Asadi Someh",HICSS,,10.24251/HICSS.2021.664,https://doi.org/10.24251/HICSS.2021.664,https://semanticscholar.org/paper/1f11e05c5484ff5a066eab37c6268ad7aceaac8d,http://scholarspace.manoa.hawaii.edu/bitstream/10125/71284/1/0534.pdf,"Artificial Intelligence (AI) can benefit society, but it is also fraught with risks. Societal adoption of AI is recognized to depend on stakeholder trust in AI, yet the literature on trust in AI is fragmented, and little is known about the vulnerabilities faced by different stakeholders, making it is difficult to draw on this evidence-base to inform practice and policy. We undertake a literature review to take stock of what is known about the antecedents of trust in AI, and organize our findings around five trust challenges unique to or exacerbated by AI. Further, we develop a concept matrix identifying the key vulnerabilities to stakeholders raised by each of the challenges, and propose a multi-stakeholder approach to future research.",,Review
Trust in artificial intelligence for medical diagnoses.,People have comparable standards of performance for AI and human doctors and that trust in AI does not increase when people are told the AI outperforms the human doctor.,Search,2020,5,"Georgiana  Juravle, Andriana  Boudouraki, Miglena  Terziyska, Constantin  Rezlescu",Progress in brain research,,10.1016/bs.pbr.2020.06.006,https://doi.org/10.1016/bs.pbr.2020.06.006,https://semanticscholar.org/paper/06f0bb0b40507020a9866dc8f35f5a26700b33af,,"We present two online experiments investigating trust in artificial intelligence (AI) as a primary and secondary medical diagnosis tool and one experiment testing two methods to increase trust in AI. Participants in Experiment 1 read hypothetical scenarios of low and high-risk diseases, followed by two sequential diagnoses, and estimated their trust in the medical findings. In three between-participants groups, the first and second diagnoses were given by: human and AI, AI and human, and human and human doctors, respectively. In Experiment 2 we examined if people expected higher standards of performance from AI than human doctors, in order to trust AI treatment recommendations. In Experiment 3 we investigated the possibility to increase trust in AI diagnoses by: (i) informing our participants that the AI outperforms the human doctor, and (ii) nudging them to prefer AI diagnoses in a choice between AI and human doctors. Results indicate overall lower trust in AI, as well as for diagnoses of high-risk diseases. Participants trusted AI doctors less than humans for first diagnoses, and they were also less likely to trust a second opinion from an AI doctor for high risk diseases. Surprisingly, results highlight that people have comparable standards of performance for AI and human doctors and that trust in AI does not increase when people are told the AI outperforms the human doctor. Importantly, we find that the gap in trust between AI and human diagnoses is eliminated when people are nudged to select AI in a free-choice paradigm between human and AI diagnoses, with trust for AI diagnoses significantly increased when participants could choose their doctor. These findings isolate control over one's medical practitioner as a valid candidate for future trust-related medical diagnosis and highlight a solid potential path to smooth acceptance of AI diagnoses amongst patients.",,
Trusting artificial intelligence in cybersecurity is a double-edged sword,AI can improve cybersecurity but facilitate new forms of attacks to itself.,Search,2019,28,"Mariarosaria  Taddeo, Tom  McCutcheon, Luciano  Floridi",Nat. Mach. Intell.,,10.1038/s42256-019-0109-1,https://doi.org/10.1038/s42256-019-0109-1,https://semanticscholar.org/paper/a485c4c937a4b229455d611129ff4cd6a628f259,https://ora.ox.ac.uk/objects/uuid:8188813d-138e-409d-8686-d2d7d5fa0879/download_file?safe_filename=Trusting%2BAI%2Bin%2BCybersecurity.pdf&file_format=application%2Fpdf&type_of_work=Journal+article,"Applications of artificial intelligence (AI) for cybersecurity tasks are attracting greater attention from the private and the public sectors. Estimates indicate that the market for AI in cybersecurity will grow from US$1 billion in 2016 to a US$34.8 billion net worth by 2025. The latest national cybersecurity and defence strategies of several governments explicitly mention AI capabilities. At the same time, initiatives to define new standards and certification procedures to elicit users’ trust in AI are emerging on a global scale. However, trust in AI (both machine learning and neural networks) to deliver cybersecurity tasks is a double-edged sword: it can improve substantially cybersecurity practices, but can also facilitate new forms of attacks to the AI applications themselves, which may pose severe security threats. We argue that trust in AI for cybersecurity is unwarranted and that, to reduce security risks, some form of control to ensure the deployment of ‘reliable AI’ for cybersecurity is necessary. To this end, we offer three recommendations focusing on the design, development and deployment of AI for cybersecurity.Current national cybersecurity and defence strategies of several governments mention explicitly the use of AI. However, it will be important to develop standards and certification procedures, which involves continuous monitoring and assessment of threats. The focus should be on the reliability of AI-based systems, rather than on eliciting users’ trust in AI.",,
Do Humans Trust Advice More if it Comes from AI? An Analysis of Human-AI Interactions,Participants heed advice equally if it is suggested by AI or humans.,Search,2021,2,"Kailas  Vodrahalli, Tobias  Gerstenberg, James  Zou",ArXiv,,,,https://semanticscholar.org/paper/05c03ca164948bcd3355ac135bcfd9c728e2014c,,"In many applications of AI, the algorithm’s output is framed as a suggestion to a human user. The user may ignore the advice or take it into consideration to modify his/her decisions. With the increasing prevalence of such human-AI interactions, it is important to understand how users act (or do not act) upon AI advice, and how users regard advice differently if they believe the advice come from an “AI” versus another human. In this paper, we characterize how humans use AI suggestions relative to equivalent suggestions from a group of peer humans across several experimental settings. We find that participants’ beliefs about the human versus AI performance on a given task affects whether or not they heed the advice. When participants decide to use the advice, they do so similarly for human and AI suggestions. These results provide insights into factors that affect human-AI interactions.",,
Trusting Artificial Intelligence in Healthcare,People show much less trust in machines offering mortgage advice compared to mortgage brokers.,Search,2018,2,"Weiyu  Wang, Keng  Siau",AMCIS,,,,https://semanticscholar.org/paper/39872923340739926acb85bd580a49a4c2b32891,,"Artificial Intelligence (AI) is able to perform at humans and even surpass human’s performances in some tasks. Recent cases about self-driving cars, cashier-free supermarket Amazon Go, and virtual assistants such as Apple’s Siri and Google Assistant have illustrated the current and future potential of AI. AI and its applications have infiltrated human’s work and daily life. It is inevitable that humans need to build a working relationship with AI and its applications. On one hand, humans can benefit from this new technology, for instance, a home robot can release housewife from mundane and monotonous tasks (Siau 2017, Siau 2018). On the other hand, the potential threats pose by AI and the possible social upheavals should not be overlooked. The fatal crash of self-driving cars, the data breach of famous websites, and the potential unemployment of cab and truck drivers are hindering human’s trust and acceptance of this new technology. A study conducted by HSBC shows that only 8% of the participants would trust a machine offering mortgage advice compared to 41% trusting a mortgage broker.",,