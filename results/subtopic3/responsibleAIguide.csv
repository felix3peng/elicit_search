Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Responsible AI Tutorial,A key aspect to making AI responsible is to have a development pipeline that can promote reproducibility of results and manage the lineage of data and ML models.,Search,2022,,"Dr Mukta Paliwal, Dattaraj  Rao, Amogh Kamat Tarcar",COMAD/CODS,,10.1145/3493700.3493769,https://doi.org/10.1145/3493700.3493769,https://semanticscholar.org/paper/04b7d3004cdf971ea05803fbe39c25de561afcc5,,"There is rapid technical progress and widespread adoption of Artificial Intelligence (AI) based products and workflows influencing many aspects of human and business activities like banking, healthcare, advertising and many more. Although accuracy of AI models is undoubtedly the most important factor considered while deploying AI based products, there is urgent need to understand how AI can be designed to operate responsibly. Responsible AI is a framework that each software developing organization needs to adapt to build customer trust in the transparency, accountability, fairness, and security of deployed AI solutions. At the same time a key aspect to making AI responsible is to have a development pipeline that can promote reproducibility of results and manage the lineage of data and ML models. This tutorial will throw light on these aspects of Responsible AI with a working example demonstrating the concept. The intent of the tutorial will be to equip the audience with enough knowledge of the concepts along with code to gain appreciation for the importance of building Responsible AI.",,
Responsible AI and Its Stakeholders,All stakeholders involved in the development of AI are responsible for their systems.,Search,2020,3,"Gabriel  Lima, Meeyoung  Cha",ArXiv,,,,https://semanticscholar.org/paper/1d2eac19d1bd75d9d9f2afc919c2612b819c4ac2,,"Responsible Artificial Intelligence (AI) proposes a framework that holds all stakeholders involved in the development of AI to be responsible for their systems. It, however, fails to accommodate the possibility of holding AI responsible per se, which could close some legal and moral gaps concerning the deployment of autonomous and self-learning systems. We discuss three notions of responsibility (i.e., blameworthiness, accountability, and liability) for all stakeholders, including AI, and suggest the roles of jurisdiction and the general public in this matter.",,
Principles and business processes for responsible AI,A risk assessment approach is key to managing AI responsibly.,Search,2019,25,Roger  Clarke,Comput. Law Secur. Rev.,,10.1016/J.CLSR.2019.04.007,https://doi.org/10.1016/J.CLSR.2019.04.007,https://semanticscholar.org/paper/9042677e67305726a4511ef3e9a7a00239af2a79,,"Abstract The first article in this series examined why the world wants controls over Artificial Intelligence (AI). This second article discusses how an organisation can manage AI responsibly, in order to protect its own interests, but also those of its stakeholders and society as a whole. A limited amount of guidance is provided by ethical analysis. A much more effective approach is to apply adapted forms of the established techniques of risk assessment and risk management. Critically, risk assessment needs to be undertaken not only with the organisation's own interests in focus, but also from the perspectives of other stakeholders. To underpin this new form of business process, a set of Principles for Responsible AI is presented, consolidating proposals put forward by a diverse collection of 30 organisations.",,
"Responsible AI – Key themes, concerns & recommendations for European research and innovation","The ""Responsible AI"" research and innovation advisory process can be informed by cross-disciplinary expert input.",Search,2018,3,"Steve  Taylor, Michael  Boniface, Brian  Pickering, Michael  Anderson, David  Danks, Asbjørn  Følstad, Matthias  Leese, Vincent  Müller, Tom  Sorell, Alan F. T. Winfield, Fiona  Woollard",,,10.5281/ZENODO.1303252,https://doi.org/10.5281/ZENODO.1303252,https://semanticscholar.org/paper/d7f5b08fa9e76812bb6432625ff564542d010e9f,,"This document’s purpose is to provide input into the advisory processes that determine European support for both research into Responsible AI; and how innovation using AI that takes into account issues of responsibility can be supported. “Responsible AI” is an umbrella term for investigations into legal, ethical and moral standpoints of autonomous algorithms or applications of AI whose actions may be safety-critical or impact the lives of citizens in significant and disruptive ways. To address its purpose, this document reports a summary of results from a consultation with cross-disciplinary experts in and around the subject of Responsible AI.",,
Responsible AI: A Primer for the Legal Community,"The legal community should have a good understanding of the responsible development and deployment of artificial intelligence in order to inform, translate, and advise on the legal implications of AI systems.",Search,2020,,"Ilana  Golbin, Anand S. Rao, Ali  Hadjarian, Daniel  Krittman",2020 IEEE International Conference on Big Data (Big Data),,10.1109/BigData50022.2020.9377738,https://doi.org/10.1109/BigData50022.2020.9377738,https://semanticscholar.org/paper/7d6cd81f02876c8bdd3c75582a73734339fdd711,,"Artificial intelligence (AI) is increasingly being adopted for automation and decision-making tasks across all industries, public sector, and law. Applications range from hiring and credit limit decisions, to loan and healthcare claim approvals, to criminal sentencing, and even the selective provision of information by social media companies to different groups of viewers. The increased adoption of AI, affecting so many aspects of our daily lives, highlights the potential risks around automated decision making and the need for better governance and ethical standards when deploying such systems. In response to that need, governments, states, municipalities, private sector organizations, and industry groups around the world have drafted hundreds, perhaps even thousands at this point - of new, regulatory proposals and guidelines; many already in effect and more on the way. The data-driven and often black box nature of these systems does not absolve organizations from the social responsibility or increasingly commonplace regulatory requirements to confirm they work as intended and are deployed in a responsible manner, lest they run the risk of reputational damage, regulatory fines, and/or legal action. The legal community should have a good understanding of the responsible development and deployment of artificial intelligence in order to inform, translate, and advise on the legal implications of AI systems.",,
Report prepared by the Montreal AI Ethics Institute (MAIEI) for Publication Norms for Responsible AI by Partnership on AI,"The field of AI is currently fragmented in terms of how this technology is researched, developed, funded, etc.",Search,2020,1,"Abhishek  Gupta, Camylle  Lanteigne, Victoria Heath Montreal AI Ethics Institute, Microsoft, Algora  Lab",ArXiv,,,,https://semanticscholar.org/paper/7817a982bf9e75582c2ebd3b4ccbd8673c913a30,,"The history of science and technology shows that seemingly innocuous developments in scientific theories and research have enabled real-world applications with significant negative consequences for humanity. In order to ensure that the science and technology of AI is developed in a humane manner, we must develop research publication norms that are informed by our growing understanding of AI's potential threats and use cases. Unfortunately, it's difficult to create a set of publication norms for responsible AI because the field of AI is currently fragmented in terms of how this technology is researched, developed, funded, etc. To examine this challenge and find solutions, the Montreal AI Ethics Institute (MAIEI) collaborated with the Partnership on AI in May 2020 to host two public consultation meetups. These meetups examined potential publication norms for responsible AI, with the goal of creating a clear set of recommendations and ways forward for publishers.

In its submission, MAIEI provides six initial recommendations, these include: 1) create tools to navigate publication decisions, 2) offer a page number extension, 3) develop a network of peers, 4) require broad impact statements, 5) require the publication of expected results, and 6) revamp the peer-review process. After considering potential concerns regarding these recommendations, including constraining innovation and creating a ""black market"" for AI research, MAIEI outlines three ways forward for publishers, these include: 1) state clearly and consistently the need for established norms, 2) coordinate and build trust as a community, and 3) change the approach.",,Review
Responsible AI,AI can be a powerful tool to address global challenges and to help people lead more meaningful lives.,Search,2021,1,Ben  Shneiderman,Commun. ACM,,10.1145/3445973,https://doi.org/10.1145/3445973,https://semanticscholar.org/paper/dd8216b7c6329d7931ed5ad842263e960d15397d,,Recommendations for increasing the benefits of artificial intelligence technologies.,,
Report prepared by the Montreal AI Ethics Institute (MAIEI) on Publication Norms for Responsible AI,"The field of AI is currently fragmented in terms of how this technology is researched, developed, funded, etc.",Search,2020,1,"Abhishek  Gupta, Camylle  Lanteigne, Victoria Heath Montreal AI Ethics Institute, Microsoft, Algora  Lab",,,,,https://semanticscholar.org/paper/f68bbdc1e313965737085466c24980a7c489b34e,,"The history of science and technology shows that seemingly innocuous developments in scientific theories and research have enabled real-world applications with significant negative consequences for humanity. In order to ensure that the science and technology of AI is developed in a humane manner, we must develop research publication norms that are informed by our growing understanding of AI's potential threats and use cases. Unfortunately, it's difficult to create a set of publication norms for responsible AI because the field of AI is currently fragmented in terms of how this technology is researched, developed, funded, etc. To examine this challenge and find solutions, the Montreal AI Ethics Institute (MAIEI) co-hosted two public consultations with the Partnership on AI in May 2020. These meetups examined potential publication norms for responsible AI, with the goal of creating a clear set of recommendations and ways forward for publishers.

In its submission, MAIEI provides six initial recommendations, these include: 1) create tools to navigate publication decisions, 2) offer a page number extension, 3) develop a network of peers, 4) require broad impact statements, 5) require the publication of expected results, and 6) revamp the peer-review process. After considering potential concerns regarding these recommendations, including constraining innovation and creating a ""black market"" for AI research, MAIEI outlines three ways forward for publishers, these include: 1) state clearly and consistently the need for established norms, 2) coordinate and build trust as a community, and 3) change the approach.",,Review
A critical perspective on guidelines for responsible and trustworthy artificial intelligence,,Search,2020,4,"Banu  Buruk, Perihan Elif Ekmekci, Berna  Arda","Medicine, health care, and philosophy",,10.1007/s11019-020-09948-1,https://doi.org/10.1007/s11019-020-09948-1,https://semanticscholar.org/paper/4990df10e6423ad5d44888697dd8bc523287fd14,,"Artificial intelligence (AI) is among the fastest developing areas of advanced technology in medicine. The most important qualia of AI which makes it different from other advanced technology products is its ability to improve its original program and decision-making algorithms via deep learning abilities. This difference is the reason that AI technology stands out from the ethical issues of other advanced technology artifacts. The ethical issues of AI technology vary from privacy and confidentiality of personal data to ethical status and value of AI entities in a wide spectrum, depending on their capability of deep learning and scope of the domains in which they operate. Developing ethical norms and guidelines for planning, development, production, and usage of AI technology has become an important issue to overcome these problems. In this respect three outstanding documents have been produced: 1. The Montréal Declaration for Responsible Development of Artificial Intelligence 2. Ethics Guidelines for Trustworthy AI 3. Asilomar Artificial Intelligence Principles In this study, these three documents will be analyzed with respect to the ethical principles and values they involve, their perspectives for approaching ethical issues, and their prospects for ethical reasoning when one or more of these values and principles are in conflict. Then, the sufficiency of these guidelines for addressing current or prospective ethical issues emerging from the existence of AI technology in medicine will be evaluated. The discussion will be pursued in terms of the ambiguity of interlocutors and efficiency for working out ethical dilemmas occurring in practical life.",,
Responsible AI Challenges in End-to-end Machine Learning,,Search,2021,,"Steven Euijong Whang, Ki Hyun Tae, Yuji  Roh, Geon  Heo",ArXiv,,,,https://semanticscholar.org/paper/0233cd95dd0bc327dd72a14d60216c98021250ab,,"Responsible AI is becoming critical as AI is widely used in our everyday lives. Many companies that deploy AI publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more. In addition, these objectives are not only relevant to model training, but to all steps of end-to-end machine learning, which include data collection, data cleaning and validation, model training, model evaluation, and model management and serving. Finally, responsible AI is conceptually challenging, and supporting all the objectives must be as easy as possible. We thus propose three key research directions towards this vision – depth, breadth, and usability – to measure progress and introduce our ongoing research. First, responsible AI must be deeply supported where multiple objectives like fairness and robust must be handled together. To this end, we propose FR-Train, a holistic framework for fair and robust model training in the presence of data bias and poisoning. Second, responsible AI must be broadly supported, preferably in all steps of machine learning. Currently we focus on the data pre-processing steps and propose Slice Tuner, a selective data acquisition framework for training fair and accurate models, and MLClean, a data cleaning framework that also improves fairness and robustness. Finally, responsible AI must be usable where the techniques must be easy to deploy and actionable. We propose FairBatch, a batch selection approach for fairness that is effective and simple to use, and Slice Finder, a model evaluation tool that automatically finds problematic slices. We believe we scratched the surface of responsible AI for end-to-end machine learning and suggest research challenges moving forward.",,
Responsible AI—Two Frameworks for Ethical Design Practice,The difficulty in moving from principles to practice is a challenge for the implementation of ethical guidelines.,Search,2020,28,"Dorian  Peters, Karina  Vold, Diana  Robinson, Rafael A. Calvo",IEEE Transactions on Technology and Society,,10.1109/TTS.2020.2974991,https://doi.org/10.1109/TTS.2020.2974991,https://semanticscholar.org/paper/3193cbc9ff8a6fe9b308d7075704d5dc43accc93,https://ieeexplore.ieee.org/ielx7/8566059/8995808/09001063.pdf,"In 2019, the IEEE launched the P7000 standards projects intended to address ethical issues in the design of autonomous and intelligent systems. This move came amidst a growing public concern over the unintended consequences of artificial intelligence (AI), compounded by the lack of an anticipatory process for attending to ethical impact within professional practice. However, the difficulty in moving from principles to practice presents a significant challenge to the implementation of ethical guidelines. Herein, we describe two complementary frameworks for integrating ethical analysis into engineering practice to help address this challenge. We then provide the outcomes of an ethical analysis informed by these frameworks, conducted within the specific context of Internet-delivered therapy in digital mental health. We hope both the frameworks and analysis can provide tools and insights, not only for the context of digital healthcare but also for data-enabled and intelligent technology development more broadly.",,
Ensuring Responsible AI in Practice,Ethical reasoning by AI systems can be ensured by including human oversight and control.,Search,2019,,Virginia  Dignum,Responsible Artificial Intelligence,,10.1007/978-3-030-30371-6_6,https://doi.org/10.1007/978-3-030-30371-6_6,https://semanticscholar.org/paper/dd9b74c42bd880f84b3b2a0a984fda481ec83d49,,"In the previous chapters, we have discussed the issue of responsibility with respect to the processes to design, develop, deploy and use AI (Chapter 4), and how to deal with ethical reasoning by the AI systems themselves (Chapter 5). In this chapter, we will look at mechanisms that can ensure that all involved will indeed take the responsible route.",,
Toward an Understanding of Responsible Artificial Intelligence Practices,,Search,2020,16,"Yichuan  Wang, Mengran  Xiong, Hossein  Olya",HICSS,,10.24251/hicss.2020.610,https://doi.org/10.24251/hicss.2020.610,https://semanticscholar.org/paper/6f62e85aa4034e206caa2482e90c349c954e21fb,http://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf,"Artificial Intelligence (AI) is influencing all aspects of human and business activities nowadays. Although potential benefits emerged from AI technologies have been widely discussed in many current literature, there is an urgently need to understand how AI can be designed to operate responsibly and act in a manner meeting stakeholders’ expectations and applicable regulations. We seek to fill the gap by exploring the practices of responsible AI and identifying the potential benefits when implementing responsible AI practices. In this study, 10 responsible AI cases were selected from different industries to better understand the use of responsible AI in practices. Four responsible AI practices are identified, including governance, ethically design solutions, risk control and training and education and five strategies for firms who are considering to adopt responsible AI practices are recommended.",,
Responsible AI: requirements and challenges,AI systems require research and development efforts to foster socially beneficial applications.,Search,2019,11,Malik  Ghallab,AI Perspectives,,10.1186/s42467-019-0003-z,https://doi.org/10.1186/s42467-019-0003-z,https://semanticscholar.org/paper/a99caae8480c9d0a7186edba5d9009844a85c8d0,https://aiperspectives.springeropen.com/track/pdf/10.1186/s42467-019-0003-z,"This position paper discusses the requirements and challenges for responsible AI with respect to two interdependent objectives: (i) how to foster research and development efforts toward socially beneficial applications, and (ii) how to take into account and mitigate the human and social risks of AI systems.",,
Responsible AI for Digital Health: a Synthesis and a Research Agenda,There are significant issues regarding each of the areas of responsible AI in health AI.,Search,2021,11,"Cristina  Trocin, Patrick  Mikalef, Zacharoula  Papamitsiou, Kieran  Conboy",Information Systems Frontiers,,10.1007/s10796-021-10146-4,https://doi.org/10.1007/s10796-021-10146-4,https://semanticscholar.org/paper/758289be073e7d4e6be6e6e4a7f937ef48ae81bd,https://link.springer.com/content/pdf/10.1007/s10796-021-10146-4.pdf,"Responsible AI is concerned with the design, implementation and use of ethical, transparent, and accountable AI technology in order to reduce biases, promote fairness, equality, and to help facilitate interpretability and explainability of outcomes, which are particularly pertinent in a healthcare context. However, the extant literature on health AI reveals significant issues regarding each of the areas of responsible AI, posing moral and ethical consequences. This is particularly concerning in a health context where lives are at stake and where there are significant sensitivities that are not as pertinent in other domains outside of health. This calls for a comprehensive analysis of health AI using responsible AI concepts as a structural lens. A systematic literature review supported our data collection and sampling procedure, the corresponding analysis, and extraction of research themes helped us provide an evidence-based foundation. We contribute with a systematic description and explanation of the intellectual structure of Responsible AI in digital health and develop an agenda for future research.",,Review
Behavioral Use Licensing for Responsible AI,License terms are a useful tool for enforcement in situations where it is difficult to legislate AI usage.,Search,2020,1,"Danish  Contractor, Daniel  McDuff, Julia  Haines, Jenny  Lee, Christopher  Hines, Brent  Hecht",ArXiv,,,,https://semanticscholar.org/paper/a026dc32fb20004d25640a11acc46720280abd38,,"Scientific research and development relies on the sharing of ideas and artifacts. With the growing reliance on artificial intelligence (AI) for many different applications, the sharing of code, data, and models is important to ensure the ability to replicate methods and the democratization of scientific knowledge. Many high-profile journals and conferences expect code to be submitted and released with papers. Furthermore, developers often want to release code and models to encourage development of technology that leverages their frameworks and services. However, AI algorithms are becoming increasingly powerful and generalized. Ultimately, the context in which an algorithm is applied can be far removed from that which the developers had intended. A number of organizations have expressed concerns about inappropriate or irresponsible use of AI and have proposed AI ethical guidelines and responsible AI initiatives. While such guidelines are useful and help shape policy, they are not easily enforceable. Governments have taken note of the risks associated with certain types of AI applications and have passed legislation. While these are enforceable, they require prolonged scientific and political deliberation. In this paper we advocate the use of licensing to enable legally enforceable behavioral use conditions on software and data. We argue that licenses serve as a useful tool for enforcement in situations where it is difficult or time-consuming to legislate AI usage. Furthermore, by using such licenses, AI developers provide a signal to the AI community, as well as governmental bodies, that they are taking responsibility for their technologies and are encouraging responsible use by downstream users.",,