Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI,AI fairness checklists could provide organizational infrastructure for formalizing ad-hoc processes and empowering individual advocates.,Search,2020,92,"Michael A. Madaio, Luke  Stark, Jennifer Wortman Vaughan, Hanna M. Wallach",CHI,,10.1145/3313831.3376445,https://doi.org/10.1145/3313831.3376445,https://semanticscholar.org/paper/58bb221c1e375f254826b7b7341f74057e87676c,https://dl.acm.org/doi/pdf/10.1145/3313831.3376445,"Many organizations have published principles intended to guide the ethical development and deployment of AI systems; however, their abstract nature makes them difficult to operationalize. Some organizations have therefore produced AI ethics checklists, as well as checklists for more specific concepts, such as fairness, as applied to AI systems. But unless checklists are grounded in practitioners' needs, they may be misused. To understand the role of checklists in AI ethics, we conducted an iterative co-design process with 48 practitioners, focusing on fairness. We co-designed an AI fairness checklist and identified desiderata and concerns for AI fairness checklists in general. We found that AI fairness checklists could provide organizational infrastructure for formalizing ad-hoc processes and empowering individual advocates. We highlight aspects of organizational culture that may impact the efficacy of AI fairness checklists, and suggest future design directions.",,
Prompting Conversations about Fairness in AI Development with Checklists,Checklists can serve as “values levers” in enabling and structuring conversations about ethics.,Search,2020,,"Michael  Madaio, Luke  Stark",,,,,https://semanticscholar.org/paper/ab14e3c913c8488031dc63017f0e61ba64d22236,,"In many high-stakes domains, such as healthcare and aviation, checklists have had significant, positive impacts. However, checklists may be inappropriate when used as compliance-oriented memory aids in domains where the actions and decisions of stakeholders are contingent on and situated within particular sociocultural contexts, such as the ethical development and deployment of AI systems. In this paper, we therefore draw inspiration from the role that checklists play in the construction industry, where they are used to make sure that stakeholders communicate with one another. We study how checklists can serve as “values levers” in enabling and structuring conversations about ethics, as well as more specific concepts, such as fairness, when developing and deploying AI systems. To do this, we use data from 48 practitioners who were previously involved in co-designing an AI fairness checklist.",,
Fairlearn: A toolkit for assessing and improving fairness in AI,Fairlearn is a toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems.,Search,2020,48,"Sarah  Bird, Miro  Dudík, Richard  Edgar, Brandon  Horn, Roman  Lutz, Vanessa  Milan, Mehrnoosh  Sameki, Hanna  Wallach, Kathleen  Walker",,,,,https://semanticscholar.org/paper/5894d57ea49bd5c136ebefb1e6c3986555908ea0,,"We introduce Fairlearn, an open source toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems. Fairlearn has two components: an interactive visualization dashboard and unfairness mitigation algorithms. These components are designed to help with navigating trade-offs between fairness and model performance. We emphasize that prioritizing fairness in AI systems is a sociotechnical challenge. Because there are many complex sources of unfairness—some societal and some technical—it is not possible to fully “debias” a system or to guarantee fairness; the goal is to mitigate fairness-related harms as much as possible. As Fairlearn grows to include additional fairness metrics, unfairness mitigation algorithms, and visualization capabilities, we hope that it will be shaped by a diverse community of stakeholders, ranging from data scientists, developers, and business decision makers to the people whose lives may be affected by the predictions of AI systems.",,
Fairness Score and Process Standardization: Framework for Fairness Certification in Artificial Intelligence Systems,A Fairness Certificate issued by a designated third-party auditing agency would boost the conviction of organizations in the AI systems that they intend to deploy.,Search,2022,,"Avinash  Agarwal, Harsh  Agarwal, Nihaarika  Agarwal",ArXiv,,,,https://semanticscholar.org/paper/64c2fa6b3c161fe2a5ae20f03ff77c3250027e10,,"Decisions made by various Artificial Intelligence (AI) systems greatly influence our day-to-day lives. With the increasing use of AI systems, it becomes crucial to know that they are fair, identify the underlying biases in their decision-making, and create a standardized framework to ascertain their fairness. In this paper, we propose a novel Fairness Score to measure the fairness of a data-driven AI system and a Standard Operating Procedure (SOP) for issuing Fairness Certification for such systems. Fairness Score and audit process standardization will ensure quality, reduce ambiguity, enable comparison and improve the trustworthiness of the AI systems. It will also provide a framework to operationalise the concept of fairness and facilitate the commercial deployment of such systems. Furthermore, a Fairness Certificate issued by a designated third-party auditing agency following the standardized process would boost the conviction of the organizations in the AI systems that they intend to deploy. The Bias Index proposed in this paper also reveals comparative bias amongst the various protected attributes within the dataset. To substantiate the proposed framework, we iteratively train a model on biased and unbiased data using multiple datasets and check that the Fairness Score and the proposed process correctly identify the biases and judge the fairness.",,
AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias,A new toolkit for algorithmic fairness can facilitate the transition of fairness research algorithms to an industrial setting.,Search,2019,113,"Rachel K. E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Jacquelyn A. Martino, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",IBM J. Res. Dev.,,10.1147/jrd.2019.2942287,https://doi.org/10.1147/jrd.2019.2942287,https://semanticscholar.org/paper/333671a5fbbf726f8819138f3670524ec0405726,,"Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This article introduces a new open-source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license (

https://github.com/ibm/aif360

). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms for use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience that provides a gentle introduction to the concepts and capabilities for line-of-business users, researchers, and developers to extend the toolkit with their new algorithms and improvements and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.",,
"AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias",AI Fairness 360 is a toolkit to facilitate the transition of fairness research algorithms to use in an industrial setting.,Search,2018,316,"Rachel K. E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Jacquelyn  Martino, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",ArXiv,,,,https://semanticscholar.org/paper/c8541b1dc813f3a638d7acc79e5f972e77f3c5a7,,"Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license {this https URL). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms.

The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (this https URL) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.",,
"Designing fair AI for managing employees in organizations: a review, critique, and design agenda",Organizations must avoid unfairness toward workers in AI systems or face decreased worker effort and increased worker turnover.,Search,2020,28,"Lionel P. Robert, Casey  Pierce, Liz  Morris, Sangmi  Kim, Rasha  Alahmad",Hum. Comput. Interact.,,10.1080/07370024.2020.1735391,https://doi.org/10.1080/07370024.2020.1735391,https://semanticscholar.org/paper/e62c3e7589ecd73d910da751143a49be5510616c,http://deepblue.lib.umich.edu/bitstream/2027.42/153812/6/Robert%20et%20al.%202020%20AI%20Fairness%20New%20Proof.pdf,"ABSTRACT Organizations are rapidly deploying artificial intelligence (AI) systems to manage their workers. However, AI has been found at times to be unfair to workers. Unfairness toward workers has been associated with decreased worker effort and increased worker turnover. To avoid such problems, AI systems must be designed to support fairness and redress instances of unfairness. Despite the attention related to AI unfairness, there has not been a theoretical and systematic approach to developing a design agenda. This paper addresses the issue in three ways. First, we introduce the organizational justice theory, three different fairness types (distributive, procedural, interactional), and the frameworks for redressing instances of unfairness (retributive justice, restorative justice). Second, we review the design literature that specifically focuses on issues of AI fairness in organizations. Third, we propose a design agenda for AI fairness in organizations that applies each of the fairness types to organizational scenarios. Then, the paper concludes with implications for future research.",,Review
Towards Fairness Certification in Artificial Intelligence,AI must be monitored to ensure it does not amplify prejudices and societal stereotypes.,Search,2021,1,"Tatiana  Tommasi, Silvia  Bucci, Barbara  Caputo, Pietro  Asinari",ArXiv,,,,https://semanticscholar.org/paper/157246efaa0a667383cd78da0599231687368e0e,,"Thanks to the great progress of machine learning in the last years, several Artificial Intelligence (AI) techniques have been increasingly moving from the controlled research laboratory settings to our everyday life. The most simple examples are the spam filters that keep our email account in order, face detectors that help us when taking a portrait picture, online recommender systems that suggest which movie and clothing we might like, or interactive maps that navigate us towards our vacation home. Artificial intelligence is clearly supportive in many decision-making scenarios, but when it comes to sensitive areas such as health care, hiring policies, education, banking or justice, with major impact on individuals and society, it becomes crucial to establish guidelines on how to design, develop, deploy and monitor this technology. Indeed the decision rules elaborated by machine learning models are data-driven and there are multiple ways in which discriminatory biases can seep into data. Algorithms trained on those data incur the risk of amplifying prejudices and societal stereotypes by over associating protected attributes such as gender, ethnicity or disabilities with the prediction task.",,
Towards the Right Kind of Fairness in AI,Choosing the right kind of fairness for a given AI system depends on ethical standards and legal requirements.,Search,2021,2,"Boris  Ruf, Marcin  Detyniecki",ArXiv,,,,https://semanticscholar.org/paper/58e8e3d258dc656d9ef90f7468eeff7d7a513e89,,"Fairness is a concept of justice. Various definitions exist, some of them conflicting with each other. In the absence of an uniformly accepted notion of fairness, choosing the right kind for a specific situation has always been a central issue in human history. When it comes to implementing sustainable fairness in artificial intelligence systems, this old question plays a key role once again: How to identify the most appropriate fairness metric for a particular application? The answer is often a matter of context, and the best choice depends on ethical standards and legal requirements. Since ethics guidelines on this topic are kept rather general for now, we aim to provide more hands-on guidance with this document. Therefore, we first structure the complex landscape of existing fairness metrics and explain the different options by example. Furthermore, we propose the “Fairness Compass”, a tool which formalises the selection process and makes identifying the most appropriate fairness definition for a given system a simple, straightforward procedure. Because this process also allows to document the reasoning behind the respective decisions, we argue that this approach can help to build trust from the user through explaining and justifying the implemented fairness. ∗{boris.ruf,marcin.detyniecki}@axa.com 1 ar X iv :2 10 2. 08 45 3v 6 [ cs .A I] 2 7 A ug 2 02 1",,
Transparency Tools for Fairness in AI (Luskin),"When assessing and correcting bias in AI, ""controlled fairness"" is suitable when there is no ""ground truth"" data.",Search,2020,1,"Mingliang  Chen, Aria  Shahverdi, Sarah  Anderson, Se Yong Park, Justin  Zhang, Dana  Dachman-Soled, Kristin  Lauter, Min  Wu",ArXiv,,10.1007/978-3-030-58748-2_4,https://doi.org/10.1007/978-3-030-58748-2_4,https://semanticscholar.org/paper/03e9164acb7b1cdf9b8870ede9288c816c5cf7d9,http://arxiv.org/pdf/2007.04484,"We propose new tools for policy-makers to use when assessing and correcting fairness and bias in AI algorithms. The three tools are:

- A new definition of fairness called ""controlled fairness"" with respect to choices of protected features and filters. The definition provides a simple test of fairness of an algorithm with respect to a dataset. This notion of fairness is suitable in cases where fairness is prioritized over accuracy, such as in cases where there is no ""ground truth"" data, only data labeled with past decisions (which may have been biased).

- Algorithms for retraining a given classifier to achieve ""controlled fairness"" with respect to a choice of features and filters. Two algorithms are presented, implemented and tested. These algorithms require training two different models in two stages. We experiment with combinations of various types of models for the first and second stage and report on which combinations perform best in terms of fairness and accuracy.

- Algorithms for adjusting model parameters to achieve a notion of fairness called ""classification parity"". This notion of fairness is suitable in cases where accuracy is prioritized. Two algorithms are presented, one which assumes that protected features are accessible to the model during testing, and one which assumes protected features are not accessible during testing.

We evaluate our tools on three different publicly available datasets. We find that the tools are useful for understanding various dimensions of bias, and that in practice the algorithms are effective in starkly reducing a given observed bias when tested on new data.",,
FAIR AI: A Conceptual Framework for Democratisation of 21st Century AI,The lack of interpretability is a main inhibitor of broader use of 21st century AI.,Search,2021,,Saman  Halgamuge,"2021 International Conference on Instrumentation, Control, and Automation (ICA)",,10.1109/ICA52848.2021.9625672,https://doi.org/10.1109/ICA52848.2021.9625672,https://semanticscholar.org/paper/cae45a30452713cacad1e5ec04adfb4a065f6a8e,,"Popular models of AI have two significant deficiencies: 1) they are mostly manually designed using the experience of AI-experts 2) they lack human interpretability, i.e., users cannot make sense of the functionality of neural network architectures either semantically/linguistically or mathematically. This lack of interpretability is a main inhibitor of broader use of 21st century AI, e.g., Deep Neural Networks (DNN). The dependence on AI experts to create AI hinders the democratisation of AI and therefore the accessibility to AI. Addressing these deficiencies would provide answers to some of the valid questions about traceability, accountability and the ability to integrate existing knowledge (scientific or linguistically articulated human experience) into the model. This keynote abstract addresses these two significant deficiencies that inhibit the democratisation of AI by developing new methods that can automatically create interpretable neural network models without the help of AI-experts. The proposed cross-fertilised innovation will have a profound impact on the society through the increased accessibility and trustworthiness of AI beneficial to almost all areas of sciences, engineering, and humanities.",,
Introduction to AI Fairness,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern.",Search,2020,2,"Yunfeng  Zhang, Rachel K. E. Bellamy, Moninder  Singh, Q. Vera Liao",CHI Extended Abstracts,,10.1145/3334480.3375059,https://doi.org/10.1145/3334480.3375059,https://semanticscholar.org/paper/d49f8d57240cd04dd66d91bf53c639497139ab3a,,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. Recently, the AI research community has proposed many methods to measure and mitigate unwanted biases, and developed open-source toolkits for developers to make fair AI. This course will cover the recent development in algorithmic fairness, including the many different definitions of fairness, their corresponding quantitative measurements, and ways to mitigate biases. This course is open to beginners and is designed for anyone interested in the topic of AI fairness.",,
Introduction to AI Fairness,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern.",Search,2021,,"Yunfeng  Zhang, Rachel K. E. Bellamy, Q. Vera Liao, Moninder  Singh",CHI Extended Abstracts,,10.1145/3411763.3444998,https://doi.org/10.1145/3411763.3444998,https://semanticscholar.org/paper/e24d854f822bebd531926eee518d4b2e1455e5de,,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. Recently, the AI research community has proposed many methods to measure and mitigate unwanted biases, and developed open-source toolkits for developers to make fair AI. This course will cover the recent development in algorithmic fairness, including the many different definitions of fairness, their corresponding quantitative measurements, and ways to mitigate biases. This course is open to beginners and is designed for anyone interested in the topic of AI fairness.",,
Explaining how your AI system is fair,Sharing the reasons and principles expressed during the decision making process with the broader audience would help maintain confidence in AI systems.,Search,2021,,"Boris  Ruf, Marcin  Detyniecki",ArXiv,,,,https://semanticscholar.org/paper/1271e86aad602cfa09727dc98bae31c19ce77cc6,,"Copyright held by the owner/author(s). CHI’21,, May 8–13, 2021, Online Virtual Conference (originally Yokohama, Japan) ACM 978-1-4503-6819-3/20/04. https://doi.org/10.1145/3334480.XXXXXXX Abstract To implement fair machine learning in a sustainable way, choosing the right fairness objective is key. Since fairness is a concept of justice which comes in various, sometimes conflicting definitions, this is not a trivial task though. The most appropriate fairness definition for an artificial intelligence (AI) system is a matter of ethical standards and legal requirements, and the right choice depends on the particular use case and its context. In this position paper, we propose to use a decision tree as means to explain and justify the implemented kind of fairness to the end users. Such a structure would first of all support AI practitioners in mapping ethical principles to fairness definitions for a concrete application and therefore make the selection a straightforward and transparent process. However, this approach would also help document the reasoning behind the decision making. Due to the general complexity of the topic of fairness in AI, we argue that specifying ""fairness"" for a given use case is the best way forward to maintain confidence in AI systems. In this case, this could be achieved by sharing the reasons and principles expressed during the decision making process with the broader audience.",,
Fairness in AI applications,AI fairness problems can be resolved by forms of public consultation.,Search,2021,,Cameron  Shelley,2021 IEEE International Symposium on Technology and Society (ISTAS),,10.1109/istas52410.2021.9629140,https://doi.org/10.1109/istas52410.2021.9629140,https://semanticscholar.org/paper/048a7b203bfa22550ddd01af10f40a538a1bb27d,,"Applications of Artificial Intelligence (AI) that have broad, social impact for many people have recently increased greatly in number. They will continue to increase in ubiquity and impact for some time to come. In conjunction with this increase, many scholars have studied the nature of these impacts, including problems of fairness. Here, fairness refers to conflicts of interest between social groups that result from the configuration of these AI systems. One focus of research has been to define these fairness problems and to quantify them in a way that lends itself to calculation of fair outcomes. The purpose of this presentation is to show that this issue of fairness in AI is consistent with fairness problems posed by technological design in general and that addressing these problems goes beyond what can be readily quantified and calculated. For example, many such problems may be best resolved by forms of public consultation. This point is clarified by presenting an analytical tool, the Fairness Impact Assessment, and examples from AI and elsewhere.",,
Design Methods for Artificial Intelligence Fairness and Transparency,"Fairness and transparency in artificial intelligence continue to become more prevalent as topics for research, design and development.",Search,2021,,"Simone  Stumpf, Lorenzo  Strappelli, Subeida  Ahmed, Yuri  Nakao, Aisha  Naseer, Giulia Del Gamba, Daniele  Regoli",IUI Workshops,,,,https://semanticscholar.org/paper/baf8cb94908738c3b2d040a6b44d812e5ca80396,,"Fairness and transparency in artificial intelligence (AI) continue to become more prevalent as topics for research, design and development. General principles and guidelines for designing ethical and responsible AI systems have been proposed, yet there is a lack of design methods for these kinds of systems. In this paper, we present CoFAIR, a novel method to design user interfaces for exploring fairness, consisting of series of co-design workshops, and wider evaluation. This method can be readily applied in practice by researchers, designers and developers to create responsible and ethical AI systems.",,