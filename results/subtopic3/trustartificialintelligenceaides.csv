Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Zombies in the Loop? Humans Trust Untrustworthy AI-Advisors for Ethical Decisions,AI is overtrusted rather than distrusted.,Search,2021,,"Sebastian  Krugel, Andreas  Ostermaier, Matthias  Uhl",,,,,https://semanticscholar.org/paper/5fa1ca52f54ec170d42f899ff918ec3609895d42,,"Departing from the claim that AI needs to be trustworthy, we find that ethical advice from an AI-powered algorithm is trusted even when its users know nothing about its training data and when they learn information about it that warrants distrust. We conducted online experiments where the subjects took the role of decision-makers who received advice from an algorithm on how to deal with an ethical dilemma. We manipulated the information about the algorithm and studied its influence. Our findings suggest that AI is overtrusted rather than distrusted. We suggest digital literacy as a potential remedy to ensure the responsible use of AI.",,
Attachment and trust in artificial intelligence,People with a more secure attachment style show more trust in AI.,Search,2021,21,"Omri  Gillath, Ting  Ai, Michael  Branicky, Shawn  Keshmiri, Rob  Davison, Ryan  Spaulding",Comput. Hum. Behav.,,10.1016/j.chb.2020.106607,https://doi.org/10.1016/j.chb.2020.106607,https://semanticscholar.org/paper/d7e73699049a73cf90c82a7b5aef589d84dbe794,,"Abstract Lack of trust is one of the main obstacles standing in the way of taking full advantage of the benefits artificial intelligence (AI) has to offer. Most research on trust in AI focuses on cognitive ways to boost trust. Here, instead, we focus on boosting trust in AI via affective means. Specifically, we tested and found associations between one's attachment style—an individual difference representing the way people feel, think, and behave in relationships—and trust in AI. In Study 1 we found that attachment anxiety predicted less trust. In Study 2, we found that enhancing attachment anxiety reduced trust, whereas enhancing attachment security increased trust in AI. In Study 3, we found that exposure to attachment security cues (but not positive affect cues) resulted in increased trust as compared with exposure to neutral cues. Overall, our findings demonstrate an association between attachment security and trust in AI, and support the ability to increase trust in AI via attachment security priming.",,
Interactive Applications with Artificial Intelligence: The Role of Trust among Digital Assistant Users,Digital assistant users worry about the bidirectionality of perceived performance of digital assistants.,Search,2020,2,"Pur  Purwanto, Kuswandi  Kuswandi, Fatmah  Fatmah",,,10.17323/2500-2597.2020.2.64.75,https://doi.org/10.17323/2500-2597.2020.2.64.75,https://semanticscholar.org/paper/5ff199d11ff0665fb2c2d034bcdf60352e090d6b,https://foresight-journal.hse.ru/data/2020/07/10/1605934674/5-Purwanto-64-75.pdf,"People are increasingly dependent on technology. On the other hand, companies’ large-scale investments to establish an ongoing loyalty with technology platforms and ecosystems show negative results. This is due to lower trust, concerns about risk, and increasing issues of privacy. Despite the continuous development of digital assistant applications to increase interactivity, however, there is no guarantee that the concept of interactivity is capable of gaining users’ trust and addressing their concerns. The purpose of the present study was to analyze the effects of controllability, synchronicity, bidirectionality on perceived performance and user satisfaction with digital assistant applications as moderated by perceived trust. Amos 22.0 was used to analyze a sample of 150 digital assistant users of brands Samsung Bixby, Google Assistant, Apple Siri, and other brands. Results show that bidirectionality is the most worrying feature in terms of perceived performance of digital assistants related to trust and privacy protection issues of personal data, whereas the other two features contribute to perceived performance and digital assistant users’ satisfaction. Perceived trust plays a role in moderating the relationship between controllability, synchronicity bi-directionality of perceived performance. Finally, perceived performance has an effect on digital assistant users’ satisfaction.",,
Trust in Artificial Intelligence,"Artificial intelligence is being used for scheduling appointments, powering smart homes, recognizing people’s faces in photos, making health diagnoses, and several other things.",Search,2018,14,Arathi  Sethumadhavan,Ergonomics in Design: The Quarterly of Human Factors Applications,,10.1177/1064804618818592,https://doi.org/10.1177/1064804618818592,https://semanticscholar.org/paper/94a43052c729c8e7e3f28c50c76e46d09ec95f62,https://journals.sagepub.com/doi/pdf/10.1177/1064804618818592,"Trust is fundamental to creating a lasting relationship with another human being. In our daily lives, we encounter several situations where we place trust on other humans such as bus drivers, colleagues, and even strangers. Trust in human-human teams is initially founded on the predictability of the trustee, and as the relationship between the trustor and the trustee progresses, dependability or integrity replaces predictability as the basis of trust (Hoff & Bashir, 2015). As artificial intelligence (AI) gets smarter and smarter, it is becoming an integral part of human lives. For example, AI is being used for scheduling appointments, powering smart homes, recognizing people’s faces in photos, making health diagnoses, providing investment advice, and several other things. As with a humanhuman team, trust is a necessary ingredient for human-AI partnerships. However, trust in human-human teams progresses in the reverse order from human-human teams (e.g., Hoff & Bashir, 2015). This is because humans initially assume that AI is near perfect. Therefore, in the initial stages, faith forms the essential constituent of trust, and as the number of exchanges between the human and the AI increases, faith is replaced by dependability and predictability. Trust in AI is considered a twodimensional construct comprising trust and distrust, where trust is associated with feelings of calmness and security and distrust involves fear and worry (Lyons, Stokes, Eschleman, Alarcon, & Barelka, 2011). Unarguably, trust is a complex social process with a variety of factors determining the extent to which humans trust AI agents (Hoff & Bashir, 2015; Lee & See, 2004). Specifically, in order to calibrate the right level of trust in AI, consider dispositional (i.e., user characteristics such as age, culture, gender, and personality), internal (e.g., user characteristics such as workload, mood, self-confidence, and working memory capacity), environmental (e.g., task difficulty, perceived risks and benefits, organizational setting), and learned factors (e.g., reputation of the AI, reliability, consistency, type and timing of errors made by the AI, and users’ experience with similar agents). Design factors (e.g., appearance, ease of use, communication style, and transparency of the AI) also affect perceptions of trust. For example, anthrophomorphizing is an effective way of establishing a long-term social bond between humans and AI agents, which is driven by the neurotransmitter and hormone, oxytocin (e.g., de Visser et al., 2017). Further, anthropomorphic agents also resist breakdown in trust compared to their counterpart non-anthropomorphic agents, presumably because anthropomorphic agents remind users of humans who are forgiven more easily for being imperfect in comparison to machine-like agents. Interestingly, there is also evidence that humans tend to disclose more information to AI therapists than human therapists. Transparency of the AI also helps to calibrate the right level of trust by enabling users to develop accurate mental models of the AI underpinnings (e.g., Balfe, Sharples, & Wilson, 2018). 818592 ERGXXX10.1177/1064804618818592ergonomics in designergonomics in design research-article2018",,
Trust in Artificial Intelligence: Meta-Analytic Findings.,"There are several factors that influence trust in AI, including reliability and anthropomorphism.",Search,2021,1,"Alexandra D Kaplan, Theresa T Kessler, J Christopher Brill, P A Hancock",Human factors,,10.1177/00187208211013988,https://doi.org/10.1177/00187208211013988,https://semanticscholar.org/paper/4ad12e5c49202187f2a85351d417f81dc86b8524,,"OBJECTIVE

The present meta-analysis sought to determine significant factors that predict trust in artificial intelligence (AI). Such factors were divided into those relating to (a) the human trustor, (b) the AI trustee, and (c) the shared context of their interaction.

BACKGROUND

There are many factors influencing trust in robots, automation, and technology in general, and there have been several meta-analytic attempts to understand the antecedents of trust in these areas. However, no targeted meta-analysis has been performed examining the antecedents of trust in AI.

METHOD

Data from 65 articles examined the three predicted categories, as well as the subcategories of human characteristics and abilities, AI performance and attributes, and contextual tasking. Lastly, four common uses for AI (i.e., chatbots, robots, automated vehicles, and nonembodied, plain algorithms) were examined as further potential moderating factors.

RESULTS

Results showed that all of the examined categories were significant predictors of trust in AI as well as many individual antecedents such as AI reliability and anthropomorphism, among many others.

CONCLUSION

Overall, the results of this meta-analysis determined several factors that influence trust, including some that have no bearing on AI performance. Additionally, we highlight the areas where there is currently no empirical research.

APPLICATION

Findings from this analysis will allow designers to build systems that elicit higher or lower levels of trust, as they require.",,Meta-Analysis
Trust in Artificial Intelligence: Australian Insights,Artificial intelligence will be essential to the development of new technologies and will have many potential benefits for Australian society.,Search,2020,5,"Steve  Lockey, Nicole  Gillespie, Caitlin  Curtis",,,10.14264/b32f129,https://doi.org/10.14264/b32f129,https://semanticscholar.org/paper/6c2714f81bc323851052920d240bc21b98e2a9f8,https://espace.library.uq.edu.au/view/UQ:b32f129/LockeyGillespieCurtis_Trust_In_AI.pdf,"Artificial Intelligence (AI) is the cornerstone technology of the Fourth Industrial Revolution and is enabling rapid innovation with many potential benefits for Australian society (e.g. enhanced healthcare diagnostics, transportation optimisation) and business (e.g. enhanced efficiency and competitiveness). The COVID-19 pandemic has accelerated the uptake of advanced technology, and investment in AI continues to grow exponentially.AI also poses considerable risks and challenges to society which raises concerns about whether AI systems are worthy of trust. These concerns have been fuelled by high profile cases of AI use that were biased, discriminatory, manipulative, unlawful, or violated privacy or other human rights. Without public confidence that AI is being developed and used in an ethical and trustworthy manner, it will not be trusted and its full potential will not be realised. To echo the sentiment of Dr Alan Finkel AO, Australia’s Chief Scientist, acceptance of AI rests on “the essential foundation of trust”. Are we capable of extending our trust to AI? This national survey is the first to take a deep dive into answering this question and understanding community trust and expectations in relation to AI. To do this, we surveyed a nationally representative sample of over 2,500 Australian citizens in June to July 2020. Our findings provide important and timely research insights into the public’s trust and attitudes towards AI and lay out a pathway for strengthening trust and acceptance of AI systems.Key findings include:              - Trust is central to the acceptance of AI, and is influenced by four key drivers;              - Australians have low trust in AI systems but generally ‘accept’ or ‘tolerate’ AI;              - Australians expect AI to be regulated and carefully managed;              - Australians expect organisations to uphold the principles of trustworthy AI;              - Australians feel comfortable with some but not all uses of AI at work;              - Australians want to know more about AI but currently have low awareness and understanding of AI and its uses.We draw out the implications of the findings for government, business and NGOs and provide a roadmap to enhancing public trust in AI highlighting three key actions:              - Live up to Australian’s expectations of trustworthy AI              - Strengthen the regulatory framework for governing AI              - Strengthen Australia’s AI literacy",,
Trusting Artificial Intelligence in Healthcare,The potential threats posed by AI and the possible social upheavals should not be overlooked.,Search,2018,2,"Weiyu  Wang, Keng  Siau",AMCIS,,,,https://semanticscholar.org/paper/39872923340739926acb85bd580a49a4c2b32891,,"Artificial Intelligence (AI) is able to perform at humans and even surpass human’s performances in some tasks. Recent cases about self-driving cars, cashier-free supermarket Amazon Go, and virtual assistants such as Apple’s Siri and Google Assistant have illustrated the current and future potential of AI. AI and its applications have infiltrated human’s work and daily life. It is inevitable that humans need to build a working relationship with AI and its applications. On one hand, humans can benefit from this new technology, for instance, a home robot can release housewife from mundane and monotonous tasks (Siau 2017, Siau 2018). On the other hand, the potential threats pose by AI and the possible social upheavals should not be overlooked. The fatal crash of self-driving cars, the data breach of famous websites, and the potential unemployment of cab and truck drivers are hindering human’s trust and acceptance of this new technology. A study conducted by HSBC shows that only 8% of the participants would trust a machine offering mortgage advice compared to 41% trusting a mortgage broker.",,
Limits of trust in medical AI,"Artificial intelligence systems can be relied on but cannot be trusted, which may negatively affect medical relationships.",Search,2020,10,Joshua James Hatherley,Journal of Medical Ethics,,10.1136/medethics-2019-105935,https://doi.org/10.1136/medethics-2019-105935,https://semanticscholar.org/paper/8fd2b272d824d5322928bc40b55c047f35717f8a,,"Artificial intelligence (AI) is expected to revolutionise the practice of medicine. Recent advancements in the field of deep learning have demonstrated success in variety of clinical tasks: detecting diabetic retinopathy from images, predicting hospital readmissions, aiding in the discovery of new drugs, etc. AI’s progress in medicine, however, has led to concerns regarding the potential effects of this technology on relationships of trust in clinical practice. In this paper, I will argue that there is merit to these concerns, since AI systems can be relied on, and are capable of reliability, but cannot be trusted, and are not capable of trustworthiness. Insofar as patients are required to rely on AI systems for their medical decision-making, there is potential for this to produce a deficit of trust in relationships in clinical practice.",,
Transparency and trust in artificial intelligence systems,Transparency can have a negative impact on trust in AI systems.,Search,2020,20,"Philipp  Schmidt, Felix  Bießmann, Timm  Teubner",J. Decis. Syst.,,10.1080/12460125.2020.1819094,https://doi.org/10.1080/12460125.2020.1819094,https://semanticscholar.org/paper/7a19f30e02c34c4eb7b197d3bcd4fbcb8a4e1602,,"ABSTRACT Assistive technology featuring artificial intelligence (AI) to support human decision-making has become ubiquitous. Assistive AI achieves accuracy comparable to or even surpassing that of human experts. However, often the adoption of assistive AI systems is limited by a lack of trust of humans into an AI’s prediction. This is why the AI research community has been focusing on rendering AI decisions more transparent by providing explanations of an AIs decision. To what extent these explanations really help to foster trust into an AI system remains an open question. In this paper, we report the results of a behavioural experiment in which subjects were able to draw on the support of an ML-based decision support tool for text classification. We experimentally varied the information subjects received and show that transparency can actually have a negative impact on trust. We discuss implications for decision makers employing assistive AI technology.",,
Applied artificial intelligence and trust—The case of autonomous vehicles and medical assistance devices,Firms must emphasize the symbiosis of trust in the technology as well as in the firm to increase trust in applied AI.,Search,2016,271,"Monika  Hengstler, Ellen  Enkel, Selina  Duelli",,,10.1016/J.TECHFORE.2015.12.014,https://doi.org/10.1016/J.TECHFORE.2015.12.014,https://semanticscholar.org/paper/203b30269d27e158cd26bd1f47c3207f52e6aec8,,"Automation with inherent artificial intelligence (AI) is increasingly emerging in diverse applications, for instance, autonomous vehicles and medical assistance devices. However, despite their growing use, there is still noticeable skepticism in society regarding these applications. Drawing an analogy from human social interaction, the concept of trust provides a valid foundation for describing the relationship between humans and automation. Accordingly, this paper explores how firms systematically foster trust regarding applied AI. Based on empirical analysis using nine case studies in the transportation and medical technology industries, our study illustrates the dichotomous constitution of trust in applied AI. Concretely, we emphasize the symbiosis of trust in the technology as well as in the innovating firm and its communication about the technology. In doing so, we provide tangible approaches to increase trust in the technology and illustrate the necessity of a democratic development process for applied AI.",,
Trust in Smart Personal Assistants: A Systematic Literature Review and Development of a Research Agenda,"Smart Personal Assistants fundamentally influence the way individuals perform tasks, use services and interact with organizations.",Search,2020,10,"Naim  Zierau, Christian  Engel, Matthias  Söllner, Jan Marco Leimeister",Wirtschaftsinformatik,,10.30844/wi_2020_a7-zierau,https://doi.org/10.30844/wi_2020_a7-zierau,https://semanticscholar.org/paper/d4fb227f7401844b4516906c5ea9dadfdf0665f3,https://kobra.uni-kassel.de/bitstream/123456789/11902/1/ZierauEngelSoellnerLeimeisterTrustInSmartPersonalAssistants.pdf,"Smart Personal Assistants (SPA) fundamentally influence the way individuals perform tasks, use services and interact with organizations. They thus bear an immense economic and societal potential. However, a lack of trust rooted in perceptions of uncertainty and risk when interacting with intelligent computer agents can inhibit their adoption. In this paper, we conduct a systematic literature review to investigate the state of knowledge on trust in SPAs. Based on a concept-centric analysis of 50 papers, we derive three distinct research perspectives that constitute this nascent field: user interface-driven, interaction-driven, and explanation-driven trust in SPAs. Building on the results of our analysis, we develop a research agenda to spark and guide future research surrounding trust in SPAs. Ultimately, this paper intends to contribute to the body of knowledge of trust in artificial intelligence-based systems, specifically SPAs. It does so by proposing a novel framework mapping out their",,Review
Trusting artificial intelligence in cybersecurity is a double-edged sword,Trust in AI for cybersecurity is unwarranted and some form of control to ensure the deployment of ‘reliable AI’ for cybersecurity is necessary.,Search,2019,28,"Mariarosaria  Taddeo, Tom  McCutcheon, Luciano  Floridi",Nat. Mach. Intell.,,10.1038/s42256-019-0109-1,https://doi.org/10.1038/s42256-019-0109-1,https://semanticscholar.org/paper/a485c4c937a4b229455d611129ff4cd6a628f259,https://ora.ox.ac.uk/objects/uuid:8188813d-138e-409d-8686-d2d7d5fa0879/download_file?safe_filename=Trusting%2BAI%2Bin%2BCybersecurity.pdf&file_format=application%2Fpdf&type_of_work=Journal+article,"Applications of artificial intelligence (AI) for cybersecurity tasks are attracting greater attention from the private and the public sectors. Estimates indicate that the market for AI in cybersecurity will grow from US$1 billion in 2016 to a US$34.8 billion net worth by 2025. The latest national cybersecurity and defence strategies of several governments explicitly mention AI capabilities. At the same time, initiatives to define new standards and certification procedures to elicit users’ trust in AI are emerging on a global scale. However, trust in AI (both machine learning and neural networks) to deliver cybersecurity tasks is a double-edged sword: it can improve substantially cybersecurity practices, but can also facilitate new forms of attacks to the AI applications themselves, which may pose severe security threats. We argue that trust in AI for cybersecurity is unwarranted and that, to reduce security risks, some form of control to ensure the deployment of ‘reliable AI’ for cybersecurity is necessary. To this end, we offer three recommendations focusing on the design, development and deployment of AI for cybersecurity.Current national cybersecurity and defence strategies of several governments mention explicitly the use of AI. However, it will be important to develop standards and certification procedures, which involves continuous monitoring and assessment of threats. The focus should be on the reliability of AI-based systems, rather than on eliciting users’ trust in AI.",,
The relationship between trust in AI and trustworthy machine learning technologies,Trust can be impacted throughout the life cycle of AI-based systems.,Search,2020,50,"Ehsan  Toreini, Mhairi  Aitken, Kovila  Coopamootoo, Karen  Elliott, Carlos Gonzalez Zelaya, Aad van Moorsel",FAT*,,10.1145/3351095.3372834,https://doi.org/10.1145/3351095.3372834,https://semanticscholar.org/paper/bd4cf5a6987ef0f39b7e4e88d59b3a3365abcc70,http://arxiv.org/pdf/1912.00782,"To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.",,
Artificial Intelligence and Human Trust in Healthcare: Focus on Clinicians,"Artificial intelligence can transform health care practices with its increasing ability to translate the uncertainty and complexity in data into actionable, clinical decisions or suggestions.",Search,2020,63,"Onur  Asan, Alparslan Emrah Bayrak, Avishek  Choudhury",Journal of medical Internet research,,10.2196/15154,https://doi.org/10.2196/15154,https://semanticscholar.org/paper/77f6a75bba74f36699edceb0edb107dcfb1aaaa1,https://www.jmir.org/2020/6/e15154/PDF,"Artificial intelligence (AI) can transform health care practices with its increasing ability to translate the uncertainty and complexity in data into actionable—though imperfect—clinical decisions or suggestions. In the evolving relationship between humans and AI, trust is the one mechanism that shapes clinicians’ use and adoption of AI. Trust is a psychological mechanism to deal with the uncertainty between what is known and unknown. Several research studies have highlighted the need for improving AI-based systems and enhancing their capabilities to help clinicians. However, assessing the magnitude and impact of human trust on AI technology demands substantial attention. Will a clinician trust an AI-based system? What are the factors that influence human trust in AI? Can trust in AI be optimized to improve decision-making processes? In this paper, we focus on clinicians as the primary users of AI systems in health care and present factors shaping trust between clinicians and AI. We highlight critical challenges related to trust that should be considered during the development of any AI system for clinical use.",,
Trust in Artificial Intelligence: What do we know and why is it important?,There is a need for evidence-based research on trust in AI in order to guide policy and practice.,Search,2020,,Steve  Lockey,,,10.37421/JTSM.2020.9.207,https://doi.org/10.37421/JTSM.2020.9.207,https://semanticscholar.org/paper/1a2bf8b563290014a4375cee463f4bd4f34bfa45,,"The rise of Artificial Intelligence (AI) in our society is becoming ubiquitous and undoubtedly holds much promise. However, AI has also been implicated in high profile breaches of trust or ethical standards, and concerns have been raised over the use of AI in initiatives and technologies that could be inimical to society. Public trust and perceptions of AI trustworthiness underpin AI systems’ social licence to operate, and a myriad of company, industry, governmental and intergovernmental reports have set out principles for ethical and trustworthy AI. To guide the responsible stewardship of AI into our society, a firm foundation of research on trust in AI to enable evidence-based policy and practice is required. However, in order to inform and guide future research, it is imperative to first take stock and understand what is already know about human trust in AI. As such, we undertake a review of 100 papers examining the relationship between trust and AI. We found a fragmented, disjointed and siloed literature with an empirical emphasis on experimentation and surveys relating to specific AI technologies. While findings suggest some convergence on the importance of explainability as a determinant of trust in AI technologies, there are still gaps between conceptual arguments and what has been examined empirically. We urge future research to take a more holistic approach and investigate how trust in different referents impacts on attitudinal and behavioural intentions. Doing so will facilitate a more nuanced understanding of what it means to develop trustworthy AI.",,Review
Building trust when using artificial intelligence,"Trust plays a key role in ensuring recognition in society, continuous progress and development of artificial intelligence.",Search,2021,,"A. A. Dashkov, Yu. O. Nesterova",E-Management,,10.26425/2658-3445-2021-4-2-28-36,https://doi.org/10.26425/2658-3445-2021-4-2-28-36,https://semanticscholar.org/paper/0ac5f81e5ef6e4c26b5a0680c9217f3e1639469b,https://e-management.guu.ru/jour/article/download/150/101,"In the XXI century, “trust” becomes a category that manifests itself in a variety of ways and affects many areas of human activity, including the economy and business. With the development of information and communication technologies and end-to-end technologies, this influence is becoming more and more noticeable. A special place in digital technologies is occupied by human trust when interacting with artificial intelligence and machine learning systems. In this case, trust becomes a potential stumbling block in the field of further development of interaction between artificial intelligence and humans. Trust plays a key role in ensuring recognition in society, continuous progress and development of artificial intelligence.The article considers human trust in artificial intelligence and machine learning systems from different sides. The main objectives of the research paper are to structure existing research on this subject and identify the most important ways to create trust among potential consumers of artificial intelligence products. The article investigates the attitude to artificial intelligence in different countries, as well as the need for trust among users of artificial intelligence systems and analyses the impact of distrust on business. The authors identified the factors that are crucial in the formation of the initial level of trust and the development of continuous trust in artificial intelligence.",,