Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Artificial intelligence for global health,"AI can play an important role in addressing global health care inequities at the individual patient, health system, and population levels.",Search,2019,28,"Ahmed  Hosny, Hugo J. W. L. Aerts",Science,,10.1126/science.aay5189,https://doi.org/10.1126/science.aay5189,https://semanticscholar.org/paper/81948150618bda32e028e2cd3623e28762dd864e,https://cris.maastrichtuniversity.nl/ws/files/75370040/Aerts_2019_Artificial_intelligence_for_global_health.pdf,"Socially responsible technologies promise to help address health care inequalities Artificial intelligence (AI) has demonstrated great progress in the detection, diagnosis, and treatment of diseases. Deep learning, a subset of machine learning based on artificial neural networks, has enabled applications with performance levels approaching those of trained professionals in tasks including the interpretation of medical images and discovery of drug compounds (1). Not surprisingly, most AI developments in health care cater to the needs of high-income countries (HICs), where the majority of research is conducted. Conversely, little is discussed about what AI can bring to medical practice in low- and middle-income countries (LMICs), where workforce shortages and limited resources constrain the access to and quality of care. AI could play an important role in addressing global health care inequities at the individual patient, health system, and population levels. However, challenges in developing and implementing AI applications must be addressed ahead of widespread adoption and measurable impact.",,
Responsible AI for Digital Health: a Synthesis and a Research Agenda,The issues regarding responsible AI in health are particularly concerning given the sensitivities in that domain.,Search,2021,11,"Cristina  Trocin, Patrick  Mikalef, Zacharoula  Papamitsiou, Kieran  Conboy",Information Systems Frontiers,,10.1007/s10796-021-10146-4,https://doi.org/10.1007/s10796-021-10146-4,https://semanticscholar.org/paper/758289be073e7d4e6be6e6e4a7f937ef48ae81bd,https://link.springer.com/content/pdf/10.1007/s10796-021-10146-4.pdf,"Responsible AI is concerned with the design, implementation and use of ethical, transparent, and accountable AI technology in order to reduce biases, promote fairness, equality, and to help facilitate interpretability and explainability of outcomes, which are particularly pertinent in a healthcare context. However, the extant literature on health AI reveals significant issues regarding each of the areas of responsible AI, posing moral and ethical consequences. This is particularly concerning in a health context where lives are at stake and where there are significant sensitivities that are not as pertinent in other domains outside of health. This calls for a comprehensive analysis of health AI using responsible AI concepts as a structural lens. A systematic literature review supported our data collection and sampling procedure, the corresponding analysis, and extraction of research themes helped us provide an evidence-based foundation. We contribute with a systematic description and explanation of the intellectual structure of Responsible AI in digital health and develop an agenda for future research.",,Review
Is There a Place for Responsible Artificial Intelligence in Pandemics? A Tale of Two Countries,The UK’s NHS COVID-19 application has exhibited limited success in fighting the virus despite adhering to responsible AI requirements.,Search,2021,,"Ramzi  El-Haddadeh, Adam  Fadlalla, Nitham M Hindi",Information systems frontiers : a journal of research and innovation,,10.1007/s10796-021-10140-w,https://doi.org/10.1007/s10796-021-10140-w,https://semanticscholar.org/paper/6a4ee216f2e41739ef564f7fa2f0d83ea3ffca6e,https://link.springer.com/content/pdf/10.1007/s10796-021-10140-w.pdf,"This research examines the considerations of responsible Artificial Intelligence in the deployment of AI-based COVID-19 digital proximity tracking and tracing applications in two countries; the State of Qatar and the United Kingdom. Based on the alignment level analysis with the Good AI Society’s framework and sentiment analysis of official tweets, the diagnostic analysis resulted in contrastive findings for the two applications. While the application EHTERAZ (Arabic for precaution) in Qatar has fallen short in adhering to the responsible AI requirements, it has contributed significantly to controlling the pandemic. On the other hand, the UK’s NHS COVID-19 application has exhibited limited success in fighting the virus despite relatively abiding by these requirements. This underlines the need for obtaining a practical and contextual view for a comprehensive discourse on responsible AI in healthcare. Thereby offering necessary guidance for striking a balance between responsible AI requirements and managing pressures towards fighting the pandemic.",,
Towards Responsible Artificial Intelligence in Long-term Care: A Scoping Review on Practical Approaches.,There is limited empirical evidence detailing how responsible AI innovation is addressed in context.,Search,2021,,"Dirk R M Lukkien, Henk Herman Nap, Hendrik P Buimer, Alexander  Peine, Wouter P C Boon, Johannes C F Ket, Mirella M N Minkman, Ellen H M Moors",The Gerontologist,,10.1093/geront/gnab180,https://doi.org/10.1093/geront/gnab180,https://semanticscholar.org/paper/a2adc42d39d81e6cdb5b32c45451c42fd3aa3427,https://academic.oup.com/gerontologist/advance-article-pdf/doi/10.1093/geront/gnab180/42216937/gnab180.pdf,"BACKGROUND AND OBJECTIVES

Artificial intelligence (AI) is widely positioned to become a key element of intelligent technologies used in the long-term care (LTC) for older adults. The increasing relevance and adoption of AI has encouraged debate over the societal and ethical implications of introducing and scaling AI. This scoping review investigates how the design and implementation of AI technologies in LTC is addressed responsibly: so called responsible innovation (RI).

RESEARCH DESIGN AND METHODS

We conducted a systematic literature search in five electronic databases using concepts related to LTC, AI and RI. We then performed a descriptive and thematic analysis to map the key concepts, types of evidence and gaps in the literature.

RESULTS

After reviewing 3,339 papers, 25 papers were identified that met our inclusion criteria. From this literature, we extracted three overarching themes: user-oriented AI innovation; framing AI as a solution to RI issues; and context-sensitivity. Our results provide an overview of measures taken and recommendations provided to address responsible AI innovation in LTC.

DISCUSSION AND IMPLICATIONS

The review underlines the importance of the context of use when addressing responsible AI innovation in LTC. However, limited empirical evidence actually details how responsible AI innovation is addressed in context. Therefore, we recommend expanding empirical studies on RI at the level of specific AI technologies and their local contexts of use. Also, we call for more specific frameworks for responsible AI innovation in LTC to flexibly guide researchers and innovators. Future frameworks should clearly distinguish between RI processes and outcomes.",,Review
Responsible Artificial Intelligence (AI) for Value Formation and Market Performance in Healthcare: the Mediating Role of Patient’s Cognitive Engagement,Different facets of responsible AI guide healthcare firms in evidence-based medicine and improved patient-centered care.,Search,2021,11,"Pradeep  Kumar, Yogesh K. Dwivedi, Ambuj  Anand",Information systems frontiers : a journal of research and innovation,,10.1007/s10796-021-10136-6,https://doi.org/10.1007/s10796-021-10136-6,https://semanticscholar.org/paper/8eaeda4885475764277a056c37e034be2e46ccd2,https://link.springer.com/content/pdf/10.1007/s10796-021-10136-6.pdf,"The Healthcare sector has been at the forefront of the adoption of artificial intelligence (AI) technologies. Owing to the nature of the services and the vulnerability of a large section of end-users, the topic of responsible AI has become the subject of widespread study and discussion. We conduct a mixed-method study to identify the constituents of responsible AI in the healthcare sector and investigate its role in value formation and market performance. The study context is India, where AI technologies are in the developing phase. The results from 12 in-depth interviews enrich the more nuanced understanding of how different facets of responsible AI guide healthcare firms in evidence-based medicine and improved patient centered care. PLS-SEM analysis of 290 survey responses validates the theoretical framework and establishes responsible AI as a third-order factor. The 174 dyadic data findings also confirm the mediation mechanism of the patient’s cognitive engagement with responsible AI-solutions and perceived value, which leads to market performance.",,
Software as a Medical Device: Regulating AI in Healthcare via Responsible AI,The increased adoption of AI in healthcare demands regulation to avoid potential harm and unfair bias.,Search,2021,,"Muhammad Aurangzeb Ahmad, Steve  Overman, Christine  Allen, Vikas  Kumar, Ankur  Teredesai, Carly  Eckert",KDD,,10.1145/3447548.3470823,https://doi.org/10.1145/3447548.3470823,https://semanticscholar.org/paper/86bd1e97f6db49710b4f00f8b94dffc6f1964d9a,,"With the increased adoption of AI in healthcare, there is a growing recognition and demand to regulate AI in healthcare to avoid potential harm and unfair bias against vulnerable populations. Around a hundred governmental bodies and commissions as well as leaders in the tech sector have proposed principles to create responsible AI systems. However, most of these proposals are short on specifics which has led to charges of ethics washing. In this tutorial we offer a guide to help navigate through complex governmental regulations and explain the various constituent practical elements of a responsible AI system in healthcare in the light of proposed regulations. Additionally, we breakdown and emphasize that the recommendations from regulatory bodies like FDA or the EU are necessary but not sufficient elements of creating a responsible AI system. We elucidate how regulations and guidelines often focus on epistemic concerns to the detriment of practical concerns e.g., requirement for fairness without explicating what fairness constitutes for a use case. FDA's Software as a medical device document and EU's GDPR among other AI governance documents talk about the need for implementing sufficiently good machine learning practices. In this tutorial we elucidate what that would mean from a practical perspective for real world use cases in healthcare throughout the machine learning cycle i.e., Data Management, Data Specification, Feature Engineering, Model Evaluation, Model Specification, Model Explainability, Model Fairness, Reproducibility, checks for data leakage and model leakage. We note that conceptualizing responsible AI as a process rather than an end goal accords well with how AI systems are used in practice. We also discuss how a domain centric stakeholder perspective translates into balancing requirements for multiple competing optimization criteria.",,
Making Responsible AI the Norm rather than the Exception,The National Security Commission on Artificial Intelligence (NSCAI) document recommends Responsible AI.,Search,2021,,Abhishek  Gupta,ArXiv,,,,https://semanticscholar.org/paper/75299097114e0eca55790c8f37e4acca6a92cdaa,,"This report prepared by the Montreal AI Ethics Institute provides recommendations in response to the National Security Commission on Artificial Intelligence (NSCAI) Key Considerations for Responsible Development and Fielding of Artificial Intelligence document. The report centres on the idea that Responsible AI should be made the Norm rather than an Exception. It does so by utilizing the guiding principles of: (1) alleviating friction in existing workflows, (2) empowering stakeholders to get buy-in, and (3) conducting an effective translation of abstract standards into actionable engineering practices. After providing some overarching comments on the document from the NSCAI, the report dives into the primary contribution of an actionable framework to help operationalize the ideas presented in the document from the NSCAI. The framework consists of: (1) a learning, knowledge, and information exchange (LKIE), (2) the Three Ways of Responsible AI, (3) an empirically-driven risk-prioritization matrix, and (4) achieving the right level of complexity. All components reinforce each other to move from principles to practice in service of making Responsible AI the norm rather than the exception.",,
"Artificial intelligence in health care: laying the Foundation for Responsible, sustainable, and inclusive innovation in low- and middle-income countries",The use of AI in health care is recent and there is a lack of robust local evaluations to guide decision-making in low-resource settings.,Search,2020,19,"Hassane  Alami, Lysanne  Rivard, Pascale  Lehoux, Steven J. Hoffman, Stéphanie Bernadette Mafalda Cadeddu, Mathilde  Savoldelli, Mamane Abdoulaye Samri, Mohamed Ali Ag Ahmed, Richard  Fleet, Jean-Paul  Fortin",Globalization and Health,,10.1186/s12992-020-00584-1,https://doi.org/10.1186/s12992-020-00584-1,https://semanticscholar.org/paper/b460805f7717925953bc5c17554f736c3c6d58aa,https://globalizationandhealth.biomedcentral.com/track/pdf/10.1186/s12992-020-00584-1,"The World Health Organization and other institutions are considering Artificial Intelligence (AI) as a technology that can potentially address some health system gaps, especially the reduction of global health inequalities in low- and middle-income countries (LMICs). However, because most AI-based health applications are developed and implemented in high-income countries, their use in LMICs contexts is recent and there is a lack of robust local evaluations to guide decision-making in low-resource settings. After discussing the potential benefits as well as the risks and challenges raised by AI-based health care, we propose five building blocks to guide the development and implementation of more responsible, sustainable, and inclusive AI health care technologies in LMICs.",,
Responsible Technologies,AI needs a method to embed values into technology.,Search,2018,1,"Nardine  Osman, Carles  Sierra",GCAI,,10.29007/176h,https://doi.org/10.29007/176h,https://semanticscholar.org/paper/5b454cc8d4237cd3bb9b2997cff16e662531bbdc,https://easychair.org/publications/open/jGxF,"With the current surge of interest in ethics in AI, we present our position with respect to these challenges. Our proposal, responsible technologies, aims to (1) address a number of the ethical challenges put forward in AI, and (2) provide the first building blocks towards the development of ethical AI systems. The current discussion on how to address ethics in AI usually focuses on issues like policies, education, or research culture. There is no computational method yet mature enough to address ethics in AI. We break ground by proposing new methods and tools, underpinned by multidisciplinary research, that can make humans and machines understand their respective dynamic goals while strictly abiding by the values that inspire our societies. This position paper presents our plan of work for the development of responsible technologies that embed values within technology through what we refer to as ethics by construction. 1 The Why: The Rise of Ethics in AI The risks of artificial intelligence (AI) are high on the agendas of top AI experts and enterprises. The wide application of AI is touching our lives in many ways, fuelling the recent surge in interest in ethics in AI. Many discussions about the risks of AI have focused on future AI and its impact on our lives, such as when artificial general intelligence (AGI) is achieved. However, we argue that ethical considerations are much needed today for many existing narrow AI systems. As explained by Professor Dan Weld, narrow AI systems can be catastrophic too, noting that “Knight Capital’s automated trading system is much less intelligent than Google DeepMind’s AlphaGo, but the former lost $440 million in just forty-five minutes. AlphaGo hasn’t and can’t hurt anyone.” 1Top AI scientists have been debating artificial general intelligence (AGI) and the risks of AI in general. Open letters and declarations are being signed by top AI scientists warning about the risks of AI. Some examples are the Open Letter on AI (www.futureoflife.org/ai-open-letter), the Barcelona Declaration for the Proper Development and Use of Artificial Intelligence in Europe (www.iiia.csic.es/barcelonadeclaration/), and the Open Letter on Autonomous Weapons (www.futureoflife.org/open-letter-autonomous-weapons/). 2Institutes and foundations are being set up to promote beneficial AI. Some examples are the Future of Life Institute (www.futureoflife.org), the Partnership on AI (www.partnershiponai.org), Centre for the Study of Existential Risk (www.cser.ac.uk), and OpenAI (www.openai.com). 3One interesting example is the panel at the Beneficial AI Conference in 2017, which discussed the likelihood and the possible outcome of human-level AGI, and what would we like to happen: www.youtube.com/watch? v= OFBwz4R6Fi0 4www.futureoflife.org/2017/03/23/ai-risks-principle/ D. Lee, A. Steen and T. Walsh (eds.), GCAI-2018 (EPiC Series in Computing, vol. 55), pp. 134–147 Responsible Technologies Osman and Sierra One pressing issue, for example, is explainability in AI. Machine learning today is helping with parole decisions, loan decisions, and even job decisions. Racism and sexism of the algorithms is emerging. Their racism against African Americans has been revealed in parole decisions in the US. In job recruitment, it has been noted that different job ads were targeting different ethnic groups. Explainable AI is now a necessity, to help us understand how critical decisions are made. But this also raises the question of what values do we want our AI systems to adhere to? Do we really want a racist algorithm? As such, the value alignment problem has also been gaining ground. It states that “highly autonomous AI systems should be designed so that their goals and behaviours can be assured to align with human values throughout their operation”. In fact, the list of ethical concerns can be a long and daunting list, with issues like explainability, transparency, accountability and responsibility, to name a few. With such a wide range of concerns, we choose to focus on what we refer to as responsible technologies, covering a selected number of the raised concerns. We present our focus and objectives next. 2 The What: Focus and Objectives We propose the term responsible technologies, getting inspiration from the European Commission’s work on responsible research and innovation, which has been declared a key action of the ‘Science with and for Society’ objective and a cross-cutting issue in Horizon 2020, stating that “research and innovation must respond to the needs and ambitions of society, reflect its values, and be responsible”. We adopt, and slightly adapt, this declaration to define responsible technologies as technologies that respond to the needs and ambitions of society, reflect its values, and put people in control . As such, we place needs and values as the basis of responsible technologies, and we introduce the idea of putting the humans in control of their technologies. This is in line with many requirements already put forward by several declarations and open letters on AI, such as the following AI principles developed at the 2017 Beneficial AI Conference: 1) value alignment, which states that AI systems should be designed so that their goals and behaviours can be assured to align with human values throughout their operation; 2) human values, which states that AI systems should be designed and operated so as to be compatible with ideals of human dignity, rights, freedoms, and cultural diversity; 3) personal privacy, which states that people should have the right to access, manage and control the data they generate, given AI systems’ power to analyse and utilise that data; 4) shared benefit, which states that AI technologies should benefit and empower as many people as possible; and 5) human control, which states that humans should choose how and whether to delegate decisions to AI systems, to accomplish human-chosen objectives. As such, and in addition to basing responsible technologies on people’s needs and values (which we believe addresses principles 1–4 above), we also put the humans in control of their technology so that they have a direct say on how their technology evolves according to their evolving needs and values (addressing principle 5 above). Our objectives may be summarised as follows. (1) Develop a novel methodology and mechanisms for the design and development of responsible technologies that are based on people’s needs and values, and evolve with people’s evolving needs and values. (2) Give people control over their technologies so they can decide amongst themselves on their needs and values, and how their technology should behave accordingly. We present in the following section our 5www.newscientist.com/article/mg23230971-200-the-irresistible-rise-of-artificial-intelligence/ 6www.futureoflife.org/2017/02/03/align-artificial-intelligence-with-human-values/ 7www.futureoflife.org/ai-principles/",,
"Human-centered automation and AI - Ideas, insights, and issues from the Intelligent Cockpit Aids research effort","The NASA-Langley Intelligent Cockpit Aids research program is developing concepts for inflight subsystem fault management, replanning missions, and communications management.",Search,1989,5,"Kathy H. Abbott, Paul C. Schutte",,,,,https://semanticscholar.org/paper/89a917a9df5bb7dd0036bdfef70de8f4c3a6a8ed,,"A development status evaluation is presented for the NASA-Langley Intelligent Cockpit Aids research program, which encompasses AI, human/machine interfaces, and conventional automation. Attention is being given to decision-aiding concepts for human-centered automation, with emphasis on inflight subsystem fault management, inflight mission replanning, and communications management. The cockpit envisioned is for advanced commercial transport aircraft.",,
Responsible AI by Design,A large organization is putting in place a company-wide methodology to minimize the risk of undesired consequences of AI.,Search,2019,6,"Richard  Benjamins, Alberto  Barbado, Daniel  Sierra",ArXiv,,,,https://semanticscholar.org/paper/61e65d862d613c8817008e19fa0e66a84bd49c1d,,"Recently, a lot of attention has been given to undesired consequences of Artificial Intelligence (AI), such as unfair bias leading to discrimination, or the lack of explanations of the results of AI systems. There are several important questions to answer before AI can be deployed at scale in our businesses and societies. Most of these issues are being discussed by experts and the wider communities, and it seems there is broad consensus on where they come from. There is, however, less consensus on, and experience with how to practically deal with those issues in organizations that develop and use AI, both from a technical and organizational perspective. In this paper, we discuss the practical case of a large organization that is putting in place a company-wide methodology to minimize the risk of undesired consequences of AI. We hope that other organizations can learn from this and that our experience contributes to making the best of AI while minimizing its risks.",,
Responsible AI by Design in Practice,A large organization is putting in place a company-wide methodology to minimize the risk of undesired consequences of AI.,Search,2019,6,"Richard  Benjamins, Alberto  Barbado, Daniel  Sierra",,,,,https://semanticscholar.org/paper/6f41c058bfa1d469986ee50a6a7718314ae95141,,"Recently, a lot of attention has been given to undesired consequences of Artificial Intelligence (AI), such as unfair bias leading to discrimination, or the lack of explanations of the results of AI systems. There are several important questions to answer before AI can be deployed at scale in our businesses and societies. Most of these issues are being discussed by experts and the wider communities, and it seems there is broad consensus on where they come from. There is, however, less consensus on, and experience with how to practically deal with those issues in organizations that develop and use AI, both from a technical and organizational perspective. In this paper, we discuss the practical case of a large organization that is putting in place a company-wide methodology to minimize the risk of undesired consequences of AI. We hope that other organizations can learn from this and that our experience contributes to making the best of AI while minimizing its risks.",,
An ethically mindful approach to AI for health care,,Search,2020,12,"Jessica  Morley, Luciano  Floridi",The Lancet,,10.1016/S0140-6736(19)32975-7,https://doi.org/10.1016/S0140-6736(19)32975-7,https://semanticscholar.org/paper/b9bc01ba06e37bc1f514099af5418e0bfbbafbf1,,"254 www.thelancet.com Vol 395 January 25, 2020 Health-care systems worldwide face increasing demand, a rise in chronic disease, and resource constraints. At the same time, the use of digital health technologies in all care settings has led to an expansion of data. These data, if harnessed appropriately, could enable health-care providers to target the causes of ill-health and monitor the effectiveness of preventions and interventions. For this reason, policy makers, politicians, clinical entrepreneurs, and computer and data scientists argue that a key part of health-care solutions will be artificial Intelligence (AI), particularly machine learning. AI forms a key part of the National Health Service (NHS) Long-Term Plan (2019) in England, the US National Institutes of Health Strategic Plan for Data Science (2018), and China’s Healthy China 2030 strategy (2016). The willingness to embrace the potential future of medical care, expressed in these national strategies, is a positive development. Health-care providers should, however, be mindful of the risks that arise from AI’s ability to change the intrinsic nature of how health care is delivered. Such potential AI transformations raise ethical risks that are normative, epistemic, or overarching. Such risks relate to inconclusive, inscrutable, or misguided evidence; unfair outcomes or transformative effects; or traceability. Examples of these risks are described in the table. To mitigate these risks, a bold and systematic approach is needed to the implementation of AI solutions in health care that recognises the challenges and addresses them directly. Crucially, this approach must not rely solely on hard governance measures, such as new statutory obligations, that are designed in response to calls for the development of a robust regulatory system. These measures are necessary but insufficient. Regulations provide only the necessary rules of the game, not the best strategy to win it. What is needed is an ethical focus on the end user, and their expectations, demands, needs, and rights. The challenge is the insufficiently consistent approach to this kind of analysis. To date, responses to the ethical risks, such as the NHS Code of Conduct for Data-Driven Health and Care Technology and other non-sector specific ethical codes, centre on protecting the individual. This approach is understandable because this is the An ethically mindful approach to AI for health care",,
Artificial intelligence in a crisis needs ethics with urgency,Artificial intelligence can be used in the COVID-19 response to save lives.,Search,2020,14,"Asaf  Tzachor, Jess  Whittlestone, Lalitha S Sundaram, Seán Ó hÉigeartaigh",Nat. Mach. Intell.,,10.1038/s42256-020-0195-0,https://doi.org/10.1038/s42256-020-0195-0,https://semanticscholar.org/paper/cda4b91f54ee9ecb0be363c58735caa5b0d85d35,https://www.nature.com/articles/s42256-020-0195-0.pdf,"Artificial intelligence tools can help save lives in a pandemic. However, the need to implement technological solutions rapidly raises challenging ethical issues. We need new approaches for ethics with urgency, to ensure AI can be safely and beneficially used in the COVID-19 response and beyond.",,
Impact of intelligent decision aids on expert and novice decision-makers' judgments,Intelligent decision aids may be best viewed as complements to expert decision-makers during complex problem analysis and resolution.,Search,2004,106,"Vicky  Arnold, Philip A. Collier, Stewart A. Leech, Steve G. Sutton",,,10.1111/J.1467-629X.2004.00099.X,https://doi.org/10.1111/J.1467-629X.2004.00099.X,https://semanticscholar.org/paper/15e3297c7bf03edd605e6e6743b1af7a52def26b,,"Businesses have invested tremendous resources into intelligent decision aid development. A good match between user and aid may improve the expert decision-maker's decision quality. However, novices may be prone to poorer decision-making if intelligent decision aids are more expert than the user. The present paper provides an empirical test of the impact of decision aids on subjects with differential expertise levels. The results support the contention that intelligent decision aids aggravate bias in novices' decision-making but mitigate bias in experts' decision-making processes. Intelligent decision aids may be best viewed as complements to expert decision-makers during complex problem analysis and resolution. Copyright 2004 Accounting and Finance Association of Australia and New Zealand.",,
Ethical issues involved in the growing AIDS crisis.,The tradition of the AMA since its organization in 1847 has been to provide a responsible public policy regarding AIDS patients.,Search,1988,103,"John H. Burkhart, Russel H. Patterson, Nancy W. Dickey, Leo M. Henikoff, Douglas D. Lind, Richard J. McMurray, Michael A. Puzak, Michael T. Valley, Mary M. Devlin",Journal of the Tennessee Medical Association,,10.1001/JAMA.1988.03720090050034,https://doi.org/10.1001/JAMA.1988.03720090050034,https://semanticscholar.org/paper/3e17f23e24a0dd72c98b3df24dcf9eb0d514f7f6,,"THE COUNCIL on Ethical and Judicial Affairs of the American Medical Association (AMA) recognizes the growing crisis created by the acquired immunodeficiency syndrome (AIDS) as a crucial health problem involving the physician's ethical responsibility to his patients and to society. The House of Delegates adopted Report YY (1987 Annual Meeting) of the Board of Trustees, which provides excellent guidance for a responsible public policy. As stated therein, AIDS patients are entitled to competent medical service with compassion and respect for human dignity and to the safeguard of their confidences within the constraints of the law. Those persons who are afflicted with the disease or who are seropositive have the right to be free from discrimination. A physician may not ethically refuse to treat a patient whose condition is within the physician's current realm of competence solely because the patient is seropositive. The tradition of the AMA, since its organization in",,