Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Safe and sound - artificial intelligence in hazardous applications,Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making.,Search,2000,272,"John  Fox, Subrata Kumar Das",,,,,https://semanticscholar.org/paper/f62d215d4c40d299929d2e7fa105d54623a9e1bf,,"Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making, where small faults or errors can spell human catastrophe. This book describes, from both practical and theoretical perspectives, an AI technology for supporting sound clinical decision making and safe patient management. The technology combines techniques from conventional software engineering with a systematic method for building intelligent agents. Although the focus is on medicine, many of the ideas can be applied to AI systems in other hazardous settings. The book also covers a number of general AI problems, including knowledge representation and expertise modeling, reasoning and decision making under uncertainty, planning and scheduling, and the design and implementation of intelligent agents.The book, written in an informal style, begins with the medical background and motivations, technical challenges, and proposed solutions. It then turns to a wide-ranging discussion of intelligent and autonomous agents, with particular reference to safety and hazard management. The final section provides a detailed discussion of the knowledge representation and other aspects of the agent model developed in the book, along with a formal logical semantics for the language.",,
Decision Support for Safe AI Design,There is considerable interest in ethical designs for artificial intelligence that do not pose risks to humans.,Search,2012,4,Bill  Hibbard,AGI,,10.1007/978-3-642-35506-6_13,https://doi.org/10.1007/978-3-642-35506-6_13,https://semanticscholar.org/paper/c40cad8cacaa47aac6d4cab8f80749beaebd690c,http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_57.pdf,"There is considerable interest in ethical designs for artificial intelligence (AI) that do not pose risks to humans. This paper proposes using elements of Hutter's agent-environment framework to define a decision support system for simulating, visualizing and analyzing AI designs to understand their consequences. The simulations do not have to be accurate predictions of the future; rather they show the futures that an agent design predicts will fulfill its motivations and that can be explored by AI designers to find risks to humans. In order to safely create a simulation model this paper shows that the most probable finite stochastic program to explain a finite history is finitely computable, and that there is an agent that makes such a computation without any unintended instrumental actions. It also discusses the risks of running an AI in a simulated environment.",,
Safe AI Systems,The safe AI systems must satisfy with 2 majors applying the security measures and analyzing the system.,Search,2018,,Ananta  Bhatt,International Journal of Computer Applications,,10.5120/ijca2018918205,https://doi.org/10.5120/ijca2018918205,https://semanticscholar.org/paper/97aba8083bc5ad49af27a5c26c2bd40e6a17d6a3,,"As you know, it is inevitable to cease the advancement in AI because of the quest for developing systems that learns from experience and comply with the human capabilities. The leading ground-breaking impacts offers both positive and negative aspirations and are addressed as part of this research. To meet with the evil effects of the AI systems, is now the need of the hour to direct the focus on creating safe AI systems. The safe system must satisfy with 2 majorsapplying the security measures and analysing the system. Moreover, system must fulfil the risk and safety valuesintelligence, goals and safety in order to minimise the destruction that may occur due AI technology. However, further research is needed on the actual implementation of the safe AI systems in order to proceed with this research.",,
Safe AI for CPS (Invited Paper),Safety can be ruled out through formal verification for cyber-physical systems using AI for planning and control.,Search,2018,5,"Nathan  Fulton, André  Platzer",2018 IEEE International Test Conference (ITC),,10.1109/TEST.2018.8624774,https://doi.org/10.1109/TEST.2018.8624774,https://semanticscholar.org/paper/03fce03060f6110b2772958f58539320bab519ed,,"Autonomous cyber-physical systems-such as self-driving cars and autonomous drones-often leverage artificial intelligence and machine learning algorithms to act well in open environments. Although testing plays an important role in ensuring safety and robustness, modern autonomous systems have grown so complex that achieving safety via testing alone is intractable. Formal verification reduces this testing burden by ruling out large classes of errant behavior at design time. This paper reviews recent work toward developing formal methods for cyber-physical systems that use AI for planning and control by combining the rigor of formal proofs with the flexibility of reinforcement learning.",,Review
Safe Artificial Intelligence and Formal Methods - (Position Paper),Formal methods can assist in building safer and reliable AI.,Search,2016,9,Emil  Vassev,ISoLA,,10.1007/978-3-319-47166-2_49,https://doi.org/10.1007/978-3-319-47166-2_49,https://semanticscholar.org/paper/796dcb12b96e5d0c2f358b1b67fdfba36d023acc,https://ulir.ul.ie/bitstream/10344/5407/2/Vassev_2016_safe.pdf,"In one aspect of our life or another, today we all live with AI. For example, the mechanisms behind the search engines operating on the Internet do not just retrieve information, but also constantly learn how to respond more rapidly and usefully to our requests. Although framed by its human inventors, this AI is getting stronger and more powerful every day to go beyond the original human intentions in the future. One of the major questions emerging along with the propagation of AI in both technology and life is about safety in AI. This paper presents the author’s view about how formal methods can assist us in building safer and reliable AI.",,
How Safe Is Artificial Intelligence?,Machine learning dramatically changes our civilization.,Search,2019,1,Klaus  Mainzer,Artificial intelligence - When do machines take over?,,10.1007/978-3-662-59717-0_11,https://doi.org/10.1007/978-3-662-59717-0_11,https://semanticscholar.org/paper/5493c1017f9c42565e7f01a170e12a99d128ea82,,"Machine learning dramatically changes our civilization. We rely more and more on efficient algorithms, because otherwise the complexity of our civilizing infrastructure would not be manageable: Our brains are too slow and hopelessly overwhelmed by the amount of data we have to deal with. But how secure are AI algorithms? In practical applications, learning algorithms refer to models of neural networks, which themselves are extremely complex. They are fed and trained with huge amounts of data. The number of necessary parameters explodes exponentially. Nobody knows exactly what happens in these “black boxes” in detail. A statistical trial-and-error procedure often remains. But how should questions of responsibility be decided in, e.g., autonomous driving or in medicine, if the methodological basics remain dark?",,
Safe Artificial General Intelligence via Distributed Ledger Technology,Artificial general intelligence (AGI) progression metrics indicate AGI will occur within decades.,Search,2019,7,Kristen W. Carlson,Big Data Cogn. Comput.,,10.3390/BDCC3030040,https://doi.org/10.3390/BDCC3030040,https://semanticscholar.org/paper/7b30e2b8fc6928ed9e3e4753a1272652ce4b8157,https://www.mdpi.com/2504-2289/3/3/40/pdf,"Artificial general intelligence (AGI) progression metrics indicate AGI will occur within decades. No proof exists that AGI will benefit humans and not harm or eliminate humans. A set of logically distinct conceptual components is proposed that are necessary and sufficient to (1) ensure various AGI scenarios will not harm humanity, and (2) robustly align AGI and human values and goals. By systematically addressing pathways to malevolent AI we can induce the methods/axioms required to redress them. Distributed ledger technology (DLT, “blockchain”) is integral to this proposal, e.g., “smart contracts” are necessary to address the evolution of AI that will be too fast for human monitoring and intervention. The proposed axioms: (1) Access to technology by market license. (2) Transparent ethics embodied in DLT. (3) Morality encrypted via DLT. (4) Behavior control structure with values at roots. (5) Individual bar-code identification of critical components. (6) Configuration Item (from business continuity/disaster recovery planning). (7) Identity verification secured via DLT. (8) “Smart” automated contracts based on DLT. (9) Decentralized applications—AI software modules encrypted via DLT. (10) Audit trail of component usage stored via DLT. (11) Social ostracism (denial of resources) augmented by DLT petitions. (12) Game theory and mechanism design.",,
Controlling Safety of Artificial Intelligence-Based Systems in Healthcare,The safety controlling system could guide and control the implementation of artificial intelligence systems in healthcare.,Search,2021,6,"Mohammad Reza Davahli, Waldemar  Karwowski, Krzysztof  Fiok, Thomas T. H. Wan, Hamid R. Parsaei",Symmetry,,10.3390/sym13010102,https://doi.org/10.3390/sym13010102,https://semanticscholar.org/paper/f725733c4a497ef80f089d391c7b9f5304206d28,https://www.mdpi.com/2073-8994/13/1/102/pdf,"In response to the need to address the safety challenges in the use of artificial intelligence (AI), this research aimed to develop a framework for a safety controlling system (SCS) to address the AI black-box mystery in the healthcare industry. The main objective was to propose safety guidelines for implementing AI black-box models to reduce the risk of potential healthcare-related incidents and accidents. The system was developed by adopting the multi-attribute value model approach (MAVT), which comprises four symmetrical parts: extracting attributes, generating weights for the attributes, developing a rating scale, and finalizing the system. On the basis of the MAVT approach, three layers of attributes were created. The first level contained 6 key dimensions, the second level included 14 attributes, and the third level comprised 78 attributes. The key first level dimensions of the SCS included safety policies, incentives for clinicians, clinician and patient training, communication and interaction, planning of actions, and control of such actions. The proposed system may provide a basis for detecting AI utilization risks, preventing incidents from occurring, and developing emergency plans for AI-related risks. This approach could also guide and control the implementation of AI systems in the healthcare industry.",,
Safety of Artificial Intelligence: A Collaborative Model,Achieving and assuring the safety of systems that use artificial intelligence requires unique solutions.,Search,2020,1,"John  McDermid, Yan  Jia",AISafety@IJCAI,,,,https://semanticscholar.org/paper/355bcdbd0e611afd38c1dc7ccebb29fb1c873cda,,"Achieving and assuring the safety of systems that use artificial intelligence (AI), especially machine learning (ML), pose some specific challenges that require unique solutions. However, that does not mean that good safety and software engineering practices are no longer relevant. This paper shows how the issues associated with AI and ML can be tackled by integrating with established safety and software engineering practices. It sets out a three-layer model, going from top to bottom: system safety/functional safety; “AI/ML safety”; and safety-critical software engineering. This model gives both a basis for achieving and assuring safety and a structure for collaboration between safety engineers and AI/ML specialists. The model is illustrated with a healthcare use case which uses deep reinforcement learning for treating sepsis patients. It is argued that this model is general and that it should underpin future standards and guidelines for safety of this class of system which employ ML, particularly because the model can facilitate collaboration between the different communities.",,
Guidelines for Artificial Intelligence Containment,Safety container software can be used to study and analyze intelligent artificial agents while maintaining safety.,Search,2019,20,"James  Babcock, János  Kramár, Roman V. Yampolskiy",Next-Generation Ethics,,10.1017/9781108616188.008,https://doi.org/10.1017/9781108616188.008,https://semanticscholar.org/paper/b0c820f9d7fa4d8c56a7887548834b7ba65ddfa5,http://www.vetta.org/documents/Machine_Super_Intelligence.pdf,"With almost daily improvements in capabilities of artificial intelligence it is more important than ever to develop safety software for use by the AI research community. Building on our previous work on AI Containment Problem we propose a number of guidelines which should help AI safety researchers to develop reliable sandboxing software for intelligent programs of all levels. Such safety container software will make it possible to study and analyze intelligent artificial agent while maintaining certain level of safety against information leakage, social engineering attacks and cyberattacks from within the container.",,
Building Safer AGI by introducing Artificial Stupidity,Artificial Stupidity can be used to make safe Artificial General Intelligence.,Search,2018,18,"Michaël  Trazzi, Roman V. Yampolskiy",ArXiv,,,,https://semanticscholar.org/paper/feef1ddc618a406e4b125b51ad7de0505516c8e6,,"Artificial Intelligence (AI) achieved super-human performance in a broad variety of domains. We say that an AI is made Artificially Stupid on a task when some limitations are deliberately introduced to match a human's ability to do the task. An Artificial General Intelligence (AGI) can be made safer by limiting its computing power and memory, or by introducing Artificial Stupidity on certain tasks. We survey human intellectual limits and give recommendations for which limits to implement in order to build a safe AGI.",,
"Safe and Sound: Artificial Intelligence in Hazardous Applications - John Fox, Subrata Das, AAAI Press, Menlo Park, CA, and MIT Press, Cambridge, MA/London, UK, 2000, 326 pp., References, Index, Illus., ISBN 0-262-06211-9",Artificial intelligence techniques can solve difficult problems in a more satisfactory way than conventional software.,Search,2003,2,Mar  Marcos,Artif. Intell. Medicine,,10.1016/S0933-3657(02)00082-9,https://doi.org/10.1016/S0933-3657(02)00082-9,https://semanticscholar.org/paper/0ab5618de22bfdb929e3a79248040d0476da2ea3,,"Since the inception of the Artificial Intelligence (AI) discipline, there have been plenty of developments of systems that can be labeled as intelligent. Intelligent systems have the potential of solving difficult problems in a more satisfactory way than conventional software would do, usually making use of heuristics or knowledge that the experts in the domain posses. As AI techniques improve, the range of tasks that intelligent systems are able to perform will grow. This will make them potentially suitable for a higher number of application fields. However, many fields are inherently safety-critical. If we want to put intelligent systems into use in those fields we should pay special attention to safety issues. In a scenario where an intelligent system operates in a complex environment, safety is mainly concerned with ensuring that the system responds adequately to any hazard that might arise, e.g. due to unexpected interactions of the actions it has performed. The book by John Fox and Subrata Das presents an approach to the improvement of safety in AI systems, combining ideas from software engineering, conventional system engineering and AI. It draws lessons from the extensive experience of the authors in the development of medical AI applications. Concerning the management of hazards, their proposal consists in using AI techniques similar to the ones employed for problem-solving. If the complexity of the environment makes it impossible to predict all possible hazards, as it is the case in Medicine, a promising alternative is to provide the system with additional capabilities to dynamically reason about hazards. As the title suggests, the book addresses the problem of making intelligent systems sound and safe. According to this, the two main topics treated in the book are, respectively, how to design intelligent systems that are able to perform complex, knowledge-intensive tasks, and how to make them safe. Both topics are illustrated with numerous examples from the field of Medicine. Nevertheless, the focus of attention is both on the practical achievements and the underlying theoretical framework. The book is structured in three parts. Part I introduces PROforma, a methodology and a technology for building intelligent systems. Part II discusses the safety issues that arise when deploying intelligent systems in complex and changing environments. Finally, Part III describes the theoretical concepts underlying the PROforma technology. A feature of this book is that there are different possible ways to read it, depending on the interest of the reader in the theoretical details of Part III. For those readers mainly interested in this part, a summary of the first two parts is Artificial Intelligence in Medicine 27 (2003) 103–106",,
Safe AI—is this possible?☆,"The inherently self-learning, self-adaptive nature of artificial intelligence methods may require functionally deterministic controllers.",Search,1995,11,M. G. Rodd,,,10.1016/0952-1976(95)00010-X,https://doi.org/10.1016/0952-1976(95)00010-X,https://semanticscholar.org/paper/c53376e4e49ca954f4f4f2ca520b6042967f7dda,,"Abstract This deliberately provocative paper poses the question as to whether it is professionally acceptable to use AI-based control systems in real-time automation. The premise is that the increasing demands for improved control of industrial processes are resulting in a search for new control techniques which, it is suggested, will be based on various AI methodologies. However, in the face of the inherently self-learning, self-adaptive nature of such techniques, it is pointed out that to produce safe, predictable systems, the controllers should be functionally deterministic! In posing this dilemma, the paper suggests possible ways in which it may be resolved.",,
Making algorithms safe for workers: occupational risks associated with work managed by artificial intelligence,Artificial intelligence mechanisms can have health hazards for workers.,Search,2021,,Adrián  Todolí-Signes,Transfer: European Review of Labour and Research,,10.1177/10242589211035040,https://doi.org/10.1177/10242589211035040,https://semanticscholar.org/paper/458b3ed1c05d33f0341ef3115108b5650b1d82c9,,"It is increasingly common for companies to use artificial intelligence mechanisms to manage work. This study examines the health hazards caused by these new forms of technological management. Occupational risks can be reduced if they are taken into account when programming an algorithm. This study confirms the need for algorithms to be correctly programmed, taking account of these occupational risks. In the same way as supervisors have to be trained in risk prevention to be able to perform their work, the algorithm must be programmed to weigh up the occupational risks – and when such features do not exist, steps must be taken to prevent the algorithm being used to direct workers. The algorithm must assess all (known) factors posing a risk to workers’ health and safety. It therefore seems necessary to incorporate a mandatory risk assessment performed by specialists in the programming of algorithms so that all ascertained risks can be taken into account.",,
Towards Safe Artificial General Intelligence,"If humanity would find itself in conflict with a system of much greater intelligence than itself, then human society would likely lose.",Search,2018,21,Tom  Everitt,,,10.25911/5D134A2F8A7D3,https://doi.org/10.25911/5D134A2F8A7D3,https://semanticscholar.org/paper/8b1e149ae23ea7839c9e3a2bd063c354ff7075d0,,"The field of artificial intelligence has recently experienced a number of breakthroughs thanks to progress in deep learning and reinforcement learning. Computer algorithms now outperform humans at Go, Jeopardy, image classification, and lip reading, and are becoming very competent at driving cars and interpreting natural language. The rapid development has led many to conjecture that artificial intelligence with greater-thanhuman ability on a wide range of tasks may not be far. This in turn raises concerns whether we know how to control such systems, in case we were to successfully build them. Indeed, if humanity would find itself in conflict with a system of much greater intelligence than itself, then human society would likely lose. One way to make sure we avoid such a conflict is to ensure that any future AI system with potentially greater-thanhuman-intelligence has goals that are aligned with the goals of the rest of humanity. For example, it should not wish to kill humans or steal their resources. The main focus of this thesis will therefore be goal alignment, i.e. how to design artificially intelligent agents with goals coinciding with the goals of their designers. Focus will mainly be directed towards variants of reinforcement learning, as reinforcement learning currently seems to be the most promising path towards powerful artificial intelligence. We identify and categorize goal misalignment problems in reinforcement learning agents as designed today, and give examples of how these agents may cause catastrophes in the future. We also suggest a number of reasonably modest modifications that can be used to avoid or mitigate each identified misalignment problem. Finally, we also study various choices of decision algorithms, and conditions for when a powerful reinforcement learning system will permit us to shut it down. The central conclusion is that while reinforcement learning systems as designed today are inherently unsafe to scale to human levels of intelligence, there are ways to potentially address many of these issues without straying too far from the currently so successful reinforcement learning paradigm. Much work remains in turning the high-level proposals suggested in this thesis into practical algorithms, however. Central claim: There are a number of theoretically valid, partial solutions to the problem of keeping artificial general intelligence both safe and useful.",,
A Framework for Safeguarding Artificial Intelligence Systems Within Healthcare Domain,Some known risks exist for using artificial intelligence in healthcare but no clear framework to evaluate predictive algorithms.,Search,2019,4,Avishek  Choudhury,British Journal of Healthcare Management,,10.12968/BJHC.2019.0066,https://doi.org/10.12968/BJHC.2019.0066,https://semanticscholar.org/paper/795bea6fb6a899805edf40140a8415fae5f68f7c,,"In healthcare, research on artificial intelligence is becoming increasingly dedicated to applying predictive analytic techniques to make clinical predictions. Even though artificial intelligence has shown promising results in cancer image recognition, triage service automation, and in disease prognosis, its clinical value has not been addressed. Currently, there is a lack of understanding around how some of these algorithms work. Despite knowing the potential risks associated with using artificial intelligence in healthcare, there is no clear framework to evaluate predictive algorithms, which are being commercially implemented within the healthcare industry. To ensure patient safety, regulatory authorities should ensure that proposed algorithms meet the accepted standards of clinical benefit, just as they do for therapeutics and predictive biomarkers. In this article, we offer a framework for the evaluation of predictive algorithms. Although not exhaustive, these criteria can enhance the quality of predictive algorithms and ensure that the algorithms effectively improve clinical outcomes.",,