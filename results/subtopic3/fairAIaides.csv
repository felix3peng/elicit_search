Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Fairlearn: A toolkit for assessing and improving fairness in AI,Fairlearn is a toolkit to assess and improve the fairness of AI systems.,Search,2020,48,"Sarah  Bird, Miro  Dudík, Richard  Edgar, Brandon  Horn, Roman  Lutz, Vanessa  Milan, Mehrnoosh  Sameki, Hanna  Wallach, Kathleen  Walker",,,,,https://semanticscholar.org/paper/5894d57ea49bd5c136ebefb1e6c3986555908ea0,,"We introduce Fairlearn, an open source toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems. Fairlearn has two components: an interactive visualization dashboard and unfairness mitigation algorithms. These components are designed to help with navigating trade-offs between fairness and model performance. We emphasize that prioritizing fairness in AI systems is a sociotechnical challenge. Because there are many complex sources of unfairness—some societal and some technical—it is not possible to fully “debias” a system or to guarantee fairness; the goal is to mitigate fairness-related harms as much as possible. As Fairlearn grows to include additional fairness metrics, unfairness mitigation algorithms, and visualization capabilities, we hope that it will be shaped by a diverse community of stakeholders, ranging from data scientists, developers, and business decision makers to the people whose lives may be affected by the predictions of AI systems.",,
Design Methods for Artificial Intelligence Fairness and Transparency,"Fairness and transparency in artificial intelligence continue to become more prevalent as topics for research, design and development.",Search,2021,,"Simone  Stumpf, Lorenzo  Strappelli, Subeida  Ahmed, Yuri  Nakao, Aisha  Naseer, Giulia Del Gamba, Daniele  Regoli",IUI Workshops,,,,https://semanticscholar.org/paper/baf8cb94908738c3b2d040a6b44d812e5ca80396,,"Fairness and transparency in artificial intelligence (AI) continue to become more prevalent as topics for research, design and development. General principles and guidelines for designing ethical and responsible AI systems have been proposed, yet there is a lack of design methods for these kinds of systems. In this paper, we present CoFAIR, a novel method to design user interfaces for exploring fairness, consisting of series of co-design workshops, and wider evaluation. This method can be readily applied in practice by researchers, designers and developers to create responsible and ethical AI systems.",,
AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias,Fairness is an increasingly important concern as machine learning models are used to support decision making.,Search,2019,113,"Rachel K. E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Jacquelyn A. Martino, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",IBM J. Res. Dev.,,10.1147/jrd.2019.2942287,https://doi.org/10.1147/jrd.2019.2942287,https://semanticscholar.org/paper/333671a5fbbf726f8819138f3670524ec0405726,,"Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This article introduces a new open-source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license (

https://github.com/ibm/aif360

). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms for use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience that provides a gentle introduction to the concepts and capabilities for line-of-business users, researchers, and developers to extend the toolkit with their new algorithms and improvements and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.",,
Workshop on AI fairness for people with disabilities,Artificial intelligence is increasingly being used in decision-making that directly impacts people's lives.,Search,2020,5,"Shari  Trewin, Meredith Ringel Morris, Stacy  Branham, Walter S. Lasecki, Shiri  Azenkot, Nicole  Bleuel, Phill  Jenkins, Jeffrey P. Bigham",ACM SIGACCESS Access. Comput.,,10.1145/3386296.3386297,https://doi.org/10.1145/3386296.3386297,https://semanticscholar.org/paper/0c3a31ed1fc3213ab601653b11f91f0e83a20d95,,"This year the ASSETS conference is hosting a workshop on AI Fairness for People with Disabilities the day before the main conference program begins. This workshop will bring together forty participants to discuss the practical, ethical, and legal ramifications of emerging AI-powered technologies for people with disabilities. We organized this workshop because artificial intelligence is increasingly being used in decision-making that directly impacts people's lives.",,
Toward fairness in AI for people with disabilities SBG@a research roadmap,"AI technologies can improve the lives of people with disabilities, but may not work properly for disabled people.",Search,2020,39,"Anhong  Guo, Ece  Kamar, Jennifer Wortman Vaughan, Hanna  Wallach, Meredith Ringel Morris",ACM SIGACCESS Access. Comput.,,10.1145/3386296.3386298,https://doi.org/10.1145/3386296.3386298,https://semanticscholar.org/paper/e74eb6147977d94ac5db4ff779e6d4e53feeed75,,"AI technologies have the potential to dramatically impact the lives of people with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for many state-of-the-art AI systems, such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing, or language prediction algorithms that can augment communication for people with speech or cognitive disabilities. However, widely deployed AI systems may not work properly for PWD, or worse, may actively discriminate against them. These considerations regarding fairness in AI for PWD have thus far received little attention. In this position paper, we identify potential areas of concern regarding how several AI technology categories may impact particular disability constituencies if care is not taken in their design, development, and testing. We intend for this risk assessment of how various classes of AI might interact with various classes of disability to provide a roadmap for future research that is needed to gather data, test these hypotheses, and build more inclusive algorithms.",,
"AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias","AI Fairness 360 is a toolkit for detecting, understanding, and mitigating unwanted algorithmic bias.",Search,2018,316,"Rachel K. E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Jacquelyn  Martino, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",ArXiv,,,,https://semanticscholar.org/paper/c8541b1dc813f3a638d7acc79e5f972e77f3c5a7,,"Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license {this https URL). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms.

The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (this https URL) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.",,
Artificial intelligence fairness in the context of accessibility research on intelligent systems for people who are deaf or hard of hearing,Artificial intelligence fairness for people with disabilities requires including data from people with disabilities in training sets.,Search,2020,9,"Sushant  Kafle, Abraham  Glasser, Sedeeq  Al-khazraji, Larwan  Berke, Matthew  Seita, Matt  Huenerfauth",ACM SIGACCESS Access. Comput.,,10.1145/3386296.3386300,https://doi.org/10.1145/3386296.3386300,https://semanticscholar.org/paper/e3f537503b645b6ae51a1488d0188a2d9bfd5210,http://arxiv.org/pdf/1908.10414,"We discuss issues of Artificial Intelligence (AI) fairness for people with disabilities, with examples drawn from our research on HCI for AI-based systems for people who are Deaf or Hard of Hearing (DHH). In particular, we discuss the need for inclusion of data from people with disabilities in training sets, the lack of interpretability of AI systems, ethical responsibilities of access technology researchers and companies, the need for appropriate evaluation metrics for AI-based access technologies (to determine if they are ready to be deployed and if they can be trusted by users), and the ways in which AI systems influence human behavior and influence the set of abilities needed by users to successfully interact with computing systems.",,
Considerations for AI fairness for people with disabilities,AI systems should offer opportunities to redress errors and for users to raise fairness concerns.,Search,2019,19,"Shari  Trewin, Sara  Basson, Michael  Muller, Stacy  Branham, Jutta  Treviranus, Daniel  Gruen, Daniel  Hebert, Natalia  Lyckowski, Erich  Manser",SIGAI,,10.1145/3362077.3362086,https://doi.org/10.1145/3362077.3362086,https://semanticscholar.org/paper/68d3fa028db42157d988d2b8ad7d495c157e14e1,,"In society today, people experiencing disability can face discrimination. As artificial intelligence solutions take on increasingly important roles in decision-making and interaction, they have the potential to impact fair treatment of people with disabilities in society both positively and negatively. We describe some of the opportunities and risks across four emerging AI application areas: employment, education, public safety, and healthcare, identified in a workshop with participants experiencing a range of disabilities. In many existing situations, non-AI solutions are already discriminatory, and introducing AI runs the risk of simply perpetuating and replicating these flaws. We next discuss strategies for supporting fairness in the context of disability throughout the AI development lifecycle. AI systems should be reviewed for potential impact on the user in their broader context of use. They should offer opportunities to redress errors, and for users and those impacted to raise fairness concerns. People with disabilities should be included when sourcing data to build models, and in testing, to create a more inclusive and robust system. Finally, we offer pointers into an established body of literature on human-centered design processes and philosophies that may assist AI and ML engineers in innovating algorithms that reduce harm and ultimately enhance the lives of people with disabilities.",,
AI Fairness for People with Disabilities: Point of View,Disability information is highly sensitive and not always shared because of the potential for discrimination.,Search,2018,30,Shari  Trewin,ArXiv,,,,https://semanticscholar.org/paper/8fc60a7489b76641ceee5da9180a3ca76b18560d,,"We consider how fair treatment in society for people with disabilities might be impacted by the rise in the use of artificial intelligence, and especially machine learning methods. We argue that fairness for people with disabilities is different to fairness for other protected attributes such as age, gender or race. One major difference is the extreme diversity of ways disabilities manifest, and people adapt. Secondly, disability information is highly sensitive and not always shared, precisely because of the potential for discrimination. Given these differences, we explore definitions of fairness and how well they work in the disability space. Finally, we suggest ways of approaching fairness for people with disabilities in AI applications.",,
Explaining how your AI system is fair,Choosing the right fairness objective is key to implementing fair machine learning in a sustainable way.,Search,2021,,"Boris  Ruf, Marcin  Detyniecki",ArXiv,,,,https://semanticscholar.org/paper/1271e86aad602cfa09727dc98bae31c19ce77cc6,,"Copyright held by the owner/author(s). CHI’21,, May 8–13, 2021, Online Virtual Conference (originally Yokohama, Japan) ACM 978-1-4503-6819-3/20/04. https://doi.org/10.1145/3334480.XXXXXXX Abstract To implement fair machine learning in a sustainable way, choosing the right fairness objective is key. Since fairness is a concept of justice which comes in various, sometimes conflicting definitions, this is not a trivial task though. The most appropriate fairness definition for an artificial intelligence (AI) system is a matter of ethical standards and legal requirements, and the right choice depends on the particular use case and its context. In this position paper, we propose to use a decision tree as means to explain and justify the implemented kind of fairness to the end users. Such a structure would first of all support AI practitioners in mapping ethical principles to fairness definitions for a concrete application and therefore make the selection a straightforward and transparent process. However, this approach would also help document the reasoning behind the decision making. Due to the general complexity of the topic of fairness in AI, we argue that specifying ""fairness"" for a given use case is the best way forward to maintain confidence in AI systems. In this case, this could be achieved by sharing the reasons and principles expressed during the decision making process with the broader audience.",,
Transparency Tools for Fairness in AI (Luskin),"Tools are useful for understanding various dimensions of bias, and that in practice the algorithms are effective in starkly reducing a given observed bias when tested on new data.",Search,2020,1,"Mingliang  Chen, Aria  Shahverdi, Sarah  Anderson, Se Yong Park, Justin  Zhang, Dana  Dachman-Soled, Kristin  Lauter, Min  Wu",ArXiv,,10.1007/978-3-030-58748-2_4,https://doi.org/10.1007/978-3-030-58748-2_4,https://semanticscholar.org/paper/03e9164acb7b1cdf9b8870ede9288c816c5cf7d9,http://arxiv.org/pdf/2007.04484,"We propose new tools for policy-makers to use when assessing and correcting fairness and bias in AI algorithms. The three tools are:

- A new definition of fairness called ""controlled fairness"" with respect to choices of protected features and filters. The definition provides a simple test of fairness of an algorithm with respect to a dataset. This notion of fairness is suitable in cases where fairness is prioritized over accuracy, such as in cases where there is no ""ground truth"" data, only data labeled with past decisions (which may have been biased).

- Algorithms for retraining a given classifier to achieve ""controlled fairness"" with respect to a choice of features and filters. Two algorithms are presented, implemented and tested. These algorithms require training two different models in two stages. We experiment with combinations of various types of models for the first and second stage and report on which combinations perform best in terms of fairness and accuracy.

- Algorithms for adjusting model parameters to achieve a notion of fairness called ""classification parity"". This notion of fairness is suitable in cases where accuracy is prioritized. Two algorithms are presented, one which assumes that protected features are accessible to the model during testing, and one which assumes protected features are not accessible during testing.

We evaluate our tools on three different publicly available datasets. We find that the tools are useful for understanding various dimensions of bias, and that in practice the algorithms are effective in starkly reducing a given observed bias when tested on new data.",,
Introduction to AI Fairness,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern.",Search,2020,2,"Yunfeng  Zhang, Rachel K. E. Bellamy, Moninder  Singh, Q. Vera Liao",CHI Extended Abstracts,,10.1145/3334480.3375059,https://doi.org/10.1145/3334480.3375059,https://semanticscholar.org/paper/d49f8d57240cd04dd66d91bf53c639497139ab3a,,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. Recently, the AI research community has proposed many methods to measure and mitigate unwanted biases, and developed open-source toolkits for developers to make fair AI. This course will cover the recent development in algorithmic fairness, including the many different definitions of fairness, their corresponding quantitative measurements, and ways to mitigate biases. This course is open to beginners and is designed for anyone interested in the topic of AI fairness.",,
Introduction to AI Fairness,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern.",Search,2021,,"Yunfeng  Zhang, Rachel K. E. Bellamy, Q. Vera Liao, Moninder  Singh",CHI Extended Abstracts,,10.1145/3411763.3444998,https://doi.org/10.1145/3411763.3444998,https://semanticscholar.org/paper/e24d854f822bebd531926eee518d4b2e1455e5de,,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. Recently, the AI research community has proposed many methods to measure and mitigate unwanted biases, and developed open-source toolkits for developers to make fair AI. This course will cover the recent development in algorithmic fairness, including the many different definitions of fairness, their corresponding quantitative measurements, and ways to mitigate biases. This course is open to beginners and is designed for anyone interested in the topic of AI fairness.",,
Design for fairness in AI: Cooking a fair AI Dish,AI development teams need support in creating more ethical AI systems.,Search,2019,1,Dasha  Simons,,,,,https://semanticscholar.org/paper/1786ae209452ad3a187c640d2fa0905a16c7d6aa,,"Artificial intelligence (AI) is an emerging field which unleashes massive new (business) opportunities. The potential growth and broad application of the AI technology has great economic benefits however also severe societal implications. Simultaneously, ethical challenges arise with its development. Questions of values and ethics are becoming urgent, as systems can be negatively biased and the decision processes are often not traceable, while impacting our lives. Abstract concepts such as fairness and values need to find their way into the fast and agile AI development processes. The contemporary (research and practice) fields tackle these challenges by technological feats, ethical AI principles and strategies. However, it are the decisions made by humans today and tomorrow that will shape our future. It is, therefore, alarming the translation of ethics to that day to day work of the AI development team is missing. Hence, the central aim of this thesis is to explore and design support for AI teams with the creation of more ethical AI systems, bridging the gap between ethical AI principles and current practice. By that, design for organizational capacity for the development of fairer AI by using strategic design and critical design approaches. In this thesis, due to the diversity and magnitude of ethical challenges in AI, particular attention is paid to two challenges, fairness and value-alignment, to benefit from a design perspective. Three streams of expertise are brought together to tackle these challenges: AI, applied ethics and design. Ethics bears critique, and this thesis argues that it can benefit from a design perspective, using imagination in the solution space and synthesized thinking for implementable ideas instead of solely discussion. The thesis focuses on ways how design approaches can supplement the ethical ones and thereby stimulate the ethical uptake in the AI field. Instead of defining what fairness is, this thesis takes a novel approach in unraveling ten unfairness sources in the AI development. It is aspired to reduce these sources of unfairness in AI, in project specific fashion. In AI practice, the ways ethics is incorporated and how value tensions are resolved is under-researched. In depth interviews, generative tools and provotypes are conducted and designed to research and critique the contemporary AI field in relation to ethics, both with IBM and their clients. Simultaneously to inquire novel value tensions in its development. Five main value tensions are unraveled in its relation to fairness. All above is consolidated a framework to design for organizational capacity and team support leading to the creation of fairer and value-aligned AI systems. With this framework an organizational role is designed, the ethical coach, to aid the AI team with cocreating fairer and value-aligned AI systems with an accompanying modular toolkit. The modular toolkit is iterated upon multiple times and uses the AI dish metaphor. Finally, two evaluation sessions with IBM and their clients as well as the conversations concerning of the implementation of the toolkit led to recommendations for further development including education and implementation directions.",,
Human-Centered Approaches to Fair and Responsible AI,The HCI community can contribute to AI systems to account for the real-world contexts in which they will be embedded.,Search,2020,5,"Min Kyung Lee, Nina  Grgić-Hlača, Michael Carl Tschantz, Reuben  Binns, Adrian  Weller, Michelle  Carney, Kori  Inkpen",CHI Extended Abstracts,,10.1145/3334480.3375158,https://doi.org/10.1145/3334480.3375158,https://semanticscholar.org/paper/7e02e25869b60f23c15bff7ffd8d406ff3321fca,,"As AI changes the way decisions are made in organizations and governments, it is ever more important to ensure that these systems work according to values that diverse users and groups find important. Researchers have proposed numerous algorithmic techniques to formalize statistical fairness notions, but emerging work suggests that AI systems must account for the real-world contexts in which they will be embedded in order to actually work fairly. These findings call for an expanded research focus beyond statistical fairness to that which includes fundamental understandings of human use and the social impact of AI systems, a theme central to the HCI community. The HCI community can contribute novel understandings, methods, and techniques for incorporating human values and cultural norms into AI systems; address human biases in developing and using AI; and empower individual users and society to audit and control AI systems. Our goal is to bring together academic and industry researchers in the fields of HCI, ML and AI, and the social sciences to devise a cross-disciplinary research agenda for fair and responsible AI systems. This workshop will build on previous algorithmic fairness workshops at AI and ML conferences, map research and design opportunities for future innovations, and disseminate them in each community.",,
FAIR AI: A Conceptual Framework for Democratisation of 21st Century AI,The lack of interpretability is a main inhibitor of broader use of 21st century AI.,Search,2021,,Saman  Halgamuge,"2021 International Conference on Instrumentation, Control, and Automation (ICA)",,10.1109/ICA52848.2021.9625672,https://doi.org/10.1109/ICA52848.2021.9625672,https://semanticscholar.org/paper/cae45a30452713cacad1e5ec04adfb4a065f6a8e,,"Popular models of AI have two significant deficiencies: 1) they are mostly manually designed using the experience of AI-experts 2) they lack human interpretability, i.e., users cannot make sense of the functionality of neural network architectures either semantically/linguistically or mathematically. This lack of interpretability is a main inhibitor of broader use of 21st century AI, e.g., Deep Neural Networks (DNN). The dependence on AI experts to create AI hinders the democratisation of AI and therefore the accessibility to AI. Addressing these deficiencies would provide answers to some of the valid questions about traceability, accountability and the ability to integrate existing knowledge (scientific or linguistically articulated human experience) into the model. This keynote abstract addresses these two significant deficiencies that inhibit the democratisation of AI by developing new methods that can automatically create interpretable neural network models without the help of AI-experts. The proposed cross-fertilised innovation will have a profound impact on the society through the increased accessibility and trustworthiness of AI beneficial to almost all areas of sciences, engineering, and humanities.",,