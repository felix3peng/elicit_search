Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Fairlearn: A toolkit for assessing and improving fairness in AI,Fairlearn is a toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems.,Search,2020,48,"Sarah  Bird, Miro  Dudík, Richard  Edgar, Brandon  Horn, Roman  Lutz, Vanessa  Milan, Mehrnoosh  Sameki, Hanna  Wallach, Kathleen  Walker",,,,,https://semanticscholar.org/paper/5894d57ea49bd5c136ebefb1e6c3986555908ea0,,"We introduce Fairlearn, an open source toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems. Fairlearn has two components: an interactive visualization dashboard and unfairness mitigation algorithms. These components are designed to help with navigating trade-offs between fairness and model performance. We emphasize that prioritizing fairness in AI systems is a sociotechnical challenge. Because there are many complex sources of unfairness—some societal and some technical—it is not possible to fully “debias” a system or to guarantee fairness; the goal is to mitigate fairness-related harms as much as possible. As Fairlearn grows to include additional fairness metrics, unfairness mitigation algorithms, and visualization capabilities, we hope that it will be shaped by a diverse community of stakeholders, ranging from data scientists, developers, and business decision makers to the people whose lives may be affected by the predictions of AI systems.",,
Transparency Tools for Fairness in AI (Luskin),"When assessing and correcting bias in AI, ""controlled fairness"" is suitable when there is no ""ground truth"" data.",Search,2020,1,"Mingliang  Chen, Aria  Shahverdi, Sarah  Anderson, Se Yong Park, Justin  Zhang, Dana  Dachman-Soled, Kristin  Lauter, Min  Wu",ArXiv,,10.1007/978-3-030-58748-2_4,https://doi.org/10.1007/978-3-030-58748-2_4,https://semanticscholar.org/paper/03e9164acb7b1cdf9b8870ede9288c816c5cf7d9,http://arxiv.org/pdf/2007.04484,"We propose new tools for policy-makers to use when assessing and correcting fairness and bias in AI algorithms. The three tools are:

- A new definition of fairness called ""controlled fairness"" with respect to choices of protected features and filters. The definition provides a simple test of fairness of an algorithm with respect to a dataset. This notion of fairness is suitable in cases where fairness is prioritized over accuracy, such as in cases where there is no ""ground truth"" data, only data labeled with past decisions (which may have been biased).

- Algorithms for retraining a given classifier to achieve ""controlled fairness"" with respect to a choice of features and filters. Two algorithms are presented, implemented and tested. These algorithms require training two different models in two stages. We experiment with combinations of various types of models for the first and second stage and report on which combinations perform best in terms of fairness and accuracy.

- Algorithms for adjusting model parameters to achieve a notion of fairness called ""classification parity"". This notion of fairness is suitable in cases where accuracy is prioritized. Two algorithms are presented, one which assumes that protected features are accessible to the model during testing, and one which assumes protected features are not accessible during testing.

We evaluate our tools on three different publicly available datasets. We find that the tools are useful for understanding various dimensions of bias, and that in practice the algorithms are effective in starkly reducing a given observed bias when tested on new data.",,
Assessing the Fairness of AI Recruitment systems,There is limited scientific research on the fairness of AI recruitment systems.,Search,2019,5,Akhil  Krishnakumar,,,,,https://semanticscholar.org/paper/eb58d3ef9847a73015b229dc7c4e414ded1a9c9e,,"Businesses have leveraged Artificial Intelligence (AI) into many of their operational activities such as marketing, sales, and finance for its speed and cost-effectiveness. Lately, AI has also found applications in organizational recruitment processes. Unlike the conventional rule-based systems, present-day AI systems learn from data patterns—supported by the growing volumes of (big) data and increasing computing capacity—and make decisions independently without any human interventions. Thus, the perception that AI is fact-oriented and unbiased has led to this change in organizational recruitment practices. Though recent studies have shown that AI decisions could be unfair, scientific research on the fairness of AI recruitment systems is limited. This research fills this gap by designing a conceptual model to assist top-level HR managers in assessing the fairness of AI recruitment tools while drawing from information systems and responsible innovation literature. Guided by Design Science Research (DSR), the development of the model entailed three cycles of research, i.e., relevance cycle (which focused on design environment), rigor cycle (which focused on the existing knowledge base), and design cycle (which focused on development and evaluation). The design environment was explored by reviewing the literature on fairness in recruitment and algorithmic biases. Understanding both the recruitment fairness and potential causes of unfairness in AI helped to define the goal of the conceptual model. The design cycle was informed by the design principles for responsible AI, namely Accountability, Responsibility, and Transparency (ART), and General Data Protection Regulation (GDPR). The model presents seven dimensions which translate the principles to design requirements to assess the fairness of AI recruitment system. They are: (1)Justification; (2)Explanation; (3)Anticipation; (4)Reflexiveness; (5)Inclusion; (6)Responsiveness; and (7)Auditablity. The model also ties these concepts with specific criteria of conventional recruitment fairness such as consistency, interpersonal fairness, job-relatedness, and statistical parity. Finally, the completeness of the model was evaluated by discussing its alignment with other frameworks that had similar objective and utility of the model was validated by collecting feedback from the intended users. This thesis project makes several scientific and practical contributions. The research discusses the potential risks of using AI in the context of HR recruitment systems thereby contributes to the limited literature available in this respect. By using the DSR methodology for building the assessment model, this research serves as a case for DSR methodology in designing a non-IS artifact. Furthermore, the thesis has unified scattered studies in recruitment justice to provide a comprehensive overview of the characteristics of a fair recruitment system. Building on the theoretical contributions, the study has developed an assessment model to assist top-level HR managers in assessing the fairness of an AI recruitment tool. Employing this assessment tool can have positive effects on a business organization and society by eradicating the unfairness or bias that AI recruitment tools can bring into the organization. It would also raise awareness regarding the risks of AI. Given that the GDPR (article 35) mandate organizations to take responsibility in assessing the impact while introducing automated processing in new contexts or purposes, the assessment model designed in this study supports these regulations.",,Review
AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias,"A comprehensive set of metrics, explanations, and algorithms to mitigate bias in datasets and models is included in the AI Fairness 360 toolkit.",Search,2019,113,"Rachel K. E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Jacquelyn A. Martino, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",IBM J. Res. Dev.,,10.1147/jrd.2019.2942287,https://doi.org/10.1147/jrd.2019.2942287,https://semanticscholar.org/paper/333671a5fbbf726f8819138f3670524ec0405726,,"Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This article introduces a new open-source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license (

https://github.com/ibm/aif360

). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms for use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience that provides a gentle introduction to the concepts and capabilities for line-of-business users, researchers, and developers to extend the toolkit with their new algorithms and improvements and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.",,
Design Methods for Artificial Intelligence Fairness and Transparency,"Fairness and transparency in artificial intelligence continue to become more prevalent topics for research, design and development.",Search,2021,,"Simone  Stumpf, Lorenzo  Strappelli, Subeida  Ahmed, Yuri  Nakao, Aisha  Naseer, Giulia Del Gamba, Daniele  Regoli",IUI Workshops,,,,https://semanticscholar.org/paper/baf8cb94908738c3b2d040a6b44d812e5ca80396,,"Fairness and transparency in artificial intelligence (AI) continue to become more prevalent as topics for research, design and development. General principles and guidelines for designing ethical and responsible AI systems have been proposed, yet there is a lack of design methods for these kinds of systems. In this paper, we present CoFAIR, a novel method to design user interfaces for exploring fairness, consisting of series of co-design workshops, and wider evaluation. This method can be readily applied in practice by researchers, designers and developers to create responsible and ethical AI systems.",,
"AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias",AI Fairness 360 is a toolkit to facilitate the transition of fairness research algorithms to use in an industrial setting.,Search,2018,316,"Rachel K. E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Jacquelyn  Martino, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",ArXiv,,,,https://semanticscholar.org/paper/c8541b1dc813f3a638d7acc79e5f972e77f3c5a7,,"Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license {this https URL). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms.

The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (this https URL) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.",,
"AI Evaluation: past, present and future",AI systems are becoming more complex and unpredictable.,Search,2014,24,José  Hernández-Orallo,ArXiv,,,,https://semanticscholar.org/paper/8f2d78fb9293eac038a6a431d23ca5a18df7bfb4,,"Artificial intelligence develops techniques and systems whose performance must be evaluated on a regular basis in order to certify and foster progress in the discipline. We will describe and critically assess the different ways AI systems are evaluated. We first focus on the traditional task-oriented evaluation approach. We see that black-box (behavioural evaluation) is becoming more and more common, as AI systems are becoming more complex and unpredictable. We identify three kinds of evaluation: Human discrimination, problem benchmarks and peer confrontation. We describe the limitations of the many evaluation settings and competitions in these three categories and propose several ideas for a more systematic and robust evaluation. We then focus on a less customary (and challenging) ability-oriented evaluation approach, where a system is characterised by its (cognitive) abilities, rather than by the tasks it is designed to solve. We discuss several possibilities: the adaptation of cognitive tests used for humans and animals, the development of tests derived from algorithmic information theory or more general approaches under the perspective of universal psychometrics.",,
Fairness Score and Process Standardization: Framework for Fairness Certification in Artificial Intelligence Systems,A Fairness Score and a Standard Operating Procedure (SOP) for issuing Fairness Certification for AI systems would improve the trustworthiness of AI systems.,Search,2022,,"Avinash  Agarwal, Harsh  Agarwal, Nihaarika  Agarwal",ArXiv,,,,https://semanticscholar.org/paper/64c2fa6b3c161fe2a5ae20f03ff77c3250027e10,,"Decisions made by various Artificial Intelligence (AI) systems greatly influence our day-to-day lives. With the increasing use of AI systems, it becomes crucial to know that they are fair, identify the underlying biases in their decision-making, and create a standardized framework to ascertain their fairness. In this paper, we propose a novel Fairness Score to measure the fairness of a data-driven AI system and a Standard Operating Procedure (SOP) for issuing Fairness Certification for such systems. Fairness Score and audit process standardization will ensure quality, reduce ambiguity, enable comparison and improve the trustworthiness of the AI systems. It will also provide a framework to operationalise the concept of fairness and facilitate the commercial deployment of such systems. Furthermore, a Fairness Certificate issued by a designated third-party auditing agency following the standardized process would boost the conviction of the organizations in the AI systems that they intend to deploy. The Bias Index proposed in this paper also reveals comparative bias amongst the various protected attributes within the dataset. To substantiate the proposed framework, we iteratively train a model on biased and unbiased data using multiple datasets and check that the Fairness Score and the proposed process correctly identify the biases and judge the fairness.",,
Cognitive and Emotional Response to Fairness in AI – A Systematic Review,The individual perception of fairness in AI applications is strongly context-dependent.,Search,2019,3,"Janine  Baleis, Birte  Keller, Christopher  Starke, Frank  Marcinkowski",,,,,https://semanticscholar.org/paper/b5a92d0fda53c4720f00c3d75682f665c6ceb6a4,,"Artificial intelligence is increasingly used to make decisions that can have a significant impact on people's lives. These decisions can disadvantage certain groups of individuals. A central question that follows is the feasibility of justice in AI applications. Therefore, it should be considered which demands such applications have to meet and where the transfer of social order to algorithmic contexts still needs to be overhauled. Previous research efforts in the context of discrimination come from different disciplines and shed light on problems from specific perspectives on the basis of various definitions. An interdisciplinary approach to this topic is still lacking, which is why it is considered sensible to systematically summarise research findings across disciplines in order to find parallels and combine common fairness requirements. This endeavour is the aim of this paper. As a result of the systematic review, it can be stated that the individual perception of fairness in AI applications is strongly context-dependent. In addition, transparency, trust and individual moral concepts demonstrably have an influence on the individual perception of fairness in AI applications. Within the interdisciplinary scientific discourse, fairness is conceptualized by various definitions, which is why there is no consensus on a uniform definition of fairness in the scientific literature to date.",,Systematic Review
AI evaluation campaigns during robotics competitions: the METRICS paradigm,"Artificial intelligence and robotics competitions are usually organized independently, in silos.",Search,2020,1,"Guillaume  Avrin, Virginie  Barbosa, Agnes  Delaborde",,,,,https://semanticscholar.org/paper/32ee8d816f00c54d25a20c92fa89a5194049c611,,"Competitions are a proven and cost-effective method to quickly develop new disruptive technologies for new markets. However, artificial intelligence and robotics competitions are usually organized independently, in silos. To get the most out of competitions, evaluation of robots should be modular (tasks are designed to assess independently different technological building blocks of the robotic systems) and open (evaluation tools and data must be publicly and freely available online). This not only meets the benchmarking needs of end users (mostly interested in performance measurements of the complete system), but also those of integrators and developers of intelligent components (camera, lidar, radar, actuators, etc.).

With this objective in mind, the H2020 METRICS project (2020-2023) organizes competitions in four application areas (Healthcare, Infrastructure Inspection and Maintenance, Agri-Food, and Agile Production) relying on both physical testing facilities (field evaluation campaign) and virtual testing facilities (data-based evaluation campaign) to mobilize, in addition to the European robotics community, the artificial intelligence one.

This article presents this approach, which aims to go beyond the METRICS project and pave the way for a new robotics and artificial intelligence competition paradigm.",,
Towards the Right Kind of Fairness in AI,Choosing the right kind of fairness for a given AI system depends on ethical standards and legal requirements.,Search,2021,2,"Boris  Ruf, Marcin  Detyniecki",ArXiv,,,,https://semanticscholar.org/paper/58e8e3d258dc656d9ef90f7468eeff7d7a513e89,,"Fairness is a concept of justice. Various definitions exist, some of them conflicting with each other. In the absence of an uniformly accepted notion of fairness, choosing the right kind for a specific situation has always been a central issue in human history. When it comes to implementing sustainable fairness in artificial intelligence systems, this old question plays a key role once again: How to identify the most appropriate fairness metric for a particular application? The answer is often a matter of context, and the best choice depends on ethical standards and legal requirements. Since ethics guidelines on this topic are kept rather general for now, we aim to provide more hands-on guidance with this document. Therefore, we first structure the complex landscape of existing fairness metrics and explain the different options by example. Furthermore, we propose the “Fairness Compass”, a tool which formalises the selection process and makes identifying the most appropriate fairness definition for a given system a simple, straightforward procedure. Because this process also allows to document the reasoning behind the respective decisions, we argue that this approach can help to build trust from the user through explaining and justifying the implemented fairness. ∗{boris.ruf,marcin.detyniecki}@axa.com 1 ar X iv :2 10 2. 08 45 3v 6 [ cs .A I] 2 7 A ug 2 02 1",,
The Myth of Complete AI-Fairness,Nothing is ever completely fair in all situations.,Search,2021,1,Virginia  Dignum,AIME,,10.1007/978-3-030-77211-6_1,https://doi.org/10.1007/978-3-030-77211-6_1,https://semanticscholar.org/paper/62b5415c382c4b6584789a51262e1e902cb07998,http://arxiv.org/pdf/2104.12544,"Just recently, IBM invited me to participate in a panel titled “Will AI ever be completely fair?” My first reaction was that it surely would be a very short panel, as the only possible answer is ‘no’. In this short paper, I wish to further motivate my position in that debate: “I will never be completely fair. Nothing ever is. The point is not complete fairness, but the need to establish metrics and thresholds for fairness that ensure trust in AI systems”. The idea of fairness and justice has long and deep roots in Western civilization, and is strongly linked to ethics. It is therefore not strange that it is core to the current discussion about the ethics of development and use of AI systems. Given that we often associate fairness with consistency and accuracy, the idea that our decisions and decisions affecting us can become fairer by replacing human judgement by automated, numerical, systems, is therefore appealing. However, as Laurie Anderson recently said “If you think technology will solve your problems, you don’t understand technology — and you don’t understand your problems.” AI is not magic, and its results are fundamentally constrained by the convictions and expectations of those that build, manage, deploy and use it. Which makes it crucial that we understand the mechanisms behind the systems and their decision capabilities. The pursuit of fair AI is currently a lively one. One involving many researchers, meetings and conferences (of which FAccT is the most known) and refers to the notion that an algorithm is fair, if its results are independent of given variables, especially those considered sensitive, such as the traits of individuals which should not correlate with the outcome (i.e. gender, ethnicity, sexual orientation, disability, etc.). However, nothing is ever 100% fair in 100% of the situations, and due to complex networked connection, to ensure fairness for one (group) may lead to unfairness for others. Moreover, what we consider fair often does depend on the traits of individuals. An obvious example are social services. Most people believe in the need for some form of social services, whether it is for children, for the elderly, for the sick or the poor. And many of us will benefit from social services at least once in our lives. Decision making in the attribution of social benefits is dependent on individual characteristics such as",,
Introduction to AI Fairness,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern.",Search,2020,2,"Yunfeng  Zhang, Rachel K. E. Bellamy, Moninder  Singh, Q. Vera Liao",CHI Extended Abstracts,,10.1145/3334480.3375059,https://doi.org/10.1145/3334480.3375059,https://semanticscholar.org/paper/d49f8d57240cd04dd66d91bf53c639497139ab3a,,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. Recently, the AI research community has proposed many methods to measure and mitigate unwanted biases, and developed open-source toolkits for developers to make fair AI. This course will cover the recent development in algorithmic fairness, including the many different definitions of fairness, their corresponding quantitative measurements, and ways to mitigate biases. This course is open to beginners and is designed for anyone interested in the topic of AI fairness.",,
Introduction to AI Fairness,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern.",Search,2021,,"Yunfeng  Zhang, Rachel K. E. Bellamy, Q. Vera Liao, Moninder  Singh",CHI Extended Abstracts,,10.1145/3411763.3444998,https://doi.org/10.1145/3411763.3444998,https://semanticscholar.org/paper/e24d854f822bebd531926eee518d4b2e1455e5de,,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. Recently, the AI research community has proposed many methods to measure and mitigate unwanted biases, and developed open-source toolkits for developers to make fair AI. This course will cover the recent development in algorithmic fairness, including the many different definitions of fairness, their corresponding quantitative measurements, and ways to mitigate biases. This course is open to beginners and is designed for anyone interested in the topic of AI fairness.",,
A proposed framework for UX evaluation of artificial intelligence services,Artificial intelligence services require different user experience evaluations than traditional services.,Search,2021,,"Su-Jin  Hur, Sung-Hee  Kim",,,,,https://semanticscholar.org/paper/bd5f95d7d1c5b8978f21207540d2efcbfcd3c4a2,,"As artificial intelligence develops rapidly, we can experience it in our everyday life such as with medical, education, and game applications. Traditional SW services were programmed explicitly by the intention of the programmer, and we have conducted evaluation on it. However, due to the uncertianty of AI services, risk follows to the products. Therefore, UX evaluations need to be different from traditional UX evaluations. Therefore, in this paper we suggest a AI-UX framework that consideres the task delegability, UX evaluations metrics, and individual differences.",,
Toward AI research methodology: three case studies in evaluation,"The roles of evaluation in empirical artificial intelligence (AI) research are described, in an idealized cyclic model and in the context of three case studies.",Search,1989,55,"Paul R. Cohen, Adele E. Howe",IEEE Trans. Syst. Man Cybern.,,10.1109/21.31069,https://doi.org/10.1109/21.31069,https://semanticscholar.org/paper/06e8b97451e9be65bd96eab7cb1ffe75dc3b082a,,"The roles of evaluation in empirical artificial intelligence (AI) research are described, in an idealized cyclic model and in the context of three case studies. The case studies illustrate the pitfalls in evaluation and the contributions of evaluation at all stages of the research cycle. Evaluation methods are contrasted with those of the behavioral sciences, and it is concluded that AI must define and refine its own methods. To this end, several experiment schemas and many specific evaluation criteria are described. Recommendations are offered in the hope of encouraging the development and practice of evaluation methods in AI. The first case study illustrates problems with evaluating knowledge-based systems, specifically a portfolio management expert system called FOLIO. The second study focuses on the relationship between evaluation and the evolution of the GRANT system, specifically, how the evaluations changed as GRANT's knowledge base was sealed up. Third, the cyclic nature of a given research model is examined. >",,