Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Safe and sound - artificial intelligence in hazardous applications,Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making.,Search,2000,272,"John  Fox, Subrata Kumar Das",,,,,https://semanticscholar.org/paper/f62d215d4c40d299929d2e7fa105d54623a9e1bf,,"Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making, where small faults or errors can spell human catastrophe. This book describes, from both practical and theoretical perspectives, an AI technology for supporting sound clinical decision making and safe patient management. The technology combines techniques from conventional software engineering with a systematic method for building intelligent agents. Although the focus is on medicine, many of the ideas can be applied to AI systems in other hazardous settings. The book also covers a number of general AI problems, including knowledge representation and expertise modeling, reasoning and decision making under uncertainty, planning and scheduling, and the design and implementation of intelligent agents.The book, written in an informal style, begins with the medical background and motivations, technical challenges, and proposed solutions. It then turns to a wide-ranging discussion of intelligent and autonomous agents, with particular reference to safety and hazard management. The final section provides a detailed discussion of the knowledge representation and other aspects of the agent model developed in the book, along with a formal logical semantics for the language.",,
Guidelines for Artificial Intelligence Containment,"Safety container software can contain the information leakage, social engineering attacks, and cyberattacks from within the container.",Search,2019,20,"James  Babcock, János  Kramár, Roman V. Yampolskiy",Next-Generation Ethics,,10.1017/9781108616188.008,https://doi.org/10.1017/9781108616188.008,https://semanticscholar.org/paper/b0c820f9d7fa4d8c56a7887548834b7ba65ddfa5,http://www.vetta.org/documents/Machine_Super_Intelligence.pdf,"With almost daily improvements in capabilities of artificial intelligence it is more important than ever to develop safety software for use by the AI research community. Building on our previous work on AI Containment Problem we propose a number of guidelines which should help AI safety researchers to develop reliable sandboxing software for intelligent programs of all levels. Such safety container software will make it possible to study and analyze intelligent artificial agent while maintaining certain level of safety against information leakage, social engineering attacks and cyberattacks from within the container.",,
Governing the safety of artificial intelligence in healthcare,Artificial intelligence technologies will introduce new risks to healthcare and amplify existing ones.,Search,2019,28,Carl  Macrae,BMJ Quality & Safety,,10.1136/bmjqs-2019-009484,https://doi.org/10.1136/bmjqs-2019-009484,https://semanticscholar.org/paper/b91e4c2234c950a8a639ba1d3f97dcd51220520a,https://nottingham-repository.worktribe.com/preview/2062727/MacraeGoverningAIsafety.pdf,"Artificial intelligence (AI) has enormous potential to improve the safety of healthcare, from increasing diagnostic accuracy,1 to optimising treatment planning,2 to forecasting outcomes of care.3 However, integrating AI technologies into the delivery of healthcare is likely to introduce a range of new risks and amplify existing ones. For instance, failures in widely used software have the potential to quickly affect large numbers of patients4; hidden assumptions in underlying data and models can lead to AI systems delivering dangerous recommendations that are insensitive to local care processes,5 6 and opaque AI techniques such as deep learning can make explaining and learning from failure extremely difficult.7 8 To maximise the benefits of AI in healthcare and to build trust among patients and practitioners, it will therefore be essential to robustly govern the risks that AI poses to patient safety.

In a recent review in this journal, Challen and colleagues present an important and timely analysis of some of the key technological risks associated with the application of machine learning in clinical settings.9 Machine learning is a subfield of AI that focuses on the development of algorithms that are automatically derived and optimised through exposure to large quantities of exemplar ‘training’ data.10 The outputs of machine learning algorithms are essentially classifications of patterns that provide some sort of prediction—for instance, predicting whether an image shows a malignant melanoma or a benign mole.11 Some of the basic techniques of machine learning have existed for half a century or more, but progress in the field has accelerated rapidly due to advances in the development of ‘deep’ artificial neural networks12 combined with huge increases in computational power and the availability of enormous quantities of data. These techniques have underpinned recent public demonstrations of AI systems …",,Review
Human factors challenges for the safe use of artificial intelligence in patient care,Artificial intelligence should be developed with human factors research to ensure safe use in patient care.,Search,2019,21,"Mark  Sujan, Dominic  Furniss, Kath  Grundy, Howard  Grundy, David  Nelson, Matthew  Elliott, Sean  White, Ibrahim  Habli, Nick  Reynolds",BMJ Health & Care Informatics,,10.1136/bmjhci-2019-100081,https://doi.org/10.1136/bmjhci-2019-100081,https://semanticscholar.org/paper/9a99e50b44f176ea20ed40fe90b00f4f57762666,https://informatics.bmj.com/content/bmjhci/26/1/e100081.full.pdf,"The use of artificial intelligence (AI) in patient care can offer significant benefits. However, there is a lack of independent evaluation considering AI in use. The paper argues that consideration should be given to how AI will be incorporated into clinical processes and services. Human factors challenges that are likely to arise at this level include cognitive aspects (automation bias and human performance), handover and communication between clinicians and AI systems, situation awareness and the impact on the interaction with patients. Human factors research should accompany the development of AI from the outset.",,
Towards Safe Artificial General Intelligence,"If humanity would find itself in conflict with a system of much greater intelligence than itself, then human society would likely lose.",Search,2018,21,Tom  Everitt,,,10.25911/5D134A2F8A7D3,https://doi.org/10.25911/5D134A2F8A7D3,https://semanticscholar.org/paper/8b1e149ae23ea7839c9e3a2bd063c354ff7075d0,,"The field of artificial intelligence has recently experienced a number of breakthroughs thanks to progress in deep learning and reinforcement learning. Computer algorithms now outperform humans at Go, Jeopardy, image classification, and lip reading, and are becoming very competent at driving cars and interpreting natural language. The rapid development has led many to conjecture that artificial intelligence with greater-thanhuman ability on a wide range of tasks may not be far. This in turn raises concerns whether we know how to control such systems, in case we were to successfully build them. Indeed, if humanity would find itself in conflict with a system of much greater intelligence than itself, then human society would likely lose. One way to make sure we avoid such a conflict is to ensure that any future AI system with potentially greater-thanhuman-intelligence has goals that are aligned with the goals of the rest of humanity. For example, it should not wish to kill humans or steal their resources. The main focus of this thesis will therefore be goal alignment, i.e. how to design artificially intelligent agents with goals coinciding with the goals of their designers. Focus will mainly be directed towards variants of reinforcement learning, as reinforcement learning currently seems to be the most promising path towards powerful artificial intelligence. We identify and categorize goal misalignment problems in reinforcement learning agents as designed today, and give examples of how these agents may cause catastrophes in the future. We also suggest a number of reasonably modest modifications that can be used to avoid or mitigate each identified misalignment problem. Finally, we also study various choices of decision algorithms, and conditions for when a powerful reinforcement learning system will permit us to shut it down. The central conclusion is that while reinforcement learning systems as designed today are inherently unsafe to scale to human levels of intelligence, there are ways to potentially address many of these issues without straying too far from the currently so successful reinforcement learning paradigm. Much work remains in turning the high-level proposals suggested in this thesis into practical algorithms, however. Central claim: There are a number of theoretically valid, partial solutions to the problem of keeping artificial general intelligence both safe and useful.",,
"Human-Centered Artificial Intelligence: Reliable, Safe & Trustworthy",Well-designed technologies that offer high levels of human control and high levels of computer automation can increase human performance.,Search,2020,105,Ben  Shneiderman,Int. J. Hum. Comput. Interact.,,10.1080/10447318.2020.1741118,https://doi.org/10.1080/10447318.2020.1741118,https://semanticscholar.org/paper/e49f67fa5c946ad24afcf59699a9cacf1ca53924,http://arxiv.org/pdf/2002.04087,"ABSTRACT Well-designed technologies that offer high levels of human control and high levels of computer automation can increase human performance, leading to wider adoption. The Human-Centered Artificial Intelligence (HCAI) framework clarifies how to (1) design for high levels of human control and high levels of computer automation so as to increase human performance, (2) understand the situations in which full human control or full computer control are necessary, and (3) avoid the dangers of excessive human control or excessive computer control. The methods of HCAI are more likely to produce designs that are Reliable, Safe & Trustworthy (RST). Achieving these goals will dramatically increase human performance, while supporting human self-efficacy, mastery, creativity, and responsibility.",,
Artificial intelligence in health care: accountability and safety,There's a need to include artificial intelligence developers and systems safety engineers in assessments of moral accountability for patient harm.,Search,2020,27,"Ibrahim  Habli, Tom  Lawton, Zoe  Porter",Bulletin of the World Health Organization,,10.2471/BLT.19.237487,https://doi.org/10.2471/BLT.19.237487,https://semanticscholar.org/paper/a132e66655f9d4f8edba54df053c6656c4c617be,https://europepmc.org/articles/pmc7133468?pdf=render,"Abstract The prospect of patient harm caused by the decisions made by an artificial intelligence-based clinical tool is something to which current practices of accountability and safety worldwide have not yet adjusted. We focus on two aspects of clinical artificial intelligence used for decision-making: moral accountability for harm to patients; and safety assurance to protect patients against such harm. Artificial intelligence-based tools are challenging the standard clinical practices of assigning blame and assuring safety. Human clinicians and safety engineers have weaker control over the decisions reached by artificial intelligence systems and less knowledge and understanding of precisely how the artificial intelligence systems reach their decisions. We illustrate this analysis by applying it to an example of an artificial intelligence-based system developed for use in the treatment of sepsis. The paper ends with practical suggestions for ways forward to mitigate these concerns. We argue for a need to include artificial intelligence developers and systems safety engineers in our assessments of moral accountability for patient harm. Meanwhile, none of the actors in the model robustly fulfil the traditional conditions of moral accountability for the decisions of an artificial intelligence system. We should therefore update our conceptions of moral accountability in this context. We also need to move from a static to a dynamic model of assurance, accepting that considerations of safety are not fully resolvable during the design of the artificial intelligence system before the system has been deployed.",,
How to achieve trustworthy artificial intelligence for health,"The EU's guidance leaves room for local, contextualized discretion in the global health sector.",Search,2020,15,"Kristine  Bærøe, Ainar  Miyata-Sturm, Edmund  Henden",Bulletin of the World Health Organization,,10.2471/BLT.19.237289,https://doi.org/10.2471/BLT.19.237289,https://semanticscholar.org/paper/0a3c92d6c4fa4670743fb13758916067e772a143,https://europepmc.org/articles/pmc7133476?pdf=render,"Abstract Artificial intelligence holds great promise in terms of beneficial, accurate and effective preventive and curative interventions. At the same time, there is also awareness of potential risks and harm that may be caused by unregulated developments of artificial intelligence. Guiding principles are being developed around the world to foster trustworthy development and application of artificial intelligence systems. These guidelines can support developers and governing authorities when making decisions about the use of artificial intelligence. The High-Level Expert Group on Artificial Intelligence set up by the European Commission launched the report Ethical guidelines for trustworthy artificial intelligence in2019. The report aims to contribute to reflections and the discussion on the ethics of artificial intelligence technologies also beyond the countries of the European Union (EU). In this paper, we use the global health sector as a case and argue that the EU’s guidance leaves too much room for local, contextualized discretion for it to foster trustworthy artificial intelligence globally. We point to the urgency of shared globalized efforts to safeguard against the potential harms of artificial intelligence technologies in health care.",,
Building Safer AGI by introducing Artificial Stupidity,Artificial Stupidity can make Artificial General Intelligence safer by limiting its computing power and memory.,Search,2018,18,"Michaël  Trazzi, Roman V. Yampolskiy",ArXiv,,,,https://semanticscholar.org/paper/feef1ddc618a406e4b125b51ad7de0505516c8e6,,"Artificial Intelligence (AI) achieved super-human performance in a broad variety of domains. We say that an AI is made Artificially Stupid on a task when some limitations are deliberately introduced to match a human's ability to do the task. An Artificial General Intelligence (AGI) can be made safer by limiting its computing power and memory, or by introducing Artificial Stupidity on certain tasks. We survey human intellectual limits and give recommendations for which limits to implement in order to build a safe AGI.",,
"Safe and Sound: Artificial Intelligence in Hazardous Applications - John Fox, Subrata Das, AAAI Press, Menlo Park, CA, and MIT Press, Cambridge, MA/London, UK, 2000, 326 pp., References, Index, Illus., ISBN 0-262-06211-9",Artificial intelligence techniques can solve difficult problems in a more satisfactory way than conventional software.,Search,2003,2,Mar  Marcos,Artif. Intell. Medicine,,10.1016/S0933-3657(02)00082-9,https://doi.org/10.1016/S0933-3657(02)00082-9,https://semanticscholar.org/paper/0ab5618de22bfdb929e3a79248040d0476da2ea3,,"Since the inception of the Artificial Intelligence (AI) discipline, there have been plenty of developments of systems that can be labeled as intelligent. Intelligent systems have the potential of solving difficult problems in a more satisfactory way than conventional software would do, usually making use of heuristics or knowledge that the experts in the domain posses. As AI techniques improve, the range of tasks that intelligent systems are able to perform will grow. This will make them potentially suitable for a higher number of application fields. However, many fields are inherently safety-critical. If we want to put intelligent systems into use in those fields we should pay special attention to safety issues. In a scenario where an intelligent system operates in a complex environment, safety is mainly concerned with ensuring that the system responds adequately to any hazard that might arise, e.g. due to unexpected interactions of the actions it has performed. The book by John Fox and Subrata Das presents an approach to the improvement of safety in AI systems, combining ideas from software engineering, conventional system engineering and AI. It draws lessons from the extensive experience of the authors in the development of medical AI applications. Concerning the management of hazards, their proposal consists in using AI techniques similar to the ones employed for problem-solving. If the complexity of the environment makes it impossible to predict all possible hazards, as it is the case in Medicine, a promising alternative is to provide the system with additional capabilities to dynamically reason about hazards. As the title suggests, the book addresses the problem of making intelligent systems sound and safe. According to this, the two main topics treated in the book are, respectively, how to design intelligent systems that are able to perform complex, knowledge-intensive tasks, and how to make them safe. Both topics are illustrated with numerous examples from the field of Medicine. Nevertheless, the focus of attention is both on the practical achievements and the underlying theoretical framework. The book is structured in three parts. Part I introduces PROforma, a methodology and a technology for building intelligent systems. Part II discusses the safety issues that arise when deploying intelligent systems in complex and changing environments. Finally, Part III describes the theoretical concepts underlying the PROforma technology. A feature of this book is that there are different possible ways to read it, depending on the interest of the reader in the theoretical details of Part III. For those readers mainly interested in this part, a summary of the first two parts is Artificial Intelligence in Medicine 27 (2003) 103–106",,
Application of Artificial Intelligence in the Health Care Safety Context: Opportunities and Challenges,Artificial intelligence can provide opportunities in diagnosis and treatment processes but may face safety challenges.,Search,2019,28,"Samer  Ellahham, Nour  Ellahham, Mecit Can Emre Simsekler",American journal of medical quality : the official journal of the American College of Medical Quality,,10.1177/1062860619878515,https://doi.org/10.1177/1062860619878515,https://semanticscholar.org/paper/a40761088d3afd1ad8fc698f813fe2c06d99c528,,"There is a growing awareness that artificial intelligence (AI) has been used in the analysis of complicated and big data to provide outputs without human input in various health care contexts, such as bioinformatics, genomics, and image analysis. Although this technology can provide opportunities in diagnosis and treatment processes, there still may be challenges and pitfalls related to various safety concerns. To shed light on such opportunities and challenges, this article reviews AI in health care along with its implication for safety. To provide safer technology through AI, this study shows that safe design, safety reserves, safe fail, and procedural safeguards are key strategies, whereas cost, risk, and uncertainty should be identified for all potential technical systems. It is also suggested that clear guidance and protocols should be identified and shared with all stakeholders to develop and adopt safer AI applications in the health care context.",,Review
A Framework for Safeguarding Artificial Intelligence Systems Within Healthcare Domain,There is a lack of understanding around how some of these algorithms work.,Search,2019,4,Avishek  Choudhury,British Journal of Healthcare Management,,10.12968/BJHC.2019.0066,https://doi.org/10.12968/BJHC.2019.0066,https://semanticscholar.org/paper/795bea6fb6a899805edf40140a8415fae5f68f7c,,"In healthcare, research on artificial intelligence is becoming increasingly dedicated to applying predictive analytic techniques to make clinical predictions. Even though artificial intelligence has shown promising results in cancer image recognition, triage service automation, and in disease prognosis, its clinical value has not been addressed. Currently, there is a lack of understanding around how some of these algorithms work. Despite knowing the potential risks associated with using artificial intelligence in healthcare, there is no clear framework to evaluate predictive algorithms, which are being commercially implemented within the healthcare industry. To ensure patient safety, regulatory authorities should ensure that proposed algorithms meet the accepted standards of clinical benefit, just as they do for therapeutics and predictive biomarkers. In this article, we offer a framework for the evaluation of predictive algorithms. Although not exhaustive, these criteria can enhance the quality of predictive algorithms and ensure that the algorithms effectively improve clinical outcomes.",,
"Artificial intelligence, bias and clinical safety",Artificial intelligence research is becoming increasingly focused on applying machine learning techniques to complex problems.,Search,2019,237,"Robert  Challen, Joshua  Denny, Martin  Pitt, Luke  Gompels, Tom  Edwards, Krasimira  Tsaneva-Atanasova",BMJ Quality & Safety,,10.1136/bmjqs-2018-008370,https://doi.org/10.1136/bmjqs-2018-008370,https://semanticscholar.org/paper/6e23ae3969078ecd6e59260a895c96c360b4921a,https://qualitysafety.bmj.com/content/qhc/28/3/231.full.pdf,"In medicine, artificial intelligence (AI) research is becoming increasingly focused on applying machine learning (ML) techniques to complex problems, and so allowing computers to make predictions from large amounts of patient data, by learning their own associations.1 Estimates of the impact of AI on the wider economy globally vary wildly, with a recent report suggesting a 14% effect on global gross domestic product by 2030, half of which coming from productivity improvements.2 These predictions create political appetite for the rapid development of the AI industry,3 and healthcare is a priority area where this technology has yet to be exploited.2 3 The digital health revolution described by Duggal et al 4 is already in full swing with the potential to ‘disrupt’ healthcare. Health AI research has demonstrated some impressive results,5–10 but its clinical value has not yet been realised, hindered partly by a lack of a clear understanding of how to quantify benefit or ensure patient safety, and increasing concerns about the ethical and medico-legal impact.11

This analysis is written with the dual aim of helping clinical safety professionals to critically appraise current medical AI research from a quality and safety perspective, and supporting research and development in AI by highlighting some of the clinical safety questions that must be considered if medical application of these exciting technologies is to be successful.

Clinical decision support systems (DSS) are in widespread use in medicine and have had most impact providing guidance on the safe prescription of medicines,12 guideline adherence, simple risk screening13 or prognostic scoring.14 These systems use predefined rules, which have predictable behaviour and are usually shown to reduce clinical error,12 although sometimes inadvertently introduce safety issues themselves.15 16 Rules-based systems have also been developed to address diagnostic uncertainty17–19 …",,
Medical ethics considerations on artificial intelligence,,Search,2019,41,Kadircan H. Keskinbora,Journal of Clinical Neuroscience,,10.1016/j.jocn.2019.03.001,https://doi.org/10.1016/j.jocn.2019.03.001,https://semanticscholar.org/paper/bc6519025efcd4db20267691e0b6eabfe44dac95,,"Artificial intelligence (AI) is currently one of the mostly controversial matters of the world. This article discusses AI in terms of the medical ethics issues involved, both existing and potential. Once artificial intelligence is fully developed within electronic systems, it will afford many useful applications in many sectors ranging from banking, agriculture, medical procedures to military operations, especially by decreasing the involvement of humans in critically dangerous activities. Robots as well as computers themselves are embodiments of values inasmuch as they entail actions and choices, but their practical applications are modelled or programmed by the engineers building the systems. AI will need algorithmic procedures to ensure safety in the implementation of such systems. The AI algorithms written could naturally contain errors that may result in unforeseen consequences and unfair outcomes along economic and racial class lines. It is crucial that measures be taken to monitor technological developments ensuring preventative and precautionary safeguards are in place to safeguard the rights of those involved against direct or indirect coercion. While it is the responsibility of AI researchers to ensure that the future impact is more positive than negative, ethicists and philosophers need to be deeply involved in the development of such technologies from the beginning.",,
Artificial intelligence in medical diagnosis.,Computer programs that simulate expert human reasoning do not show clinically useful results.,Search,1988,166,"P  Szolovits, R S Patil, W B Schwartz",Annals of internal medicine,,10.7326/0003-4819-108-1-80,https://doi.org/10.7326/0003-4819-108-1-80,https://semanticscholar.org/paper/2640a33d0b8f3cac8942ee5e60498d482b0c10d9,,"In an attempt to overcome limitations inherent in conventional computer-aided diagnosis, investigators have created programs that simulate expert human reasoning. Hopes that such a strategy would lead to clinically useful programs have not been fulfilled, but many of the problems impeding creation of effective artificial intelligence programs have been solved. Strategies have been developed to limit the number of hypotheses that a program must consider and to incorporate pathophysiologic reasoning. The latter innovation permits a program to analyze cases in which one disorder influences the presentation of another. Prototypes embodying such reasoning can explain their conclusions in medical terms that can be reviewed by the user. Despite these advances, further major research and developmental efforts will be necessary before expert performance by the computer becomes a reality.",,
Enabling artificial intelligence in high acuity medical environments,AI-based assistive and decision support systems can improve patient outcomes and increase efficiency.,Search,2019,6,"Martin  Kasparick, Björn  Andersen, Stefan  Franke, Max  Rockstroh, Frank  Golatowski, Dirk  Timmermann, Josef  Ingenerf, Thomas  Neumuth",Minimally invasive therapy & allied technologies : MITAT : official journal of the Society for Minimally Invasive Therapy,,10.1080/13645706.2019.1599957,https://doi.org/10.1080/13645706.2019.1599957,https://semanticscholar.org/paper/97519c6303e0133fbdf188c0ce94ef2db1f5192d,,"Abstract Acute patient treatment can heavily profit from AI-based assistive and decision support systems, in terms of improved patient outcome as well as increased efficiency. Yet, only very few applications have been reported because of the limited accessibility of device data due to the lack of adoption of open standards, and the complexity of regulatory/approval requirements for AI-based systems. The fragmentation of data, still being stored in isolated silos, results in limited accessibility for AI in healthcare and machine learning is complicated by the loss of semantics in data conversions. We outline a reference model that addresses the requirements of innovative AI-based research systems as well as the clinical reality. The integration of networked medical devices and Clinical Repositories based on open standards, such as IEEE 11073 SDC and HL7 FHIR, will foster novel assistance and decision support. The reference model will make point-of-care device data available for AI-based approaches. Semantic interoperability between Clinical and Research Repositories will allow correlating patient data, device data, and the patient outcome. Thus, complete workflows in high acuity environments can be analysed. Open semantic interoperability will enable the improvement of patient outcome and the increase of efficiency on a large scale and across clinical applications.",,