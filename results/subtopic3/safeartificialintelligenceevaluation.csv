Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
On Safety Assessment of Artificial Intelligence,Safety assessment is required for AI systems if they are used in safety-related applications.,Search,2020,5,"Jens  Braband, Hendrik  Schäbe",ArXiv,,10.21683/1729-2646-2020-20-4-25-34,https://doi.org/10.21683/1729-2646-2020-20-4-25-34,https://semanticscholar.org/paper/99cadaf09e78f298ebd920bdb8a6ae39025523e4,https://www.dependability.ru/jour/article/download/392/646,"In this paper we discuss how systems with Artificial Intelligence (AI) can undergo safety assessment. This is relevant, if AI is used in safety related applications. Taking a deeper look into AI models, we show, that many models of artificial intelligence, in particular machine learning, are statistical models. Safety assessment would then have t o concentrate on the model that is used in AI, besides the normal assessment procedure. Part of the budget of dangerous random failures for the relevant safety integrity level needs to be used for the probabilistic faulty behavior of the AI system. We demonstrate our thoughts with a simple example and propose a research challenge that may be decisive for the use of AI in safety related systems.",,
A Scenario-Based Method for Safety Certification of Artificial Intelligent Software,Safety-critical systems can perform AI programs if their AI modules' trust abilities are satisfied.,Search,2010,2,"Guoqi  Li, Minyan  Lu, Bin  Liu",2010 International Conference on Artificial Intelligence and Computational Intelligence,,10.1109/AICI.2010.339,https://doi.org/10.1109/AICI.2010.339,https://semanticscholar.org/paper/6091d3429b8e1b88bc37a766129a1c0ae5089d4a,,"Artificial intelligence (AI) is attractive for safety critical fields. However, there have been few success cases, for the AI technique is usually lack of determinism and predictability, which is usually regarded as a disqualifier in a safety on text. Increased researches and supererogatory efforts are providing to incorporate AI into the safety-critical systems in recent years. In this paper, we present a scenario-based method for safety certification, with the method AI modules of system could be evaluated before invoked, if its trust ability is satisfied, then the program will be performed for safety critical systems, otherwise it will be terminated to ask human assistance or postpone the missions.",,
Human factors challenges for the safe use of artificial intelligence in patient care,AI should be developed with human factors research accompanying it from the outset.,Search,2019,21,"Mark  Sujan, Dominic  Furniss, Kath  Grundy, Howard  Grundy, David  Nelson, Matthew  Elliott, Sean  White, Ibrahim  Habli, Nick  Reynolds",BMJ Health & Care Informatics,,10.1136/bmjhci-2019-100081,https://doi.org/10.1136/bmjhci-2019-100081,https://semanticscholar.org/paper/9a99e50b44f176ea20ed40fe90b00f4f57762666,https://informatics.bmj.com/content/bmjhci/26/1/e100081.full.pdf,"The use of artificial intelligence (AI) in patient care can offer significant benefits. However, there is a lack of independent evaluation considering AI in use. The paper argues that consideration should be given to how AI will be incorporated into clinical processes and services. Human factors challenges that are likely to arise at this level include cognitive aspects (automation bias and human performance), handover and communication between clinicians and AI systems, situation awareness and the impact on the interaction with patients. Human factors research should accompany the development of AI from the outset.",,
Decision Support for Safe AI Design,Simulations do not have to be accurate predictions of the future to be useful for AI evaluation.,Search,2012,4,Bill  Hibbard,AGI,,10.1007/978-3-642-35506-6_13,https://doi.org/10.1007/978-3-642-35506-6_13,https://semanticscholar.org/paper/c40cad8cacaa47aac6d4cab8f80749beaebd690c,http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_57.pdf,"There is considerable interest in ethical designs for artificial intelligence (AI) that do not pose risks to humans. This paper proposes using elements of Hutter's agent-environment framework to define a decision support system for simulating, visualizing and analyzing AI designs to understand their consequences. The simulations do not have to be accurate predictions of the future; rather they show the futures that an agent design predicts will fulfill its motivations and that can be explored by AI designers to find risks to humans. In order to safely create a simulation model this paper shows that the most probable finite stochastic program to explain a finite history is finitely computable, and that there is an agent that makes such a computation without any unintended instrumental actions. It also discusses the risks of running an AI in a simulated environment.",,
Safe AI Systems,"Safe AI systems must satisfy safety measures, risk values, intelligence, and goals to minimize destruction.",Search,2018,,Ananta  Bhatt,International Journal of Computer Applications,,10.5120/ijca2018918205,https://doi.org/10.5120/ijca2018918205,https://semanticscholar.org/paper/97aba8083bc5ad49af27a5c26c2bd40e6a17d6a3,,"As you know, it is inevitable to cease the advancement in AI because of the quest for developing systems that learns from experience and comply with the human capabilities. The leading ground-breaking impacts offers both positive and negative aspirations and are addressed as part of this research. To meet with the evil effects of the AI systems, is now the need of the hour to direct the focus on creating safe AI systems. The safe system must satisfy with 2 majorsapplying the security measures and analysing the system. Moreover, system must fulfil the risk and safety valuesintelligence, goals and safety in order to minimise the destruction that may occur due AI technology. However, further research is needed on the actual implementation of the safe AI systems in order to proceed with this research.",,
Safe AI for CPS (Invited Paper),Combining formal proofs with reinforcement learning can help develop safe artificial intelligence for cyber-physical systems.,Search,2018,5,"Nathan  Fulton, André  Platzer",2018 IEEE International Test Conference (ITC),,10.1109/TEST.2018.8624774,https://doi.org/10.1109/TEST.2018.8624774,https://semanticscholar.org/paper/03fce03060f6110b2772958f58539320bab519ed,,"Autonomous cyber-physical systems-such as self-driving cars and autonomous drones-often leverage artificial intelligence and machine learning algorithms to act well in open environments. Although testing plays an important role in ensuring safety and robustness, modern autonomous systems have grown so complex that achieving safety via testing alone is intractable. Formal verification reduces this testing burden by ruling out large classes of errant behavior at design time. This paper reviews recent work toward developing formal methods for cyber-physical systems that use AI for planning and control by combining the rigor of formal proofs with the flexibility of reinforcement learning.",,Review
Safe and sound - artificial intelligence in hazardous applications,Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making.,Search,2000,272,"John  Fox, Subrata Kumar Das",,,,,https://semanticscholar.org/paper/f62d215d4c40d299929d2e7fa105d54623a9e1bf,,"Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making, where small faults or errors can spell human catastrophe. This book describes, from both practical and theoretical perspectives, an AI technology for supporting sound clinical decision making and safe patient management. The technology combines techniques from conventional software engineering with a systematic method for building intelligent agents. Although the focus is on medicine, many of the ideas can be applied to AI systems in other hazardous settings. The book also covers a number of general AI problems, including knowledge representation and expertise modeling, reasoning and decision making under uncertainty, planning and scheduling, and the design and implementation of intelligent agents.The book, written in an informal style, begins with the medical background and motivations, technical challenges, and proposed solutions. It then turns to a wide-ranging discussion of intelligent and autonomous agents, with particular reference to safety and hazard management. The final section provides a detailed discussion of the knowledge representation and other aspects of the agent model developed in the book, along with a formal logical semantics for the language.",,
Safe Artificial Intelligence and Formal Methods - (Position Paper),Formal methods can assist in building safer and reliable AI.,Search,2016,9,Emil  Vassev,ISoLA,,10.1007/978-3-319-47166-2_49,https://doi.org/10.1007/978-3-319-47166-2_49,https://semanticscholar.org/paper/796dcb12b96e5d0c2f358b1b67fdfba36d023acc,https://ulir.ul.ie/bitstream/10344/5407/2/Vassev_2016_safe.pdf,"In one aspect of our life or another, today we all live with AI. For example, the mechanisms behind the search engines operating on the Internet do not just retrieve information, but also constantly learn how to respond more rapidly and usefully to our requests. Although framed by its human inventors, this AI is getting stronger and more powerful every day to go beyond the original human intentions in the future. One of the major questions emerging along with the propagation of AI in both technology and life is about safety in AI. This paper presents the author’s view about how formal methods can assist us in building safer and reliable AI.",,
Safety of Artificial Intelligence: A Collaborative Model,A healthcare use case uses deep reinforcement learning for treating sepsis patients.,Search,2020,1,"John  McDermid, Yan  Jia",AISafety@IJCAI,,,,https://semanticscholar.org/paper/355bcdbd0e611afd38c1dc7ccebb29fb1c873cda,,"Achieving and assuring the safety of systems that use artificial intelligence (AI), especially machine learning (ML), pose some specific challenges that require unique solutions. However, that does not mean that good safety and software engineering practices are no longer relevant. This paper shows how the issues associated with AI and ML can be tackled by integrating with established safety and software engineering practices. It sets out a three-layer model, going from top to bottom: system safety/functional safety; “AI/ML safety”; and safety-critical software engineering. This model gives both a basis for achieving and assuring safety and a structure for collaboration between safety engineers and AI/ML specialists. The model is illustrated with a healthcare use case which uses deep reinforcement learning for treating sepsis patients. It is argued that this model is general and that it should underpin future standards and guidelines for safety of this class of system which employ ML, particularly because the model can facilitate collaboration between the different communities.",,
"Evaluating Machine Learning Performance for Safe, Intelligent Robots",Performance evaluation must be augmented for use with robotic systems that interact with the real world.,Search,2021,,Raymond  Sheh,2021 IEEE International Conference on Intelligence and Safety for Robotics (ISR),,10.1109/ISR50024.2021.9419381,https://doi.org/10.1109/ISR50024.2021.9419381,https://semanticscholar.org/paper/3945a486d3befd3ddf2072a604927004c6ec531e,,"The rapid advancement of Machine Learning techniques has been the primary driver of improvements in the performance of Intelligent Robots. Performance evaluation is a vital part of specifying requirements and evaluating capabilities for such systems. However, the performance evaluation techniques commonly used for machine learning systems must be augmented for use with robotic systems that interact with the real world. This paper presents a survey and discussion of factors that must be considered, beyond traditional measures such as cross-validation accuracy, when developing evaluation criteria for machine learned intelligent robotic systems.",,
The potential of artificial intelligence to improve patient safety: a scoping review,,Search,2021,8,"David W. Bates, David  Levine, Ania  Syrowatka, Masha  Kuznetsova, Kelly Jean Thomas Craig, Angela  Rui, Gretchen Purcell Jackson, Kyu  Rhee",npj Digital Medicine,,10.1038/s41746-021-00423-6,https://doi.org/10.1038/s41746-021-00423-6,https://semanticscholar.org/paper/548ebf2fc377e3ae71332f450d65eaa5c3660371,https://www.nature.com/articles/s41746-021-00423-6.pdf,"Artificial intelligence (AI) represents a valuable tool that could be used to improve the safety of care. Major adverse events in healthcare include: healthcare-associated infections, adverse drug events, venous thromboembolism, surgical complications, pressure ulcers, falls, decompensation, and diagnostic errors. The objective of this scoping review was to summarize the relevant literature and evaluate the potential of AI to improve patient safety in these eight harm domains. A structured search was used to query MEDLINE for relevant articles. The scoping review identified studies that described the application of AI for prediction, prevention, or early detection of adverse events in each of the harm domains. The AI literature was narratively synthesized for each domain, and findings were considered in the context of incidence, cost, and preventability to make projections about the likelihood of AI improving safety. Three-hundred and ninety-two studies were included in the scoping review. The literature provided numerous examples of how AI has been applied within each of the eight harm domains using various techniques. The most common novel data were collected using different types of sensing technologies: vital sign monitoring, wearables, pressure sensors, and computer vision. There are significant opportunities to leverage AI and novel data sources to reduce the frequency of harm across all domains. We expect AI to have the greatest impact in areas where current strategies are not effective, and integration and complex analysis of novel, unstructured data are necessary to make accurate predictions; this applies specifically to adverse drug events, decompensation, and diagnostic errors.",,Review
Controlling Safety of Artificial Intelligence-Based Systems in Healthcare,A safety controlling system could guide and control the implementation of artificial intelligence in healthcare.,Search,2021,6,"Mohammad Reza Davahli, Waldemar  Karwowski, Krzysztof  Fiok, Thomas T. H. Wan, Hamid R. Parsaei",Symmetry,,10.3390/sym13010102,https://doi.org/10.3390/sym13010102,https://semanticscholar.org/paper/f725733c4a497ef80f089d391c7b9f5304206d28,https://www.mdpi.com/2073-8994/13/1/102/pdf,"In response to the need to address the safety challenges in the use of artificial intelligence (AI), this research aimed to develop a framework for a safety controlling system (SCS) to address the AI black-box mystery in the healthcare industry. The main objective was to propose safety guidelines for implementing AI black-box models to reduce the risk of potential healthcare-related incidents and accidents. The system was developed by adopting the multi-attribute value model approach (MAVT), which comprises four symmetrical parts: extracting attributes, generating weights for the attributes, developing a rating scale, and finalizing the system. On the basis of the MAVT approach, three layers of attributes were created. The first level contained 6 key dimensions, the second level included 14 attributes, and the third level comprised 78 attributes. The key first level dimensions of the SCS included safety policies, incentives for clinicians, clinician and patient training, communication and interaction, planning of actions, and control of such actions. The proposed system may provide a basis for detecting AI utilization risks, preventing incidents from occurring, and developing emergency plans for AI-related risks. This approach could also guide and control the implementation of AI systems in the healthcare industry.",,
Guidelines for Artificial Intelligence Containment,"Safety container software can contain the information leakage, social engineering attacks, and cyberattacks from within the container.",Search,2019,20,"James  Babcock, János  Kramár, Roman V. Yampolskiy",Next-Generation Ethics,,10.1017/9781108616188.008,https://doi.org/10.1017/9781108616188.008,https://semanticscholar.org/paper/b0c820f9d7fa4d8c56a7887548834b7ba65ddfa5,http://www.vetta.org/documents/Machine_Super_Intelligence.pdf,"With almost daily improvements in capabilities of artificial intelligence it is more important than ever to develop safety software for use by the AI research community. Building on our previous work on AI Containment Problem we propose a number of guidelines which should help AI safety researchers to develop reliable sandboxing software for intelligent programs of all levels. Such safety container software will make it possible to study and analyze intelligent artificial agent while maintaining certain level of safety against information leakage, social engineering attacks and cyberattacks from within the container.",,
Towards Safe Artificial General Intelligence,"If humanity would find itself in conflict with a system of much greater intelligence than itself, then human society would likely lose.",Search,2018,21,Tom  Everitt,,,10.25911/5D134A2F8A7D3,https://doi.org/10.25911/5D134A2F8A7D3,https://semanticscholar.org/paper/8b1e149ae23ea7839c9e3a2bd063c354ff7075d0,,"The field of artificial intelligence has recently experienced a number of breakthroughs thanks to progress in deep learning and reinforcement learning. Computer algorithms now outperform humans at Go, Jeopardy, image classification, and lip reading, and are becoming very competent at driving cars and interpreting natural language. The rapid development has led many to conjecture that artificial intelligence with greater-thanhuman ability on a wide range of tasks may not be far. This in turn raises concerns whether we know how to control such systems, in case we were to successfully build them. Indeed, if humanity would find itself in conflict with a system of much greater intelligence than itself, then human society would likely lose. One way to make sure we avoid such a conflict is to ensure that any future AI system with potentially greater-thanhuman-intelligence has goals that are aligned with the goals of the rest of humanity. For example, it should not wish to kill humans or steal their resources. The main focus of this thesis will therefore be goal alignment, i.e. how to design artificially intelligent agents with goals coinciding with the goals of their designers. Focus will mainly be directed towards variants of reinforcement learning, as reinforcement learning currently seems to be the most promising path towards powerful artificial intelligence. We identify and categorize goal misalignment problems in reinforcement learning agents as designed today, and give examples of how these agents may cause catastrophes in the future. We also suggest a number of reasonably modest modifications that can be used to avoid or mitigate each identified misalignment problem. Finally, we also study various choices of decision algorithms, and conditions for when a powerful reinforcement learning system will permit us to shut it down. The central conclusion is that while reinforcement learning systems as designed today are inherently unsafe to scale to human levels of intelligence, there are ways to potentially address many of these issues without straying too far from the currently so successful reinforcement learning paradigm. Much work remains in turning the high-level proposals suggested in this thesis into practical algorithms, however. Central claim: There are a number of theoretically valid, partial solutions to the problem of keeping artificial general intelligence both safe and useful.",,
Artificial intelligence in safety-critical systems: a systematic review,Bayesian networks and deep neural networks are the most widely used AI techniques in safety-critical systems.,Search,2021,,"Yue  Wang, Sai Ho Chung",Industrial Management & Data Systems,,10.1108/imds-07-2021-0419,https://doi.org/10.1108/imds-07-2021-0419,https://semanticscholar.org/paper/dd56d26b7efd78651f9abf530741da8de7ca1a69,,"PurposeThis study is a systematic literature review of the application of artificial intelligence (AI) in safety-critical systems. The authors aim to present the current application status according to different AI techniques and propose some research directions and insights to promote its wider application.Design/methodology/approachA total of 92 articles were selected for this review through a systematic literature review along with a thematic analysis.FindingsThe literature is divided into three themes: interpretable method, explain model behavior and reinforcement of safe learning. Among AI techniques, the most widely used are Bayesian networks (BNs) and deep neural networks. In addition, given the huge potential in this field, four future research directions were also proposed.Practical implicationsThis study is of vital interest to industry practitioners and regulators in safety-critical domain, as it provided a clear picture of the current status and pointed out that some AI techniques have great application potential. For those that are inherently appropriate for use in safety-critical systems, regulators can conduct in-depth studies to validate and encourage their use in the industry.Originality/valueThis is the first review of the application of AI in safety-critical systems in the literature. It marks the first step toward advancing AI in safety-critical domain. The paper has potential values to promote the use of the term “safety-critical” and to improve the phenomenon of literature fragmentation.",,Systematic Review
Fading intelligence theory: A theory on keeping artificial intelligence safety for the future,,Search,2017,3,"Utku  Kose, Pandian  Vasant",2017 International Artificial Intelligence and Data Processing Symposium (IDAP),,10.1109/IDAP.2017.8090235,https://doi.org/10.1109/IDAP.2017.8090235,https://semanticscholar.org/paper/0df5135a1044d9df389bc4d40b71674d9de5c27d,,"As a result of unstoppable rise of Artificial Intelligence, there has been a remarkable focus on the question of “Will intelligent systems be safe for humankind of the future?” Because of that, many researchers have started to direct their works on dealing with problems that may cause problems on enabling Artificial Intelligence systems to behave out of control or take positions dangerous for humans. Such research works are currently included under the literature of Artificial Intelligence Safety and / or Future of Artificial Intelligence. In the context of the explanations, this research paper proposes a theory on achieving safe intelligent systems by considering life-time of an Artificial Intelligence based system according to some operational variables and eliminate – terminate an intelligent system, which is ‘old enough' to operate for giving chance to new generations of systems, which seem safer. The paper makes a brief introduction to the theory and opens doors widely for further research on it.",,