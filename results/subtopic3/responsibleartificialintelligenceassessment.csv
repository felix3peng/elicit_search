Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Progressing Towards Responsible AI,"Progress in AI can be assessed by its multidisciplinary and multi-stakeholder dialogue, cooperation, and public engagement.",Search,2020,2,"Teresa  Scantamburlo, Atia  Cort'es, Marie  Schacht",ArXiv,,,,https://semanticscholar.org/paper/f704ed8791dc7d0d8e7f7a9176d2fcee644161b9,,"The field of Artificial Intelligence (AI) and, in particular, the Machine Learning area, counts on a wide range of performance metrics and benchmark data sets to assess the problem-solving effectiveness of its solutions. However, the appearance of research centres, projects or institutions addressing AI solutions from a multidisciplinary and multi-stakeholder perspective suggests a new approach to assessment comprising ethical guidelines, reports or tools and frameworks to help both academia and business to move towards a responsible conceptualisation of AI. They all highlight the relevance of three key aspects: (i) enhancing cooperation among the different stakeholders involved in the design, deployment and use of AI; (ii) promoting multidisciplinary dialogue, including different domains of expertise in this process; and (iii) fostering public engagement to maximise a trusted relation with new technologies and practitioners. In this paper, we introduce the Observatory on Society and Artificial Intelligence (OSAI), an initiative grew out of the project AI4EU aimed at stimulating reflection on a broad spectrum of issues of AI (ethical, legal, social, economic and cultural). In particular, we describe our work in progress around OSAI and suggest how this and similar initiatives can promote a wider appraisal of progress in AI. This will give us the opportunity to present our vision and our modus operandi to enhance the implementation of these three fundamental dimensions.",,
Measures and Best Practices for Responsible AI,"The development of task-specific datasets, metrics, and best practices is essential to the determination of the trustworthiness of machine learning models.",Search,2021,,"Sunipa  Dev, Mehrnoosh  Sameki, Jwala  Dhamala, Cho-Jui  Hsieh",KDD,,10.1145/3447548.3469458,https://doi.org/10.1145/3447548.3469458,https://semanticscholar.org/paper/7b7f889440dd3bbb234ca5a0033b28bef1db0144,,"The use of machine learning (ML) based systems has become ubiquitous including their usage in critical applications like medicine and assistive technologies. Therefore, it is important to determine the trustworthiness of these ML models and tasks. A key component in this determination is the development of task specific datasets, metrics, and best practices which are able to measure the various aspects of responsible model development and deployment including robustness, interpretability and fairness. Further, datasets are also key when training for a given task, be it coreference resolution in language modeling or facial recognition in computer vision. Imbalances and inadequate representation in datasets can have repercussions of an undesirable nature. Some common examples include how coreference resolution systems in NLU are often not all gender inclusive, discrepancies in the measurement of how robust and trustworthy machine predictions are in domains where the selective labels problem is prevalent, and discriminatory determination of pain or care levels of people belonging to different demographics in health science applications. Development of task specific datasets which do better in this regard is also extremely vital. In this workshop, we invite contributions towards different (i) datasets which help enhance task performance and inclusivity, (ii) measures and metrics which help in determining the trustworthiness of a model/dataset, (iii) assessment or remediation tools for fairer, more transparent, robust, and reliable models, and (iv) case studies describing responsible development and deployment of AI systems across fields such as healthcare, financial services, insurance, etc. The datasets, measures, mitigation techniques, and best practices could focus on different areas including (but not restricted to) the following: Fairness and Bias Robustness Reliability and Safety Interpretability Explainability Ethical AI Causal Inference Counterfactual Example Analysis They could also be focussed on the applications in diverse fields such as industry, finance, healthcare and beyond. Text based datasets can be in languages other than English as well.",,
Responsibility and Artificial Intelligence,Responsibility in artificial intelligence requires the development of mechanisms that enable AI systems to act according to ethics and human values.,Search,2020,4,Virginia  Dignum,,,10.1093/oxfordhb/9780190067397.013.12,https://doi.org/10.1093/oxfordhb/9780190067397.013.12,https://semanticscholar.org/paper/faf748f8f38d48f2ef310e9e6f55ed277dcbebbf,,"This chapter explores the concept of responsibility in artificial intelligence (AI). Being fundamentally tools, AI systems are fully under the control and responsibility of their owners or users. However, their potential autonomy and capability to learn require that design considers accountability, responsibility, and transparency principles in an explicit and systematic manner. The main concern of Responsible AI is thus the identification of the relative responsibility of all actors involved in the design, development, deployment, and use of AI systems. Firstly, society must be prepared to take responsibility for AI impact. Secondly, Responsible AI implies the need for mechanisms that enable AI systems to act according to ethics and human values. Lastly, Responsible AI is about participation. It is necessary to understand how different people work with and live with AI technologies across cultures in order to develop frameworks for responsible AI.",,
Responsible AI Challenges in End-to-end Machine Learning,Responsible AI must be easy to deploy and actionable.,Search,2021,,"Steven Euijong Whang, Ki Hyun Tae, Yuji  Roh, Geon  Heo",ArXiv,,,,https://semanticscholar.org/paper/0233cd95dd0bc327dd72a14d60216c98021250ab,,"Responsible AI is becoming critical as AI is widely used in our everyday lives. Many companies that deploy AI publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more. In addition, these objectives are not only relevant to model training, but to all steps of end-to-end machine learning, which include data collection, data cleaning and validation, model training, model evaluation, and model management and serving. Finally, responsible AI is conceptually challenging, and supporting all the objectives must be as easy as possible. We thus propose three key research directions towards this vision – depth, breadth, and usability – to measure progress and introduce our ongoing research. First, responsible AI must be deeply supported where multiple objectives like fairness and robust must be handled together. To this end, we propose FR-Train, a holistic framework for fair and robust model training in the presence of data bias and poisoning. Second, responsible AI must be broadly supported, preferably in all steps of machine learning. Currently we focus on the data pre-processing steps and propose Slice Tuner, a selective data acquisition framework for training fair and accurate models, and MLClean, a data cleaning framework that also improves fairness and robustness. Finally, responsible AI must be usable where the techniques must be easy to deploy and actionable. We propose FairBatch, a batch selection approach for fairness that is effective and simple to use, and Slice Finder, a model evaluation tool that automatically finds problematic slices. We believe we scratched the surface of responsible AI for end-to-end machine learning and suggest research challenges moving forward.",,
Software Engineering Methods for Responsible Artificial Intelligence,An engineered design methodological framework can support ethical considerations throughout the artificial intelligence systems’ software development life-cycle.,Search,2021,,Zahoor Ul Islam,AAMAS,,10.5555/3463952.3464248,https://doi.org/10.5555/3463952.3464248,https://semanticscholar.org/paper/40a6c6adf5f8a0cffcf0c36e1963d6324add2705,,"In order to ensure responsible Artificial intelligence (AI) applications engineering, we need to make sure that the development of AI systems is mindful of the consequences for individuals and societies. By anticipating the consequences of the design choices, reflecting upon the problem being solved by engaging all stakeholders and taking appropriate actions to ensure openness and the system’s social, legal, and ethical acceptability. This research aims to develop an engineering process model by which ethical considerations can be addressed throughout the AI systems’ software development life-cycle. The design methodological framework engineered in this PhD research will support aligning system goals with key ethical values by providing explicit values analysis and interpretation mechanisms, formal representation of ethical values, mechanisms for stakeholders participation in handling ethical deliberation, and providing support for governance and compliance mechanisms.",,
Responsible AI Tutorial,A key aspect to making AI responsible is to have a development pipeline that can promote reproducibility of results and manage the lineage of data and ML models.,Search,2022,,"Dr Mukta Paliwal, Dattaraj  Rao, Amogh Kamat Tarcar",COMAD/CODS,,10.1145/3493700.3493769,https://doi.org/10.1145/3493700.3493769,https://semanticscholar.org/paper/04b7d3004cdf971ea05803fbe39c25de561afcc5,,"There is rapid technical progress and widespread adoption of Artificial Intelligence (AI) based products and workflows influencing many aspects of human and business activities like banking, healthcare, advertising and many more. Although accuracy of AI models is undoubtedly the most important factor considered while deploying AI based products, there is urgent need to understand how AI can be designed to operate responsibly. Responsible AI is a framework that each software developing organization needs to adapt to build customer trust in the transparency, accountability, fairness, and security of deployed AI solutions. At the same time a key aspect to making AI responsible is to have a development pipeline that can promote reproducibility of results and manage the lineage of data and ML models. This tutorial will throw light on these aspects of Responsible AI with a working example demonstrating the concept. The intent of the tutorial will be to equip the audience with enough knowledge of the concepts along with code to gain appreciation for the importance of building Responsible AI.",,
Responsible AI and Its Stakeholders,All stakeholders involved in the development of AI are responsible for their systems.,Search,2020,3,"Gabriel  Lima, Meeyoung  Cha",ArXiv,,,,https://semanticscholar.org/paper/1d2eac19d1bd75d9d9f2afc919c2612b819c4ac2,,"Responsible Artificial Intelligence (AI) proposes a framework that holds all stakeholders involved in the development of AI to be responsible for their systems. It, however, fails to accommodate the possibility of holding AI responsible per se, which could close some legal and moral gaps concerning the deployment of autonomous and self-learning systems. We discuss three notions of responsibility (i.e., blameworthiness, accountability, and liability) for all stakeholders, including AI, and suggest the roles of jurisdiction and the general public in this matter.",,
Artificial intelligence in health care: accountability and safety,A static to a dynamic model is required from a static to a dynamic model of safety assurance.,Search,2020,27,"Ibrahim  Habli, Tom  Lawton, Zoe  Porter",Bulletin of the World Health Organization,,10.2471/BLT.19.237487,https://doi.org/10.2471/BLT.19.237487,https://semanticscholar.org/paper/a132e66655f9d4f8edba54df053c6656c4c617be,https://europepmc.org/articles/pmc7133468?pdf=render,"Abstract The prospect of patient harm caused by the decisions made by an artificial intelligence-based clinical tool is something to which current practices of accountability and safety worldwide have not yet adjusted. We focus on two aspects of clinical artificial intelligence used for decision-making: moral accountability for harm to patients; and safety assurance to protect patients against such harm. Artificial intelligence-based tools are challenging the standard clinical practices of assigning blame and assuring safety. Human clinicians and safety engineers have weaker control over the decisions reached by artificial intelligence systems and less knowledge and understanding of precisely how the artificial intelligence systems reach their decisions. We illustrate this analysis by applying it to an example of an artificial intelligence-based system developed for use in the treatment of sepsis. The paper ends with practical suggestions for ways forward to mitigate these concerns. We argue for a need to include artificial intelligence developers and systems safety engineers in our assessments of moral accountability for patient harm. Meanwhile, none of the actors in the model robustly fulfil the traditional conditions of moral accountability for the decisions of an artificial intelligence system. We should therefore update our conceptions of moral accountability in this context. We also need to move from a static to a dynamic model of assurance, accepting that considerations of safety are not fully resolvable during the design of the artificial intelligence system before the system has been deployed.",,
Toward Responsible AI by Planning to Fail,"AI failures are inevitable and must be systematically and proactively identified, assessed, and mitigated.",Search,2020,1,Saleema  Amershi,KDD,,10.1145/3394486.3409557,https://doi.org/10.1145/3394486.3409557,https://semanticscholar.org/paper/65e65e775bd1f09501faaa3cc8f852b2ea33e308,,"The potential for AI technologies to enhance human capabilities and improve our lives is of little debate; yet, neither is their potential to cause harm and social disruption. While preventing or minimizing AI biases and harms is justifiably the subject of intense study in academic, industrial and even legal communities, an approach centered on acknowledging and planning for AI-based failures has the potential to shed new light on how to develop and deploy responsible AI-based systems. In this talk, I will discuss the sociotechnical nature of several inherent and unavoidable AI failures and why it is important for the industry to systematically and proactively identify, assess, and mitigate harms caused by such failures in our AI-based products and services. I will then present Microsoft's recently released Guidelines for Human-AI Interaction and how we've been using them at Microsoft to help teams think through and prepare for different types of AI failures.",,
Toward an Understanding of Responsible Artificial Intelligence Practices,There is an urgently need to understand how AI can be designed to operate responsibly and act in a manner meeting stakeholders’ expectations and applicable regulations.,Search,2020,16,"Yichuan  Wang, Mengran  Xiong, Hossein  Olya",HICSS,,10.24251/hicss.2020.610,https://doi.org/10.24251/hicss.2020.610,https://semanticscholar.org/paper/6f62e85aa4034e206caa2482e90c349c954e21fb,http://eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf,"Artificial Intelligence (AI) is influencing all aspects of human and business activities nowadays. Although potential benefits emerged from AI technologies have been widely discussed in many current literature, there is an urgently need to understand how AI can be designed to operate responsibly and act in a manner meeting stakeholders’ expectations and applicable regulations. We seek to fill the gap by exploring the practices of responsible AI and identifying the potential benefits when implementing responsible AI practices. In this study, 10 responsible AI cases were selected from different industries to better understand the use of responsible AI in practices. Four responsible AI practices are identified, including governance, ethically design solutions, risk control and training and education and five strategies for firms who are considering to adopt responsible AI practices are recommended.",,
Responsible AI: A Primer for the Legal Community,The data-driven and often black box nature of these systems does not absolve organizations from the social responsibility.,Search,2020,,"Ilana  Golbin, Anand S. Rao, Ali  Hadjarian, Daniel  Krittman",2020 IEEE International Conference on Big Data (Big Data),,10.1109/BigData50022.2020.9377738,https://doi.org/10.1109/BigData50022.2020.9377738,https://semanticscholar.org/paper/7d6cd81f02876c8bdd3c75582a73734339fdd711,,"Artificial intelligence (AI) is increasingly being adopted for automation and decision-making tasks across all industries, public sector, and law. Applications range from hiring and credit limit decisions, to loan and healthcare claim approvals, to criminal sentencing, and even the selective provision of information by social media companies to different groups of viewers. The increased adoption of AI, affecting so many aspects of our daily lives, highlights the potential risks around automated decision making and the need for better governance and ethical standards when deploying such systems. In response to that need, governments, states, municipalities, private sector organizations, and industry groups around the world have drafted hundreds, perhaps even thousands at this point - of new, regulatory proposals and guidelines; many already in effect and more on the way. The data-driven and often black box nature of these systems does not absolve organizations from the social responsibility or increasingly commonplace regulatory requirements to confirm they work as intended and are deployed in a responsible manner, lest they run the risk of reputational damage, regulatory fines, and/or legal action. The legal community should have a good understanding of the responsible development and deployment of artificial intelligence in order to inform, translate, and advise on the legal implications of AI systems.",,
Responsible Artificial Intelligence (AI) for Value Formation and Market Performance in Healthcare: the Mediating Role of Patient’s Cognitive Engagement,The Healthcare sector has been at the forefront of the adoption of artificial intelligence technologies.,Search,2021,11,"Pradeep  Kumar, Yogesh K. Dwivedi, Ambuj  Anand",Information systems frontiers : a journal of research and innovation,,10.1007/s10796-021-10136-6,https://doi.org/10.1007/s10796-021-10136-6,https://semanticscholar.org/paper/8eaeda4885475764277a056c37e034be2e46ccd2,https://link.springer.com/content/pdf/10.1007/s10796-021-10136-6.pdf,"The Healthcare sector has been at the forefront of the adoption of artificial intelligence (AI) technologies. Owing to the nature of the services and the vulnerability of a large section of end-users, the topic of responsible AI has become the subject of widespread study and discussion. We conduct a mixed-method study to identify the constituents of responsible AI in the healthcare sector and investigate its role in value formation and market performance. The study context is India, where AI technologies are in the developing phase. The results from 12 in-depth interviews enrich the more nuanced understanding of how different facets of responsible AI guide healthcare firms in evidence-based medicine and improved patient centered care. PLS-SEM analysis of 290 survey responses validates the theoretical framework and establishes responsible AI as a third-order factor. The 174 dyadic data findings also confirm the mediation mechanism of the patient’s cognitive engagement with responsible AI-solutions and perceived value, which leads to market performance.",,
Responsible AI for Digital Health: a Synthesis and a Research Agenda,Issues regarding each of the areas of responsible AI pose moral and ethical consequences in a health context.,Search,2021,11,"Cristina  Trocin, Patrick  Mikalef, Zacharoula  Papamitsiou, Kieran  Conboy",Information Systems Frontiers,,10.1007/s10796-021-10146-4,https://doi.org/10.1007/s10796-021-10146-4,https://semanticscholar.org/paper/758289be073e7d4e6be6e6e4a7f937ef48ae81bd,https://link.springer.com/content/pdf/10.1007/s10796-021-10146-4.pdf,"Responsible AI is concerned with the design, implementation and use of ethical, transparent, and accountable AI technology in order to reduce biases, promote fairness, equality, and to help facilitate interpretability and explainability of outcomes, which are particularly pertinent in a healthcare context. However, the extant literature on health AI reveals significant issues regarding each of the areas of responsible AI, posing moral and ethical consequences. This is particularly concerning in a health context where lives are at stake and where there are significant sensitivities that are not as pertinent in other domains outside of health. This calls for a comprehensive analysis of health AI using responsible AI concepts as a structural lens. A systematic literature review supported our data collection and sampling procedure, the corresponding analysis, and extraction of research themes helped us provide an evidence-based foundation. We contribute with a systematic description and explanation of the intellectual structure of Responsible AI in digital health and develop an agenda for future research.",,Review
Towards Responsible Artificial Intelligence in Long-term Care: A Scoping Review on Practical Approaches.,There is limited empirical evidence detailing how responsible artificial intelligence innovation is addressed in context.,Search,2021,,"Dirk R M Lukkien, Henk Herman Nap, Hendrik P Buimer, Alexander  Peine, Wouter P C Boon, Johannes C F Ket, Mirella M N Minkman, Ellen H M Moors",The Gerontologist,,10.1093/geront/gnab180,https://doi.org/10.1093/geront/gnab180,https://semanticscholar.org/paper/a2adc42d39d81e6cdb5b32c45451c42fd3aa3427,https://academic.oup.com/gerontologist/advance-article-pdf/doi/10.1093/geront/gnab180/42216937/gnab180.pdf,"BACKGROUND AND OBJECTIVES

Artificial intelligence (AI) is widely positioned to become a key element of intelligent technologies used in the long-term care (LTC) for older adults. The increasing relevance and adoption of AI has encouraged debate over the societal and ethical implications of introducing and scaling AI. This scoping review investigates how the design and implementation of AI technologies in LTC is addressed responsibly: so called responsible innovation (RI).

RESEARCH DESIGN AND METHODS

We conducted a systematic literature search in five electronic databases using concepts related to LTC, AI and RI. We then performed a descriptive and thematic analysis to map the key concepts, types of evidence and gaps in the literature.

RESULTS

After reviewing 3,339 papers, 25 papers were identified that met our inclusion criteria. From this literature, we extracted three overarching themes: user-oriented AI innovation; framing AI as a solution to RI issues; and context-sensitivity. Our results provide an overview of measures taken and recommendations provided to address responsible AI innovation in LTC.

DISCUSSION AND IMPLICATIONS

The review underlines the importance of the context of use when addressing responsible AI innovation in LTC. However, limited empirical evidence actually details how responsible AI innovation is addressed in context. Therefore, we recommend expanding empirical studies on RI at the level of specific AI technologies and their local contexts of use. Also, we call for more specific frameworks for responsible AI innovation in LTC to flexibly guide researchers and innovators. Future frameworks should clearly distinguish between RI processes and outcomes.",,Review
Role of Risks in the Development of Responsible Artificial Intelligence in the Digital Healthcare Domain,Artificial intelligence in the healthcare field raises some concerns related to privacy and ethical aspects.,Search,2021,1,"Shivam  Gupta, Shampy  Kamboj, Surajit  Bag",Information Systems Frontiers,,10.1007/s10796-021-10174-0,https://doi.org/10.1007/s10796-021-10174-0,https://semanticscholar.org/paper/a5340e0005d70f836a64be27ce720775c6aac79c,,"The use of artificial intelligence (AI) in the healthcare field is gaining popularity. However, it also raises some concerns related to privacy and ethical aspects that require the development of a responsible AI framework. The principle of responsible AI states that artificial intelligence-based systems should be considered a part of composite societal and technological systems. This study attempts to establish whether AI risks in digital healthcare are positively associated with responsible AI. The moderating effect of perceived trust and perceived privacy risks is also examined. The theoretical model was based on perceived risk theory. Perceived risk theory is important in the context of this study, as risks related to uneasiness and uncertainty can be expected in the development of responsible AI due to the volatile nature of intelligent applications. Our research provides some interesting findings which are presented in the discussion section.",,
Artificial intelligence: how it works and criteria for assessment,Artificial intelligence technologies will have a significant impact on medical imaging.,Search,2021,,"Irena L. Shlivko, Oxana Ye. Garanina, Irina A. Klemenova, Kseniia A. Uskova, Anna M. Mironycheva, Veniamin I. Dardyk, Viktor N. Laskov",Consilium Medicum,,10.26442/20751753.2021.8.201148,https://doi.org/10.26442/20751753.2021.8.201148,https://semanticscholar.org/paper/622a4773fd895416a6cdce3b81cb087b27336a66,https://consilium.orscience.ru/2075-1753/article/download/97097/71487,"Artificial intelligence is a term used to describe computer technology in the modeling of intelligent behavior and critical thinking comparable to that of humans. To date, some of the first areas of medicine to be influenced by advances in artificial intelligence technologies will be those most dependent on imaging. These include ophthalmology, radiology, and dermatology. In connection with the emergence of numerous medical applications, scientists have formulated criteria for their assessment. This list included: clinical validation, regular application updates, functional focus, cost, availability of an information block for specialists and patients, compliance with the conditions of government regulation, and registration. One of the applications that meet all the requirements is the ProRodinki software package, developed for use by patients and specialists in the Russian Federation. Taking into account a widespread and rapidly developing competitive environment, it is necessary to soberly treat the resources of such applications, not exaggerating their capabilities and not considering them as a substitute for a specialist.",,