Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Safe and sound - artificial intelligence in hazardous applications,Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making.,Search,2000,272,"John  Fox, Subrata Kumar Das",,,,,https://semanticscholar.org/paper/f62d215d4c40d299929d2e7fa105d54623a9e1bf,,"Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making, where small faults or errors can spell human catastrophe. This book describes, from both practical and theoretical perspectives, an AI technology for supporting sound clinical decision making and safe patient management. The technology combines techniques from conventional software engineering with a systematic method for building intelligent agents. Although the focus is on medicine, many of the ideas can be applied to AI systems in other hazardous settings. The book also covers a number of general AI problems, including knowledge representation and expertise modeling, reasoning and decision making under uncertainty, planning and scheduling, and the design and implementation of intelligent agents.The book, written in an informal style, begins with the medical background and motivations, technical challenges, and proposed solutions. It then turns to a wide-ranging discussion of intelligent and autonomous agents, with particular reference to safety and hazard management. The final section provides a detailed discussion of the knowledge representation and other aspects of the agent model developed in the book, along with a formal logical semantics for the language.",,
"Visual book review 1 “ Safe and Sound , AI in hazardous applications",The critical component of the logic of argument (LA) is exposed in Axiom 8.,Search,2001,3,Boris  Kovalerchuk,,,,,https://semanticscholar.org/paper/41f895337f19cd042dd64b1942b63c48acc57128,,"A recent book “Safe and Sound, AI in hazardous applications” by John Fox and Subrata Das (AAAI Press/ The MIT Press, 2000, 293p., ISBN 0-262-06211-9) attracts attention of the research community and practitioners to the problem of safety of traditional and computer-aided medical diagnosis and treatment. The authors use medical applications as a focal point for the general safety problem through variety of hazardous applications. At first glance, the problem is already well known. However, they show that discovery and analysis of the sources of danger in hazardous applications are far from having rigorous solutions. The book uses an artificial intelligence (AI) approach, which allows one to express different types of statements in consistent logical fashion. Specifically the authors consider two main types of statements for making medical decisions: claims and their grounds. In addition, a confidence value is assigned to a claim using ground statements. For instance, the claim can be that Mr. P has a gastric ulcer and the grounds can be that Mr. P has pain after meals & ulcers are painful because of an increase in acidity. The word “support” can express the level of confidence. In the book, the safety issue for defining diagnosis and treatment is viewed as adding restrictions on inference. The goal of restrictions is to avoid dangerous actions for patients. This review should help a reader to see conditions for successful applications of logic of argument (LA), which is the central theme of the book. The book contains three parts (see Figure 1): Part 1: Method for building software agents, which produce Rigorously Engineered Decisions. Part 2: Technique for deploying agents in general and hazardous environment with medical examples. Part 3: Formal and logical aspect of the method. The spread of the central theme over the book is presented in Figure 1. Chapter 4 provides an informal description of LA, Chapters 13 and 15 contain formalisms and Chapter 15 implementation of LA in Prolog language. The critical component of LA is exposed in Axiom 8 (Chapter 13). This axiom is the central assumption of LA. It actually formalizes the requirement of independence of arguments used in LA in inferring diagnosis or treatment recommendation. The property is also known as truth-functionality in Artificial Intelligence literature [1] and was a subject of intensive discussions in AI community for years [1,2,3].",,Review
Safe AI Systems,AI must meet safety requirements to minimize the destruction that may occur due to its technology.,Search,2018,,Ananta  Bhatt,International Journal of Computer Applications,,10.5120/ijca2018918205,https://doi.org/10.5120/ijca2018918205,https://semanticscholar.org/paper/97aba8083bc5ad49af27a5c26c2bd40e6a17d6a3,,"As you know, it is inevitable to cease the advancement in AI because of the quest for developing systems that learns from experience and comply with the human capabilities. The leading ground-breaking impacts offers both positive and negative aspirations and are addressed as part of this research. To meet with the evil effects of the AI systems, is now the need of the hour to direct the focus on creating safe AI systems. The safe system must satisfy with 2 majorsapplying the security measures and analysing the system. Moreover, system must fulfil the risk and safety valuesintelligence, goals and safety in order to minimise the destruction that may occur due AI technology. However, further research is needed on the actual implementation of the safe AI systems in order to proceed with this research.",,
Controlling Safety of Artificial Intelligence-Based Systems in Healthcare,A safety controlling system can guide and control the implementation of AI systems in the healthcare industry.,Search,2021,6,"Mohammad Reza Davahli, Waldemar  Karwowski, Krzysztof  Fiok, Thomas T. H. Wan, Hamid R. Parsaei",Symmetry,,10.3390/sym13010102,https://doi.org/10.3390/sym13010102,https://semanticscholar.org/paper/f725733c4a497ef80f089d391c7b9f5304206d28,https://www.mdpi.com/2073-8994/13/1/102/pdf,"In response to the need to address the safety challenges in the use of artificial intelligence (AI), this research aimed to develop a framework for a safety controlling system (SCS) to address the AI black-box mystery in the healthcare industry. The main objective was to propose safety guidelines for implementing AI black-box models to reduce the risk of potential healthcare-related incidents and accidents. The system was developed by adopting the multi-attribute value model approach (MAVT), which comprises four symmetrical parts: extracting attributes, generating weights for the attributes, developing a rating scale, and finalizing the system. On the basis of the MAVT approach, three layers of attributes were created. The first level contained 6 key dimensions, the second level included 14 attributes, and the third level comprised 78 attributes. The key first level dimensions of the SCS included safety policies, incentives for clinicians, clinician and patient training, communication and interaction, planning of actions, and control of such actions. The proposed system may provide a basis for detecting AI utilization risks, preventing incidents from occurring, and developing emergency plans for AI-related risks. This approach could also guide and control the implementation of AI systems in the healthcare industry.",,
Governing the safety of artificial intelligence in healthcare,Artificial intelligence technologies will introduce new risks to healthcare and amplify existing ones.,Search,2019,28,Carl  Macrae,BMJ Quality & Safety,,10.1136/bmjqs-2019-009484,https://doi.org/10.1136/bmjqs-2019-009484,https://semanticscholar.org/paper/b91e4c2234c950a8a639ba1d3f97dcd51220520a,https://nottingham-repository.worktribe.com/preview/2062727/MacraeGoverningAIsafety.pdf,"Artificial intelligence (AI) has enormous potential to improve the safety of healthcare, from increasing diagnostic accuracy,1 to optimising treatment planning,2 to forecasting outcomes of care.3 However, integrating AI technologies into the delivery of healthcare is likely to introduce a range of new risks and amplify existing ones. For instance, failures in widely used software have the potential to quickly affect large numbers of patients4; hidden assumptions in underlying data and models can lead to AI systems delivering dangerous recommendations that are insensitive to local care processes,5 6 and opaque AI techniques such as deep learning can make explaining and learning from failure extremely difficult.7 8 To maximise the benefits of AI in healthcare and to build trust among patients and practitioners, it will therefore be essential to robustly govern the risks that AI poses to patient safety.

In a recent review in this journal, Challen and colleagues present an important and timely analysis of some of the key technological risks associated with the application of machine learning in clinical settings.9 Machine learning is a subfield of AI that focuses on the development of algorithms that are automatically derived and optimised through exposure to large quantities of exemplar ‘training’ data.10 The outputs of machine learning algorithms are essentially classifications of patterns that provide some sort of prediction—for instance, predicting whether an image shows a malignant melanoma or a benign mole.11 Some of the basic techniques of machine learning have existed for half a century or more, but progress in the field has accelerated rapidly due to advances in the development of ‘deep’ artificial neural networks12 combined with huge increases in computational power and the availability of enormous quantities of data. These techniques have underpinned recent public demonstrations of AI systems …",,Review
Human factors challenges for the safe use of artificial intelligence in patient care,Artificial intelligence should be developed with human factors research to ensure safe use in patient care.,Search,2019,21,"Mark  Sujan, Dominic  Furniss, Kath  Grundy, Howard  Grundy, David  Nelson, Matthew  Elliott, Sean  White, Ibrahim  Habli, Nick  Reynolds",BMJ Health & Care Informatics,,10.1136/bmjhci-2019-100081,https://doi.org/10.1136/bmjhci-2019-100081,https://semanticscholar.org/paper/9a99e50b44f176ea20ed40fe90b00f4f57762666,https://informatics.bmj.com/content/bmjhci/26/1/e100081.full.pdf,"The use of artificial intelligence (AI) in patient care can offer significant benefits. However, there is a lack of independent evaluation considering AI in use. The paper argues that consideration should be given to how AI will be incorporated into clinical processes and services. Human factors challenges that are likely to arise at this level include cognitive aspects (automation bias and human performance), handover and communication between clinicians and AI systems, situation awareness and the impact on the interaction with patients. Human factors research should accompany the development of AI from the outset.",,
Safe AI - IS This Possible?,"The inherently self-learning, self-adaptive nature of artificial intelligence methods may require functionally deterministic controllers.",Search,1994,1,M. G. Rodd,,,10.1016/S1474-6670(17)45702-8,https://doi.org/10.1016/S1474-6670(17)45702-8,https://semanticscholar.org/paper/a9ac4c8bd3611e530f5315b54abefe100ba79ae0,,"Abstract This deliberately provocative paper poses the question as to whether it is professionally-acceptable to use AI-based control systems in real-time automation. The premise is that the increasing demands for improved control of industrial processes are resulting in a search for new control techniques which, it is suggested, will be based on various AI methodologies. However, in the face of the inherently self-learning, self-adaptive nature of such techniques, it is pointed out that to produce safe, predictable systems, the controllers should be functionally deterministic! In posing this dilemma, the paper suggests possible ways in which it may be resolved.",,
Safe and Ethical Artificial Intelligence in Radiotherapy - Lessons Learned From the Aviation Industry.,The ratio between the benefit and harms of AI can be maximised by adopting ethical and trust frameworks.,Search,2021,1,"R  Hallows, L  Glazier, M S Katz, M  Aznar, M  Williams",Clinical oncology (Royal College of Radiologists (Great Britain)),,10.1016/j.clon.2021.11.019,https://doi.org/10.1016/j.clon.2021.11.019,https://semanticscholar.org/paper/518441aba1c5f08526564b51f7b516b9837ff964,,"Ethical artificial intelligence (AI) frameworks can be the catalyst in improving the safety and wellbeing of people when developing AI systems. In 2020 Rolls-Royce released its ethical and trustworthiness toolkit, The Aletheia FrameworkTM, which helps guide organisations as they consider the ethics around the use of AI. It covers three facets: social impact, accuracy and trust, and governance - which apply across all uses of AI. By adopting AI ethics and trust frameworks, oncologists can ensure the ratio between the benefit and harms of AI can be maximised. With AI transforming every sector, collaboration across industries to share ideas and learn from each other - even unlikely partnerships between engineering and oncology - could help optimise that transition.",,
Controlling Safety of Artificial Intelligence-Based Systems in Healthcare,"Artificial intelligence (AI) models have accomplished medical tasks, at or above the performance levels of humans.",Search,2021,,"Mohammad Reza Davahli, Waldemar  Karwowski, Krzysztof  Fiok, Thomas T. H. Wan, Hamid R. Parsaei",,,10.20944/PREPRINTS202012.0313.V2,https://doi.org/10.20944/PREPRINTS202012.0313.V2,https://semanticscholar.org/paper/c63772c4844f68d88884ea4cbfd3a3739bbb59c3,https://www.mdpi.com/2073-8994/13/1/102/pdf,"Artificial intelligence (AI)-based systems have achieved significant success in healthcare since 2016, and AI models have accomplished medical tasks, at or above the performance levels of humans. Despite these achievements, various challenges exist in the application of AI in healthcare. One of the main challenges is safety, which is related to unsafe and incorrect actions and recommendations by AI algorithms. In response to the need to address the safety challenges, this research aimed to develop a safety controlling system (SCS) framework to reduce the risk of potential healthcare-related incidents. The framework was developed by adopting the multi-attribute value model approach (MAVT), which comprises four symmetrical parts: extracting attributes, generating weights for the attributes, developing a rating scale, and finalizing the system. The framework represents a set of attributes in different layers and can be used as a checklist in healthcare institutions with implemented AI models. Having these attributes in healthcare systems will lead to high scores in the SCS, which indicates safe application of AI models. The proposed framework provides a basis for implementing and monitoring safety legislation, identifying the risks in AI models’ activities, improving human-AI interactions, preventing incidents from occurring, and having an emergency plan for remaining risks.",,
Safe AI—is this possible?☆,"The inherently self-learning, self-adaptive nature of AI control systems makes them inherently unsafe.",Search,1995,11,M. G. Rodd,,,10.1016/0952-1976(95)00010-X,https://doi.org/10.1016/0952-1976(95)00010-X,https://semanticscholar.org/paper/c53376e4e49ca954f4f4f2ca520b6042967f7dda,,"Abstract This deliberately provocative paper poses the question as to whether it is professionally acceptable to use AI-based control systems in real-time automation. The premise is that the increasing demands for improved control of industrial processes are resulting in a search for new control techniques which, it is suggested, will be based on various AI methodologies. However, in the face of the inherently self-learning, self-adaptive nature of such techniques, it is pointed out that to produce safe, predictable systems, the controllers should be functionally deterministic! In posing this dilemma, the paper suggests possible ways in which it may be resolved.",,
Safe AI: creating a health care expert system,An expert system will be designed using rule-based method to indicate the most possible diagnosis for upper respiratory infections.,Search,2015,,"Marcos  Pinto, Mamun  Hasam, Hsinrong  Wei",,,,,https://semanticscholar.org/paper/b7f122b8683bd3f1efc2bf9e1b189fe26785dbde,,"This paper describes the process of creating a health care expert system (ES) step-by-step. An expert system, a branch of Artificial Intelligence (AI), will be designed using rule-based method to indicate the most possible diagnosis for upper respiratory infections (URI) such as common cold, flu (influenza), croup, and sinus infection. The expert system presents the user with a list of possible diagnoses and their corresponding explanations depending on certain user's information: age, gender, procedence region, and medical history. It is a hands-on example that can be incorporated into college courses on Web services and/or mobile applications.",,
"Towards a framework for evaluating the safety, acceptability and efficacy of AI systems for health: an initial synthesis",There are numerous reasons for limited real-world implementation of AI systems into frontline healthcare.,Search,2021,2,"Jessica  Morley, Caroline  Morton, Kassandra  Karpathakis, Mariarosaria  Taddeo, Luciano  Floridi",ArXiv,,10.2139/ssrn.3826358,https://doi.org/10.2139/ssrn.3826358,https://semanticscholar.org/paper/9ffc5d16e50ece1bceaaff95075e86da68a2c362,http://arxiv.org/pdf/2104.06910,"The potential presented by Artificial Intelligence (AI) for healthcare has long been recognised by the technical community. More recently, this potential has been recognised by policymakers, resulting in considerable public and private investment in the development of AI for healthcare across the globe. Despite this, excepting limited success stories, real-world implementation of AI systems into frontline healthcare has been limited. There are numerous reasons for this, but a main contributory factor is the lack of internationally accepted, or formalised, regulatory standards to assess AI safety and impact and effectiveness. This is a well-recognised problem with numerous ongoing research and policy projects to overcome it. Our intention here is to contribute to this problem-solving effort by seeking to set out a minimally viable framework for evaluating the safety, acceptability and efficacy of AI systems for healthcare. We do this by conducting a systematic search across Scopus, PubMed and Google Scholar to identify all the relevant literature published between January 1970 and November 2020 related to the evaluation of: output performance; efficacy; and real-world use of AI systems, and synthesising the key themes according to the stages of evaluation: pre-clinical (theoretical phase); exploratory phase; definitive phase; and post-market surveillance phase (monitoring). The result is a framework to guide AI system developers, policymakers, and regulators through a sufficient evaluation of an AI system designed for use in healthcare.",,
The potential of artificial intelligence to improve patient safety: a scoping review,Artificial intelligence has significant opportunities to reduce the frequency of harm across all domains.,Search,2021,8,"David W. Bates, David  Levine, Ania  Syrowatka, Masha  Kuznetsova, Kelly Jean Thomas Craig, Angela  Rui, Gretchen Purcell Jackson, Kyu  Rhee",npj Digital Medicine,,10.1038/s41746-021-00423-6,https://doi.org/10.1038/s41746-021-00423-6,https://semanticscholar.org/paper/548ebf2fc377e3ae71332f450d65eaa5c3660371,https://www.nature.com/articles/s41746-021-00423-6.pdf,"Artificial intelligence (AI) represents a valuable tool that could be used to improve the safety of care. Major adverse events in healthcare include: healthcare-associated infections, adverse drug events, venous thromboembolism, surgical complications, pressure ulcers, falls, decompensation, and diagnostic errors. The objective of this scoping review was to summarize the relevant literature and evaluate the potential of AI to improve patient safety in these eight harm domains. A structured search was used to query MEDLINE for relevant articles. The scoping review identified studies that described the application of AI for prediction, prevention, or early detection of adverse events in each of the harm domains. The AI literature was narratively synthesized for each domain, and findings were considered in the context of incidence, cost, and preventability to make projections about the likelihood of AI improving safety. Three-hundred and ninety-two studies were included in the scoping review. The literature provided numerous examples of how AI has been applied within each of the eight harm domains using various techniques. The most common novel data were collected using different types of sensing technologies: vital sign monitoring, wearables, pressure sensors, and computer vision. There are significant opportunities to leverage AI and novel data sources to reduce the frequency of harm across all domains. We expect AI to have the greatest impact in areas where current strategies are not effective, and integration and complex analysis of novel, unstructured data are necessary to make accurate predictions; this applies specifically to adverse drug events, decompensation, and diagnostic errors.",,Review
WHO and ITU establish benchmarking process for artificial intelligence in health,"Artificial intelligence needs regulation, adequate evaluation of efficacy, and an agreed framework for assessing results before being applied to health care.",Search,2019,32,"Thomas  Wiegand, Ramesh  Krishnamurthy, Monique  Kuglitsch, Naomi  Lee, Sameer  Pujari, Marcel  Salathé, Markus  Wenzel, Shan  Xu",The Lancet,,10.1016/S0140-6736(19)30762-7,https://doi.org/10.1016/S0140-6736(19)30762-7,https://semanticscholar.org/paper/46f202bf0a3dc340759ebad16fe9f1b4913abe47,,"Growing populations, demographic changes, and a shortage of health practitioners have placed pressures on the health-care sector. In parallel, increasing amounts of digital health data and information have become available. Artificial intelligence (AI) models that learn from these large datasets are in development and have the potential to assist with pattern recognition and classification problems in medicine—for example, early detection, diagnosis, and medical decision making. These advances promise to improve health care for patients and provide much-needed support for medical practitioners. Over the past decade, considerable resources have been allocated to exploring the use of AI for health. Although there is immense potential, many issues such as regulation, potential for bias, and adequate evaluation of efficacy must first be addressed for safe and ethical implementation of AI in health care. Modern AI algorithms are complex, and their performance depends on the quality of the training data and learning mechanism. If AI algorithms are poorly designed or the training data are biased or incomplete, errors can occur. There is no agreed framework for assessing or reporting the results of health AI models before deciding whether they are sufficiently robust for application in a population, as there is for new drugs or surgical interventions. The absence of confidence or quality control is a major barrier to the uptake of AI in health care. Creating a rigorous, standardised evaluation framework that leverages the advantages and addresses the limitations of AI models in health is crucial for realising the potential of this technology and limiting risks. Two UN agencies, WHO and the International Telecommunication Union (ITU), established a Focus Group on Artificial Intelligence for Health (FG-AI4H) in July, 2018. FG-AI4H is developing a benchmarking process for health AI models that can act as an international, independent, standard evaluation framework. To establish this evaluation and benchmarking process, FG-AI4H is calling for participation from medical, public health, AI, data analytics, and policy experts. Topic groups are being formed by communities of stakeholders allowing FG-AI4H to develop its processes for AI evaluation and benchmarking specific for each health topic. Each topic use case will be reviewed for its relevance and should impact a large and diverse part of the global population or solve a health problem that is difficult or expensive. The AI models are expected to offer improvements over current practices in quality or efficiency that would be expected to lead to better health outcomes or costeffectiveness. Once formed, topic groups will provide a forum for open collaboration among stakeholders who agree on a pragmatic, best-practice approach for benchmarking each use case, including defining the application scenario and desired output of AI models in that use case, identifying adequate sources of training and testing data, and facilitating the preparation of multisource heterogeneous data. All data for training and testing are expected to be of high quality, ethically generated, and accompanied by detailed information about their format and properties. Thus far, FG-AI4H has developed 11 topic groups in areas such as cardiovascular disease risk prediction, ophthalmology (retinal imaging diagnostics), and AI-based symptom checkers, but this approach is expected to be expanded to other tasks. The benchmarking process will be done on secure, confidential test data. Ideally, test data will originate from various sources to determine whether the use of an AI model can be generalised across different populations, measurement devices, and health-care settings. The benchmarking process for each use case within a topic needs to be defined. For many use cases it would, at least initially, be meaningful to compare model performance against human performance, or human performance with AI assistance in the same task, whereas for other tasks, comparative performance of algorithms would be more meaningful. Once these requirements are met, AI models can be submitted via an online platform to be evaluated with the test data. Established in this way, the benchmarking process will not only provide a reliable, robust, and independent evaluation system that can demonstrate the quality Published Online March 29, 2019 http://dx.doi.org/10.1016/ S0140-6736(19)30762-7",,
Senior Living Communities: Made Safer by AI,Pervasive automation can make senior living facilities safer by reducing risk and cost.,Search,2020,,"Ashutosh  Saxena, David  Cheriton",ArXiv,,,,https://semanticscholar.org/paper/53317a8f103e8ee7b22252de89e8cc128c340366,,"There is a historically unprecedented shift in demographics towards seniors, which will result in significant housing development over the coming decade. This is an enormous opportunity for real-estate operators to innovate and address the demand in this growing market. However, investments in this area are fraught with risk. Seniors often have more health issues, and Covid-19 has exposed just how vulnerable they are -- especially those living in close proximity. Conventionally, most services for seniors are ""high-touch"", requiring close physical contact with trained caregivers. Not only are trained caregivers short in supply, but the pandemic has made it evident that conventional high-touch approaches to senior care are high-cost and greater risk. There are not enough caregivers to meet the needs of this emerging demographic, and even fewer who want to undertake the additional training and risk of working in a senior facility, especially given the current pandemic. In this article, we rethink the design of senior living facilities to mitigate the risks and costs using automation. With AI-enabled pervasive automation, we claim there is an opportunity, if not an urgency, to go from high-touch to almost ""no touch"" while dramatically reducing risk and cost. Although our vision goes beyond the current reality, we cite measurements from Caspar AI-enabled senior properties that show the potential benefit of this approach.",,
Artificial intelligence in a crisis needs ethics with urgency,Artificial intelligence can be used in the COVID-19 response to save lives with new ethical approaches.,Search,2020,14,"Asaf  Tzachor, Jess  Whittlestone, Lalitha S Sundaram, Seán Ó hÉigeartaigh",Nat. Mach. Intell.,,10.1038/s42256-020-0195-0,https://doi.org/10.1038/s42256-020-0195-0,https://semanticscholar.org/paper/cda4b91f54ee9ecb0be363c58735caa5b0d85d35,https://www.nature.com/articles/s42256-020-0195-0.pdf,"Artificial intelligence tools can help save lives in a pandemic. However, the need to implement technological solutions rapidly raises challenging ethical issues. We need new approaches for ethics with urgency, to ensure AI can be safely and beneficially used in the COVID-19 response and beyond.",,