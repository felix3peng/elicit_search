Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Opening the software engineering toolbox for the assessment of trustworthy AI,Software engineering and testing practices can be used for the assessment of trustworthy AI.,Search,2020,2,"Mohit Kumar Ahuja, Mohamed-Bachir  Belaid, Pierre  Bernab'e, Mathieu  Collet, Arnaud  Gotlieb, Chhagan  Lal, Dusica  Marijan, Sagar  Sen, Aizaz  Sharif, Helge  Spieker",NeHuAI@ECAI,,,,https://semanticscholar.org/paper/3b0ff6bd000e9c615d024614343f2c1cf12bf124,,"Trustworthiness is a central requirement for the acceptance and success of human-centered artificial intelligence (AI). To deem an AI system as trustworthy, it is crucial to assess its behaviour and characteristics against a gold standard of Trustworthy AI, consisting of guidelines, requirements, or only expectations. While AI systems are highly complex, their implementations are still based on software. The software engineering community has a long-established toolbox for the assessment of software systems, especially in the context of software testing. In this paper, we argue for the application of software engineering and testing practices for the assessment of trustworthy AI. We make the connection between the seven key requirements as defined by the European Commission's AI high-level expert group and established procedures from software engineering and raise questions for future work.",,
Establishing the rules for building trustworthy AI,The European Commission’s report ‘Ethics guidelines for trustworthy AI’ facilitates international support for AI solutions that are good for humanity and the environment.,Search,2019,67,Luciano  Floridi,Nature Machine Intelligence,,10.1038/S42256-019-0055-Y,https://doi.org/10.1038/S42256-019-0055-Y,https://semanticscholar.org/paper/dc44e2be0f85b6225f05390c570885337a99ef83,https://philpapers.org/archive/FLOETR.pdf,"The European Commission’s report ‘Ethics guidelines for trustworthy AI’ provides a clear benchmark to evaluate the responsible development of AI systems, and facilitates international support for AI solutions that are good for humanity and the environment, says Luciano Floridi.",,
"A Review of Trust in Artificial Intelligence: Challenges, Vulnerabilities and Future Directions","Artificial Intelligence (AI) can benefit society, but it is also fraught with risks.",Search,2021,4,"Steven  Lockey, Nicole M. Gillespie, Daniel  Holm, Ida Asadi Someh",HICSS,,10.24251/HICSS.2021.664,https://doi.org/10.24251/HICSS.2021.664,https://semanticscholar.org/paper/1f11e05c5484ff5a066eab37c6268ad7aceaac8d,http://scholarspace.manoa.hawaii.edu/bitstream/10125/71284/1/0534.pdf,"Artificial Intelligence (AI) can benefit society, but it is also fraught with risks. Societal adoption of AI is recognized to depend on stakeholder trust in AI, yet the literature on trust in AI is fragmented, and little is known about the vulnerabilities faced by different stakeholders, making it is difficult to draw on this evidence-base to inform practice and policy. We undertake a literature review to take stock of what is known about the antecedents of trust in AI, and organize our findings around five trust challenges unique to or exacerbated by AI. Further, we develop a concept matrix identifying the key vulnerabilities to stakeholders raised by each of the challenges, and propose a multi-stakeholder approach to future research.",,Review
Human Trust in Artificial Intelligence: Review of Empirical Research,The success of integrating AI into various industries will depend on the level of human trust in the technology.,Search,2020,107,"Ella  Glikson, Anita Williams Woolley",,,10.5465/annals.2018.0057,https://doi.org/10.5465/annals.2018.0057,https://semanticscholar.org/paper/ef0c62ff070a476f216fe478cc190c773f12a1f6,,Artificial intelligence (AI) characterizes a new generation of technologies capable of interacting with the environment and aiming to simulate human intelligence. The success of integrating AI into...,,Review
Trust in Artificial Intelligence,"Factors such as anthropomorphic agents, oxytocin, mental models, and anthrophomorphizing help to calibrate the right level of trust in AI.",Search,2018,14,Arathi  Sethumadhavan,Ergonomics in Design: The Quarterly of Human Factors Applications,,10.1177/1064804618818592,https://doi.org/10.1177/1064804618818592,https://semanticscholar.org/paper/94a43052c729c8e7e3f28c50c76e46d09ec95f62,https://journals.sagepub.com/doi/pdf/10.1177/1064804618818592,"Trust is fundamental to creating a lasting relationship with another human being. In our daily lives, we encounter several situations where we place trust on other humans such as bus drivers, colleagues, and even strangers. Trust in human-human teams is initially founded on the predictability of the trustee, and as the relationship between the trustor and the trustee progresses, dependability or integrity replaces predictability as the basis of trust (Hoff & Bashir, 2015). As artificial intelligence (AI) gets smarter and smarter, it is becoming an integral part of human lives. For example, AI is being used for scheduling appointments, powering smart homes, recognizing people’s faces in photos, making health diagnoses, providing investment advice, and several other things. As with a humanhuman team, trust is a necessary ingredient for human-AI partnerships. However, trust in human-human teams progresses in the reverse order from human-human teams (e.g., Hoff & Bashir, 2015). This is because humans initially assume that AI is near perfect. Therefore, in the initial stages, faith forms the essential constituent of trust, and as the number of exchanges between the human and the AI increases, faith is replaced by dependability and predictability. Trust in AI is considered a twodimensional construct comprising trust and distrust, where trust is associated with feelings of calmness and security and distrust involves fear and worry (Lyons, Stokes, Eschleman, Alarcon, & Barelka, 2011). Unarguably, trust is a complex social process with a variety of factors determining the extent to which humans trust AI agents (Hoff & Bashir, 2015; Lee & See, 2004). Specifically, in order to calibrate the right level of trust in AI, consider dispositional (i.e., user characteristics such as age, culture, gender, and personality), internal (e.g., user characteristics such as workload, mood, self-confidence, and working memory capacity), environmental (e.g., task difficulty, perceived risks and benefits, organizational setting), and learned factors (e.g., reputation of the AI, reliability, consistency, type and timing of errors made by the AI, and users’ experience with similar agents). Design factors (e.g., appearance, ease of use, communication style, and transparency of the AI) also affect perceptions of trust. For example, anthrophomorphizing is an effective way of establishing a long-term social bond between humans and AI agents, which is driven by the neurotransmitter and hormone, oxytocin (e.g., de Visser et al., 2017). Further, anthropomorphic agents also resist breakdown in trust compared to their counterpart non-anthropomorphic agents, presumably because anthropomorphic agents remind users of humans who are forgiven more easily for being imperfect in comparison to machine-like agents. Interestingly, there is also evidence that humans tend to disclose more information to AI therapists than human therapists. Transparency of the AI also helps to calibrate the right level of trust by enabling users to develop accurate mental models of the AI underpinnings (e.g., Balfe, Sharples, & Wilson, 2018). 818592 ERGXXX10.1177/1064804618818592ergonomics in designergonomics in design research-article2018",,
Trustworthy AI Inference Systems: An Industry Research View,Industry must utilize appropriate security protection mechanisms for trustworthy AI inference systems.,Search,2020,2,"Rosario  Cammarota, Matthias  Schunter, Anand  Rajan, Fabian  Boemer, 'Agnes  Kiss, Amos  Treiber, Christian  Weinert, Thomas  Schneider, Emmanuel  Stapf, Ahmad-Reza  Sadeghi, Daniel  Demmler, Huili  Chen, Siam Umar Hussain, Sadegh  Riazi, Farinaz  Koushanfar, Saransh  Gupta, Tajan Simunic Rosing, Kamalika  Chaudhuri, Hamid  Nejatollahi, Nikil  Dutt, Mohsen  Imani, Kim  Laine, Anuj  Dubey, Aydin  Aysu, Fateme Sadat Hosseini, Chengmo  Yang, Eric  Wallace, Pamela  Norton",ArXiv,,,,https://semanticscholar.org/paper/828947e3e9e06c506e6a30e3eb7e176c0b8b953d,,"In this work, we provide an industry research view for approaching the design, deployment, and operation of trustworthy Artificial Intelligence (AI) inference systems. Such systems provide customers with timely, informed, and customized inferences to aid their decision, while at the same time utilizing appropriate security protection mechanisms for AI models. Additionally, such systems should also use Privacy-Enhancing Technologies (PETs) to protect customers' data at any time.

To approach the subject, we start by introducing trends in AI inference systems. We continue by elaborating on the relationship between Intellectual Property (IP) and private data protection in such systems. Regarding the protection mechanisms, we survey the security and privacy building blocks instrumental in designing, building, deploying, and operating private AI inference systems. For example, we highlight opportunities and challenges in AI systems using trusted execution environments combined with more recent advances in cryptographic techniques to protect data in use. Finally, we outline areas of further development that require the global collective attention of industry, academia, and government researchers to sustain the operation of trustworthy AI inference systems.",,
How Much to Trust Artificial Intelligence?,AI is prone to vulnerabilities and needs to be sufficiently reliable to do its job.,Search,2017,22,George F. Hurlburt,IT Professional,,10.1109/MITP.2017.3051326,https://doi.org/10.1109/MITP.2017.3051326,https://semanticscholar.org/paper/6b98bbdec602be0f79c434c354121de63f8bc948,,"Considerable buzz surrounds artificial intelligence, and, indeed, AI is all around us. As with any software-based technology, it is also prone to vulnerabilities. Here, the author examines how we determine whether AI is sufficiently reliable to do its job and how much we should trust its outcomes.",,
The relationship between trust in AI and trustworthy machine learning technologies,"Trustworthiness technologies can support the required qualities of fair, explainable, auditable, and safe machine learning.",Search,2020,50,"Ehsan  Toreini, Mhairi  Aitken, Kovila  Coopamootoo, Karen  Elliott, Carlos Gonzalez Zelaya, Aad van Moorsel",FAT*,,10.1145/3351095.3372834,https://doi.org/10.1145/3351095.3372834,https://semanticscholar.org/paper/bd4cf5a6987ef0f39b7e4e88d59b3a3365abcc70,http://arxiv.org/pdf/1912.00782,"To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.",,
Should We Trust (X)AI? Design Dimensions for Structured Experimental Evaluations,"There are methodological differences between the fields of machine learning, human-computer interaction, and visual analytics.",Search,2020,6,"Fabian  Sperrle, Mennatallah  El-Assady, Grace  Guo, Duen Horng Chau, Alex  Endert, Daniel  Keim",ArXiv,,,,https://semanticscholar.org/paper/33b298b9a5d4ea81255c7fbdef9ff9c2a0bb081f,,"This paper systematically derives design dimensions for the structured evaluation of explainable artificial intelligence (XAI) approaches. These dimensions enable a descriptive characterization, facilitating comparisons between different study designs. They further structure the design space of XAI, converging towards a precise terminology required for a rigorous study of XAI. Our literature review differentiates between comparative studies and application papers, revealing methodological differences between the fields of machine learning, human-computer interaction, and visual analytics. Generally, each of these disciplines targets specific parts of the XAI process. Bridging the resulting gaps enables a holistic evaluation of XAI in real-world scenarios, as proposed by our conceptual model characterizing bias sources and trust-building. Furthermore, we identify and discuss the potential for future work based on observed research gaps that should lead to better coverage of the proposed model.",,Review
An agile framework for trustworthy AI,The AI High Level Expert Group (AI-HLEG) guidelines present a list of requirements that trustworthy AI systems should meet.,Search,2020,2,"Stefan  Leijnen, Huib  Aldewereld, Rudy van Belkom, Roland  Bijvank, Roelant  Ossewaarde",NeHuAI@ECAI,,,,https://semanticscholar.org/paper/880049a16c8fea47dcfe07450668f5507db5e96d,,"From the article: The ethics guidelines put forward by the AI High Level Expert Group (AI-HLEG) present a list of seven key requirements that Human-centered, trustworthy AI systems should meet. These guidelines are useful for the evaluation of AI systems, but can be complemented by applied methods and tools for the development of trustworthy AI systems in practice. In this position paper we propose a framework for translating the AI-HLEG ethics guidelines into the specific context within which an AI system operates. This approach aligns well with a set of Agile principles commonly employed in software engineering. http://ceur-ws.org/Vol-2659/",,
Can we trust AI? An empirical investigation of trust requirements and guide to successful AI adoption,"The factors that increase overall trust in AI are access to knowledge, transparency, explainability, certification, and self-imposed standards and guidelines.",Search,2021,2,"Patrick  Bedué, Albrecht  Fritzsche",,,10.1108/JEIM-06-2020-0233,https://doi.org/10.1108/JEIM-06-2020-0233,https://semanticscholar.org/paper/2590e2a851c6c9fc61029377c1e451afdb408004,,"Purpose Artificial intelligence (AI) fosters economic growth and opens up new directions for innovation. However, the diffusion of AI proceeds very slowly and falls behind, especially in comparison to other technologies. An important path leading to better adoption rates identified is trust-building. Particular requirements for trust and their relevance for AI adoption are currently insufficiently addressed.Design/methodology/approachTo close this gap, the authors follow a qualitative approach, drawing on the extended valence framework by assessing semi-structured interviews with experts from various companies.FindingsThe authors contribute to research by finding several subcategories for the three main trust dimensions ability, integrity and benevolence, thereby revealing fundamental differences for building trust in AI compared to more traditional technologies. In particular, the authors find access to knowledge, transparency, explainability, certification, as well as self-imposed standards and guidelines to be important factors that increase overall trust in AI.Originality/valueThe results show how the valence framework needs to be elaborated to become applicable to the AI context and provide further structural orientation to better understand AI adoption intentions. This may help decision-makers to identify further requirements or strategies to increase overall trust in their AI products, creating competitive and operational advantage.",,
TrustyAI Explainability Toolkit,The TrustyAI initiative looks into explainable artificial intelligence solutions to address trustworthiness in machine learning.,Search,2021,,"Rob  Geada, Tommaso  Teofili, Rui  Vieira, Rebecca  Whitworth, Daniele  Zonca",ArXiv,,,,https://semanticscholar.org/paper/b915cbc9d5c9f5c063b6f76a71e8d0b151d066c5,,"Artificial intelligence (AI) is becoming increasingly more popular and can be found in workplaces and homes around the world. However, how do we ensure trust in these systems? Regulation changes such as the GDPR mean that users have a right to understand how their data has been processed as well as saved. Therefore if, for example, you are denied a loan you have the right to ask why. This can be hard if the method for working this out uses “black box” machine learning techniques such as neural networks. TrustyAI is a new initiative which looks into explainable artificial intelligence (XAI) solutions to address trustworthiness in ML as well as decision services landscapes. In this paper we will look at how TrustyAI can support trust in decision services and predictive models. We investigate techniques such as LIME, SHAP and counterfactuals, benchmarking both LIME and counterfactual techniques against existing implementations. We also look into an extended version of SHAP, which supports background data selection to be evaluated based on quantitative data and allows for error bounds.",,
Trustworthy AI,The pursuit of responsible AI raises the ante on both the trustworthy computing and formal methods communities.,Search,2021,7,Jeannette M. Wing,Commun. ACM,,10.1145/3448248,https://doi.org/10.1145/3448248,https://semanticscholar.org/paper/33cf9b4d6c76f988380b1adff2c06c30010f93d3,https://dl.acm.org/doi/pdf/10.1145/3448248,The pursuit of responsible AI raises the ante on both the trustworthy computing and formal methods communities.,,
"AI based Computational Trust Model for Intelligent Virtual
 Assistant",A voice-controlled intelligent virtual assistant (IVA) is very useful for illiterate and visually impaired people.,Search,2021,,"Babu  Kumar, Ajay Vikram Singh, Parul  Agarwal",,,10.29252/JIST.8.32.263,https://doi.org/10.29252/JIST.8.32.263,https://semanticscholar.org/paper/100f712248df91d413b4441765f93adb87577d52,http://jist.ir/Article/Download/15536,"The Intelligent virtual assistant (IVA) also called AI assistant or digital assistant is software developed as a product by organizations like Google, Apple, Microsoft and Amazon. Virtual assistant based on Artificial Intelligence which works and processes on natural language commands given by humans. It helps the user to work more efficiently and also saves time. It is human friendly as it works on natural language commands given by humans. Voice-controlled Intelligent Virtual Assistants (IVAs) have seen gigantic development as of late on cell phones and as independent gadgets in individuals’ homes. The intelligent virtual assistant is very useful for illiterate and visually impaired people around the world. While research has analyzed the expected advantages and downsides of these gadgets for IVA clients, barely any investigations have exactly assessed the need of security and trust as a singular choice to use IVAs. In this proposed work, different IPA users and non-users (N=1000) are surveyed to understand and analyze the barriers and motivations to adopting IPAs and how users are concerned about data privacy and trust with respect to organizational compliances and social contract related to IPA data and how these concerns have affected the acceptance and use of IPAs. We have used Naive Byes Classifier to compute trust in IVA devices and further evaluate probability of using different trusted IVA devices.",,
Trust in Distributed Artificial Intelligence,Trust allows interactions between agents where there may have been no effective interaction possible before trust.,Search,1992,90,Stephen  Marsh,MAAMAW,,10.1007/3-540-58266-5_6,https://doi.org/10.1007/3-540-58266-5_6,https://semanticscholar.org/paper/6d296cd0ddaccb6d01aa197c9ba5a04afee2d399,,"A discussion of trust is presented which focuses on multiagent systems, from the point of view of one agent in a system. The roles trust plays in various forms of interaction are considered, with the view that trust allows interactions between agents where there may have been no effective interaction possible before trust. Trust allows parties to acknowledge that, whilst there is a risk in relationships with potentially malevolent agents, some form of interaction may produce benefits, where no interaction at all may not. In addition, accepting the risk allows the trusting agent to prepare itself for possibly irresponsible or untrustworthy behaviour, thus minimizing the potential damage caused. A formalism is introduced to clarify these notions, and to permit computer simulations. An important contribution of this work is that the formalism is not allen-compassing: there are some notions of trust that are excluded. What it describes is a specific view of trust.",,
Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims,Various steps can improve the verifiability of claims made about AI systems and their associated development processes.,Search,2020,90,"Miles  Brundage, Shahar  Avin, Jasmine  Wang, Haydn  Belfield, Gretchen  Krueger, Gillian  Hadfield, Heidy  Khlaaf, Jingying  Yang, Helen  Toner, Ruth  Fong, Tegan  Maharaj, Pang Wei Koh, Sara  Hooker, Jade  Leung, Andrew  Trask, Emma  Bluemke, Jonathan  Lebensbold, Cullen  O'Keefe, Mark  Koren, Th'eo  Ryffel, JB  Rubinovitz, Tamay  Besiroglu, Federica  Carugati, Jack  Clark, Peter  Eckersley, Sarah de Haas, Maritza  Johnson, Ben  Laurie, Alex  Ingerman, Igor  Krawczuk, Amanda  Askell, Rosario  Cammarota, Andrew  Lohn, David  Krueger, Charlotte  Stix, Peter  Henderson, Logan  Graham, Carina  Prunkl, Bianca  Martin, Elizabeth  Seger, Noa  Zilberman, Se'an 'O h'Eigeartaigh, Frens  Kroeger, Girish  Sastry, Rebecca  Kagan, Adrian  Weller, Brian  Tse, Elizabeth  Barnes, Allan  Dafoe, Paul  Scharre, Ariel  Herbert-Voss, Martijn  Rasser, Shagun  Sodhani, Carrick  Flynn, Thomas Krendl Gilbert, Lisa  Dyer, Saif  Khan, Yoshua  Bengio, Markus  Anderljung",ArXiv,,,,https://semanticscholar.org/paper/62c3142956d54db158d190ce691e3c13e7897412,,"With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.",,