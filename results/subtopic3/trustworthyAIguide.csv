Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
An agile framework for trustworthy AI,The AI High Level Expert Group (AI-HLEG) ethics guidelines present a list of requirements that trustworthy AI systems should meet.,Search,2020,2,"Stefan  Leijnen, Huib  Aldewereld, Rudy van Belkom, Roland  Bijvank, Roelant  Ossewaarde",NeHuAI@ECAI,,,,https://semanticscholar.org/paper/880049a16c8fea47dcfe07450668f5507db5e96d,,"From the article: The ethics guidelines put forward by the AI High Level Expert Group (AI-HLEG) present a list of seven key requirements that Human-centered, trustworthy AI systems should meet. These guidelines are useful for the evaluation of AI systems, but can be complemented by applied methods and tools for the development of trustworthy AI systems in practice. In this position paper we propose a framework for translating the AI-HLEG ethics guidelines into the specific context within which an AI system operates. This approach aligns well with a set of Agile principles commonly employed in software engineering. http://ceur-ws.org/Vol-2659/",,
Establishing the rules for building trustworthy AI,The European Commission’s report ‘Ethics guidelines for trustworthy AI’ facilitates international support for AI solutions that are good for humanity and the environment.,Search,2019,67,Luciano  Floridi,Nature Machine Intelligence,,10.1038/S42256-019-0055-Y,https://doi.org/10.1038/S42256-019-0055-Y,https://semanticscholar.org/paper/dc44e2be0f85b6225f05390c570885337a99ef83,https://philpapers.org/archive/FLOETR.pdf,"The European Commission’s report ‘Ethics guidelines for trustworthy AI’ provides a clear benchmark to evaluate the responsible development of AI systems, and facilitates international support for AI solutions that are good for humanity and the environment, says Luciano Floridi.",,
Trustworthy AI: From Principles to Practices,"A comprehensive guide towards building trustworthy AI systems is offered, including from data acquisition to model development.",Search,2021,1,"Bo  Li, Peng  Qi, Bo  Liu, Shuai  Di, Jingen  Liu, Jiquan  Pei, Jinfeng  Yi, Bowen  Zhou",ArXiv,,,,https://semanticscholar.org/paper/c3689493757f90267908e776aeada9194fce55c7,,"Fast developing artificial intelligence (AI) technology has enabled various applied systems deployed in the real world, impacting people’s everyday lives. However, many current AI systems were found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection, etc., which not only degrades user experience but erodes the society’s trust in all AI systems. In this review, we strive to provide AI practitioners a comprehensive guide towards building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, alignment with human values, and accountability. We then survey leading approaches in these aspects in the industry. To unify the current fragmented approaches towards trustworthy AI, we propose a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items to practitioners and societal stakeholders (e.g., researchers and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges in the future development of trustworthy AI systems, where we identify the need for paradigm shift towards comprehensive trustworthy AI systems.",,Review
Designing Trustworthy AI: A Human-Machine Teaming Framework to Guide Development,"The Human-Machine Teaming Framework for Designing Ethical AI Experiences will guide AI development teams to create AI systems that are accountable, de-risked, respectful, secure, honest, and usable.",Search,2019,4,Carol J. Smith,ArXiv,,10.1184/R1/12119847.V1,https://doi.org/10.1184/R1/12119847.V1,https://semanticscholar.org/paper/e4213c10a8894e9a767633ed12a605fd65beb95b,,"Artificial intelligence (AI) holds great promise to empower us with knowledge and augment our effectiveness. We can -- and must -- ensure that we keep humans safe and in control, particularly with regard to government and public sector applications that affect broad populations. How can AI development teams harness the power of AI systems and design them to be valuable to humans? Diverse teams are needed to build trustworthy artificial intelligent systems, and those teams need to coalesce around a shared set of ethics. There are many discussions in the AI field about ethics and trust, but there are few frameworks available for people to use as guidance when creating these systems. The Human-Machine Teaming (HMT) Framework for Designing Ethical AI Experiences described in this paper, when used with a set of technical ethics, will guide AI development teams to create AI systems that are accountable, de-risked, respectful, secure, honest, and usable. To support the team's efforts, activities to understand people's needs and concerns will be introduced along with the themes to support the team's efforts. For example, usability testing can help determine if the audience understands how the AI system works and complies with the HMT Framework. The HMT Framework is based on reviews of existing ethical codes and best practices in human-computer interaction and software development. Human-machine teams are strongest when human users can trust AI systems to behave as expected, safely, securely, and understandably. Using the HMT Framework to design trustworthy AI systems will provide support to teams in identifying potential issues ahead of time and making great experiences for humans.",,Review
Comments on the “Draft Ethics Guidelines for Trustworthy AI” by the High-LevelExpert Group on Artificial Intelligence.,The European Commission appointed the High-Level Expert Group on Artificial Intelligence.,Search,2019,49,Ninja  Marnau,,,,,https://semanticscholar.org/paper/01040c2897ea82bb801931dbfe8a85a3b64d2e7f,,"The European Commission appointed the High-Level Expert Group on Artificial Intelligence (AI HLEG). The AI HLEG has the objective to support the implementation of the European strategy on Artificial Intelligence. This will include the elaboration of recommendations on future-related policy development and on ethical, legal and societal issues related to AI. In January 2019, the Commission asked stakeholders for comments on the AI HLEG’s “Draft Ethics Guidelines for Trustworthy AI”. CISPA submitted the following comments and remarks in the Stakeholders’ Consultation.",,
The EU Approach to Ethics Guidelines for Trustworthy Artificial Intelligence,The EU published Ethics Guidelines for Trustworthy Artificial Intelligence in 2018.,Search,2019,56,Nathalie A. Smuha,Computer Law Review International,,10.9785/cri-2019-200402,https://doi.org/10.9785/cri-2019-200402,https://semanticscholar.org/paper/a00ee7ae4642b986b622e0f48c845e79707b707b,https://lirias.kuleuven.be/bitstream/123456789/640572/2/EU%20Approach%20for%20Trustworthy%20AI%20-%20a%20continuous%20journey.pdf,"As part of its European strategy for Artificial Intelligence (AI), and as a response to the increasing ethical questions raised by this technology, the European Commission established an independent High-Level Expert Group on Artificial Intelligence (AI HLEG) in June 2018. The group was tasked to draft two deliverables: AI Ethics Guidelines and Policy and Investment Recommendations. Nine months later, its first deliverable was published, putting forward a comprehensive framework to achieve “Trustworthy AI” by offering ethical guidance to AI practitioners. This paper dives into the work carried out by the group, focusing in particular on its AI Ethics Guidelines. First, this paper clarifies the context that led to the creation of the AI HLEG and its mandate (I.). Subsequently, it elaborates on the Guidelines’ aim and purpose (II.), and analyses the Guidelines’ drafting process (III.). Particular focus is given to the questions surrounding the respective role played by ethics and law in the AI governance landscape (IV.), as well as some of the challenges that had to be overcome throughout the process (V.). Finally, this paper places the Guidelines in an international context, and sets out the next steps (VI.) ahead on the journey towards an appropriate governance framework for AI (VII.).",,
The Ethical Guidelines for Trustworthy AI – A Procrastination of Effective Law Enforcement,The Ethical Guidelines for Trustworthy Artificial Intelligence by the EU Commission are ineffective.,Search,2019,2,Ramak Molavi Vasse’i,Computer Law Review International,,10.9785/cri-2019-200502,https://doi.org/10.9785/cri-2019-200502,https://semanticscholar.org/paper/64223e4aa1a20a6406cd036417245385b6b33cc0,,"In the august issue of the CRi, Nathalie Smuha, the coordinator of the work of the High-Level Expert Group on AI, outlined the approach and considerations leading to the “The EU Approach to Ethics Guidelines for Trustworthy Artificial Intelligence”. This paper provides a critical assessment of the Ethical Guidelines of the EU Commission and points out why a law enforcement focused approach must be the essential next step towards a beneficial and humane development of AI. Questioning the diversity of the Commssion’s High Level Expert Group on Artificial Intelligence, the dangers of ethics shopping are exposed as well as the UN Universal Declaration of Human Rights explored as already well established alternative reference framework for AI. Having exposed the need for effective red lines, not only the hidden social and ecological cost are assessed, but also the risk of “buying-out” research and other ethical issues neglected in the Ethics Guidelines for Trustworthy Artificial Intelligence. Finally, three key weaknesses concerning the crucial translation of ethical principles into practice (enforcement) are highlighted.",,
Trustworthy AI Development Guidelines for Human System Interaction,AI development guidelines can improve the user trust in AI systems to enhance human-AI interactions.,Search,2020,3,"Chathurika S. Wickramasinghe, Daniel L. Marino, Javier  Grandio, Milos  Manic",2020 13th International Conference on Human System Interaction (HSI),,10.1109/HSI49210.2020.9142644,https://doi.org/10.1109/HSI49210.2020.9142644,https://semanticscholar.org/paper/238fa66062114f39e404c40d0b1abc03b86e54bd,,"Artificial Intelligence (AI) is influencing almost all areas of human life. Even though these AI-based systems frequently provide state-of-the-art performance, humans still hesitate to develop, deploy, and use AI systems. The main reason for this is the lack of trust in AI systems caused by the deficiency of transparency of existing AI systems. As a solution, “Trustworthy AI” research area merged with the goal of defining guidelines and frameworks for improving user trust in AI systems, allowing humans to use them without fear. While trust in AI is an active area of research, very little work exists where the focus is to build human trust to improve the interactions between human and AI systems. In this paper, we provide a concise survey on concepts of trustworthy AI. Further, we present trustworthy AI development guidelines for improving the user trust to enhance the interactions between AI systems and humans, that happen during the AI system life cycle.",,
Can we trust AI? An empirical investigation of trust requirements and guide to successful AI adoption,"The factors that increase overall trust in AI are access to knowledge, transparency, explainability, certification, and self-imposed standards and guidelines.",Search,2021,2,"Patrick  Bedué, Albrecht  Fritzsche",,,10.1108/JEIM-06-2020-0233,https://doi.org/10.1108/JEIM-06-2020-0233,https://semanticscholar.org/paper/2590e2a851c6c9fc61029377c1e451afdb408004,,"Purpose Artificial intelligence (AI) fosters economic growth and opens up new directions for innovation. However, the diffusion of AI proceeds very slowly and falls behind, especially in comparison to other technologies. An important path leading to better adoption rates identified is trust-building. Particular requirements for trust and their relevance for AI adoption are currently insufficiently addressed.Design/methodology/approachTo close this gap, the authors follow a qualitative approach, drawing on the extended valence framework by assessing semi-structured interviews with experts from various companies.FindingsThe authors contribute to research by finding several subcategories for the three main trust dimensions ability, integrity and benevolence, thereby revealing fundamental differences for building trust in AI compared to more traditional technologies. In particular, the authors find access to knowledge, transparency, explainability, certification, as well as self-imposed standards and guidelines to be important factors that increase overall trust in AI.Originality/valueThe results show how the valence framework needs to be elaborated to become applicable to the AI context and provide further structural orientation to better understand AI adoption intentions. This may help decision-makers to identify further requirements or strategies to increase overall trust in their AI products, creating competitive and operational advantage.",,
The relationship between trust in AI and trustworthy machine learning technologies,Trustworthiness technologies can support the required qualities of trustworthy AI.,Search,2020,50,"Ehsan  Toreini, Mhairi  Aitken, Kovila  Coopamootoo, Karen  Elliott, Carlos Gonzalez Zelaya, Aad van Moorsel",FAT*,,10.1145/3351095.3372834,https://doi.org/10.1145/3351095.3372834,https://semanticscholar.org/paper/bd4cf5a6987ef0f39b7e4e88d59b3a3365abcc70,http://arxiv.org/pdf/1912.00782,"To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.",,
Trustworthy AI,The pursuit of responsible AI raises the ante on both the trustworthy computing and formal methods communities.,Search,2021,7,Jeannette M. Wing,Commun. ACM,,10.1145/3448248,https://doi.org/10.1145/3448248,https://semanticscholar.org/paper/33cf9b4d6c76f988380b1adff2c06c30010f93d3,https://dl.acm.org/doi/pdf/10.1145/3448248,The pursuit of responsible AI raises the ante on both the trustworthy computing and formal methods communities.,,
Trustworthy artificial intelligence,A data-driven research framework for Trustworthy AI can facilitate the development of Trustworthy AI.,Search,2021,21,"Scott  Thiebes, Sebastian  Lins, Ali  Sunyaev",Electron. Mark.,,10.1007/S12525-020-00441-4,https://doi.org/10.1007/S12525-020-00441-4,https://semanticscholar.org/paper/9da092d7c7674e96830f8d6713a9a4f8101f984c,https://link.springer.com/content/pdf/10.1007/s12525-020-00441-4.pdf,"Artificial intelligence (AI) brings forth many opportunities to contribute to the wellbeing of individuals and the advancement of economies and societies, but also a variety of novel ethical, legal, social, and technological challenges. Trustworthy AI (TAI) bases on the idea that trust builds the foundation of societies, economies, and sustainable development, and that individuals, organizations, and societies will therefore only ever be able to realize the full potential of AI, if trust can be established in its development, deployment, and use. With this article we aim to introduce the concept of TAI and its five foundational principles (1) beneficence, (2) non-maleficence, (3) autonomy, (4) justice, and (5) explicability. We further draw on these five principles to develop a data-driven research framework for TAI and demonstrate its utility by delineating fruitful avenues for future research, particularly with regard to the distributed ledger technology-based realization of TAI.",,
Opening the software engineering toolbox for the assessment of trustworthy AI,Software engineering and testing practices can be used for the assessment of trustworthy AI.,Search,2020,2,"Mohit Kumar Ahuja, Mohamed-Bachir  Belaid, Pierre  Bernab'e, Mathieu  Collet, Arnaud  Gotlieb, Chhagan  Lal, Dusica  Marijan, Sagar  Sen, Aizaz  Sharif, Helge  Spieker",NeHuAI@ECAI,,,,https://semanticscholar.org/paper/3b0ff6bd000e9c615d024614343f2c1cf12bf124,,"Trustworthiness is a central requirement for the acceptance and success of human-centered artificial intelligence (AI). To deem an AI system as trustworthy, it is crucial to assess its behaviour and characteristics against a gold standard of Trustworthy AI, consisting of guidelines, requirements, or only expectations. While AI systems are highly complex, their implementations are still based on software. The software engineering community has a long-established toolbox for the assessment of software systems, especially in the context of software testing. In this paper, we argue for the application of software engineering and testing practices for the assessment of trustworthy AI. We make the connection between the seven key requirements as defined by the European Commission's AI high-level expert group and established procedures from software engineering and raise questions for future work.",,
Trustworthy AI,A required feature of an AI system is its ability to assign appropriate credit to data sources.,Search,2021,,"Richa  Singh, Mayank  Vatsa, Nalini  Ratha",COMAD/CODS,,10.1145/3430984.3431966,https://doi.org/10.1145/3430984.3431966,https://semanticscholar.org/paper/4f52a1c995d5b7ba2be6d419146dbf9ee6b0316c,,"Modern AI systems are reaping the advantage of novel learning methods. With their increasing usage, we are realizing the limitations and shortfalls of these systems. Brittleness to minor adversarial changes in the input data, ability to explain the decisions, address the bias in their training data, high opacity in terms of revealing the lineage of the system, how they were trained and tested, and under which parameters and conditions they can reliably guarantee a certain level of performance, are some of the most prominent limitations. Ensuring the privacy and security of the data, assigning appropriate credits to data sources, and delivering decent outputs are also required features of an AI system. We propose the tutorial on “Trustworthy AI” to address six critical issues in enhancing user and public trust in AI systems, namely: (i) bias and fairness, (ii) explainability, (iii) robust mitigation of adversarial attacks, (iv) improved privacy and security in model building, (v) being decent, and (vi) model attribution, including the right level of credit assignment to the data sources, model architectures, and transparency in lineage.",,
Traceability for Trustworthy AI: A Review of Models and Tools,A common approach and shared semantics are lacking for reproducibility tools.,Search,2021,3,"Marçal Mora Cantallops, Salvador  Sánchez-Alonso, Elena García Barriocanal, Miguel-Ángel  Sicilia",Big Data Cogn. Comput.,,10.3390/BDCC5020020,https://doi.org/10.3390/BDCC5020020,https://semanticscholar.org/paper/a8defe1d2e28879ff86e3cb5c6c86b2e78901c6f,https://www.mdpi.com/2504-2289/5/2/20/pdf,"Traceability is considered a key requirement for trustworthy artificial intelligence (AI), related to the need to maintain a complete account of the provenance of data, processes, and artifacts involved in the production of an AI model. Traceability in AI shares part of its scope with general purpose recommendations for provenance as W3C PROV, and it is also supported to different extents by specific tools used by practitioners as part of their efforts in making data analytic processes reproducible or repeatable. Here, we review relevant tools, practices, and data models for traceability in their connection to building AI models and systems. We also propose some minimal requirements to consider a model traceable according to the assessment list of the High-Level Expert Group on AI. Our review shows how, although a good number of reproducibility tools are available, a common approach is currently lacking, together with the need for shared semantics. Besides, we have detected that some tools have either not achieved full maturity, or are already falling into obsolescence or in a state of near abandonment by its developers, which might compromise the reproducibility of the research trusted to them.",,Review
Trustworthy AI Implementation (TAII) Framework for AI Systems,,Search,2021,1,Josef  Baker-Brunnbauer,,,10.2139/SSRN.3796799,https://doi.org/10.2139/SSRN.3796799,https://semanticscholar.org/paper/3337ff3dd257ae6cb5d276e34aa06f190ab06265,,"Companies and their stakeholder need practical tools and implementation guidelines besides abstract frameworks to kick off the realization of Artificial Intelligence (AI) ethics. Based on my previous research outcome AI development companies are still in the beginning of this process or have not even started yet. How is it possible to decrease the entry level barrier to kickoff AI ethics implementation? I tackle this question by combining AI ethics research with previous research findings to create the Trustworthy AI Implementation (TAII) framework. A literature review was conducted and that specifies the research and implementation status for each process step. The aim is to kickoff AI ethics and to transfer research and abstract guidelines from academia to business. The TAII process generates a meta perspective on the systemic dependencies of ethics for the company ecosystem. It generates orienteering for the AI ethics kickoff without requiring a deep background in philosophy and considers perspectives of social impact outside the software and data engineering setting. Depending on the legal regulation or area of application, the TAII process can be adapted and used with different regulations and ethical principles.",,Review