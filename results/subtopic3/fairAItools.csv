Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Transparency Tools for Fairness in AI (Luskin),Tools are useful for understanding various dimensions of bias and for reducing a given bias on new data.,Search,2020,1,"Mingliang  Chen, Aria  Shahverdi, Sarah  Anderson, Se Yong Park, Justin  Zhang, Dana  Dachman-Soled, Kristin  Lauter, Min  Wu",ArXiv,,10.1007/978-3-030-58748-2_4,https://doi.org/10.1007/978-3-030-58748-2_4,https://semanticscholar.org/paper/03e9164acb7b1cdf9b8870ede9288c816c5cf7d9,http://arxiv.org/pdf/2007.04484,"We propose new tools for policy-makers to use when assessing and correcting fairness and bias in AI algorithms. The three tools are:

- A new definition of fairness called ""controlled fairness"" with respect to choices of protected features and filters. The definition provides a simple test of fairness of an algorithm with respect to a dataset. This notion of fairness is suitable in cases where fairness is prioritized over accuracy, such as in cases where there is no ""ground truth"" data, only data labeled with past decisions (which may have been biased).

- Algorithms for retraining a given classifier to achieve ""controlled fairness"" with respect to a choice of features and filters. Two algorithms are presented, implemented and tested. These algorithms require training two different models in two stages. We experiment with combinations of various types of models for the first and second stage and report on which combinations perform best in terms of fairness and accuracy.

- Algorithms for adjusting model parameters to achieve a notion of fairness called ""classification parity"". This notion of fairness is suitable in cases where accuracy is prioritized. Two algorithms are presented, one which assumes that protected features are accessible to the model during testing, and one which assumes protected features are not accessible during testing.

We evaluate our tools on three different publicly available datasets. We find that the tools are useful for understanding various dimensions of bias, and that in practice the algorithms are effective in starkly reducing a given observed bias when tested on new data.",,
AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias,AI Fairness 360 is a Python toolkit for algorithmic fairness that helps facilitate the transition of fairness research algorithms to an industrial setting.,Search,2019,113,"Rachel K. E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Jacquelyn A. Martino, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",IBM J. Res. Dev.,,10.1147/jrd.2019.2942287,https://doi.org/10.1147/jrd.2019.2942287,https://semanticscholar.org/paper/333671a5fbbf726f8819138f3670524ec0405726,,"Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This article introduces a new open-source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license (

https://github.com/ibm/aif360

). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms for use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience that provides a gentle introduction to the concepts and capabilities for line-of-business users, researchers, and developers to extend the toolkit with their new algorithms and improvements and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.",,
"AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias","AI Fairness 360 is a toolkit for detecting, understanding, and mitigating unwanted algorithmic bias.",Search,2018,316,"Rachel K. E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Jacquelyn  Martino, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",ArXiv,,,,https://semanticscholar.org/paper/c8541b1dc813f3a638d7acc79e5f972e77f3c5a7,,"Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license {this https URL). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms.

The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (this https URL) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.",,
Fairlearn: A toolkit for assessing and improving fairness in AI,Fairlearn is a toolkit to assess and improve the fairness of AI systems.,Search,2020,48,"Sarah  Bird, Miro  Dudík, Richard  Edgar, Brandon  Horn, Roman  Lutz, Vanessa  Milan, Mehrnoosh  Sameki, Hanna  Wallach, Kathleen  Walker",,,,,https://semanticscholar.org/paper/5894d57ea49bd5c136ebefb1e6c3986555908ea0,,"We introduce Fairlearn, an open source toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems. Fairlearn has two components: an interactive visualization dashboard and unfairness mitigation algorithms. These components are designed to help with navigating trade-offs between fairness and model performance. We emphasize that prioritizing fairness in AI systems is a sociotechnical challenge. Because there are many complex sources of unfairness—some societal and some technical—it is not possible to fully “debias” a system or to guarantee fairness; the goal is to mitigate fairness-related harms as much as possible. As Fairlearn grows to include additional fairness metrics, unfairness mitigation algorithms, and visualization capabilities, we hope that it will be shaped by a diverse community of stakeholders, ranging from data scientists, developers, and business decision makers to the people whose lives may be affected by the predictions of AI systems.",,
HPCFAIR: Enabling FAIR AI for HPC Applications,"A modular, extensible framework to enable AI models to be Findable, Accessible, Interoperable and Reproducible (FAIR) is proposed.",Search,2021,,"Gaurav  Verma, Murali  Emani, Chunhua  Liao, Pei-Hung  Lin, Tristan  Vanderbruggen, Xipeng  Shen, Barbara M. Chapman",2021 IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments (MLHPC),,10.1109/mlhpc54614.2021.00011,https://doi.org/10.1109/mlhpc54614.2021.00011,https://semanticscholar.org/paper/e3e6cb8547011b57644526f0c182da991fa6062d,,"Artificial Intelligence (AI) is being adopted in different domains at an unprecedented scale. A significant interest in the scientific community also involves leveraging machine learning (ML) to effectively run high performance computing applications at scale. Given multiple efforts in this arena, there are often duplicated efforts when existing rich data sets and ML models could be leveraged instead. The primary challenge is a lack of an ecosystem to reuse and reproduce the models and datasets. In this work, we propose HPCFAIR, a modular, extensible framework to enable AI models to be Findable, Accessible, Interoperable and Reproducible (FAIR). It enables users with a structured approach to search, load, save and reuse the models in their codes. We present the design, implementation of our framework and highlight how it can be seamlessly integrated to ML-driven applications for high performance computing applications and scientific machine learning workloads.",,
Introduction to AI Fairness,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern.",Search,2020,2,"Yunfeng  Zhang, Rachel K. E. Bellamy, Moninder  Singh, Q. Vera Liao",CHI Extended Abstracts,,10.1145/3334480.3375059,https://doi.org/10.1145/3334480.3375059,https://semanticscholar.org/paper/d49f8d57240cd04dd66d91bf53c639497139ab3a,,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. Recently, the AI research community has proposed many methods to measure and mitigate unwanted biases, and developed open-source toolkits for developers to make fair AI. This course will cover the recent development in algorithmic fairness, including the many different definitions of fairness, their corresponding quantitative measurements, and ways to mitigate biases. This course is open to beginners and is designed for anyone interested in the topic of AI fairness.",,
Introduction to AI Fairness,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern.",Search,2021,,"Yunfeng  Zhang, Rachel K. E. Bellamy, Q. Vera Liao, Moninder  Singh",CHI Extended Abstracts,,10.1145/3411763.3444998,https://doi.org/10.1145/3411763.3444998,https://semanticscholar.org/paper/e24d854f822bebd531926eee518d4b2e1455e5de,,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. Recently, the AI research community has proposed many methods to measure and mitigate unwanted biases, and developed open-source toolkits for developers to make fair AI. This course will cover the recent development in algorithmic fairness, including the many different definitions of fairness, their corresponding quantitative measurements, and ways to mitigate biases. This course is open to beginners and is designed for anyone interested in the topic of AI fairness.",,
Design Methods for Artificial Intelligence Fairness and Transparency,"Fairness and transparency in artificial intelligence continue to become more prevalent topics for research, design and development.",Search,2021,,"Simone  Stumpf, Lorenzo  Strappelli, Subeida  Ahmed, Yuri  Nakao, Aisha  Naseer, Giulia Del Gamba, Daniele  Regoli",IUI Workshops,,,,https://semanticscholar.org/paper/baf8cb94908738c3b2d040a6b44d812e5ca80396,,"Fairness and transparency in artificial intelligence (AI) continue to become more prevalent as topics for research, design and development. General principles and guidelines for designing ethical and responsible AI systems have been proposed, yet there is a lack of design methods for these kinds of systems. In this paper, we present CoFAIR, a novel method to design user interfaces for exploring fairness, consisting of series of co-design workshops, and wider evaluation. This method can be readily applied in practice by researchers, designers and developers to create responsible and ethical AI systems.",,
Bias in Multimodal AI: Testbed for Fair Automatic Recruitment,FairCVtest is a fictitious automated recruitment testbed able to show the capacity of the AI behind such recruitment tool to extract sensitive information from unstructured data.,Search,2020,16,"Alejandro  Pena, Ignacio  Serna, Aythami  Morales, Julian  Fierrez",2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00022,https://doi.org/10.1109/CVPRW50498.2020.00022,https://semanticscholar.org/paper/e924bfb95435153185f8d89e77f5a3534e2a29bd,http://arxiv.org/pdf/2004.07173,"The presence of decision-making algorithms in society is rapidly increasing nowadays, while concerns about their transparency and the possibility of these algorithms becoming new sources of discrimination are arising. In fact, many relevant automated systems have been shown to make decisions based on sensitive information or discriminate certain social groups (e.g. certain biometric systems for person recognition). With the aim of studying how current multimodal algorithms based on heterogeneous sources of information are affected by sensitive elements and inner biases in the data, we propose a fictitious automated recruitment testbed: FairCVtest. We train automatic recruitment algorithms using a set of multimodal synthetic profiles consciously scored with gender and racial biases. Fair-CVtest shows the capacity of the Artificial Intelligence (AI) behind such recruitment tool to extract sensitive information from unstructured data, and exploit it in combination to data biases in undesirable (unfair) ways. Finally, we present a list of recent works developing techniques capable of removing sensitive information from the decision-making process of deep learning architectures. We have used one of these algorithms (SensitiveNets) to experiment discrimination-aware learning for the elimination of sensitive information in our multimodal AI framework. Our methodology and results show how to generate fairer AI-based tools in general, and in particular fairer automated recruitment systems.",,
"Fairkit, Fairkit, on the Wall, Who's the Fairest of Them All? Supporting Data Scientists in Training Fair Models",Fairkit-learn is a toolkit for helping data scientists reason about and understand fairness.,Search,2020,6,"Brittany  Johnson, Jesse  Bartola, Rico  Angell, Katherine  Keith, Sam  Witty, Stephen J. Giguere, Yuriy  Brun",ArXiv,,,,https://semanticscholar.org/paper/5c072297547c7f012ddd687d381b04183973af98,,"Modern software relies heavily on data and machine learning, and affects decisions that shape our world. Unfortunately, recent studies have shown that because of biases in data, software systems frequently inject bias into their decisions, from producing better closed caption transcriptions of men’s voices than of women’s voices to overcharging people of color for financial loans. To address bias in machine learning, data scientists need tools that help them understand the trade-offs between model quality and fairness in their specific data domains. Toward that end, we present fairkit-learn, a toolkit for helping data scientists reason about and understand fairness. Fairkit-learn works with state-of-the-art machine learning tools and uses the same interfaces to ease adoption. It can evaluate thousands of models produced by multiple machine learning algorithms, hyperparameters, and data permutations, and compute and visualize a small Pareto-optimal set of models that describe the optimal trade-offs between fairness and quality. We evaluate fairkitlearn via a user study with 54 students, showing that students using fairkit-learn produce models that provide a better balance between fairness and quality than students using scikit-learn and IBM AI Fairness 360 toolkits. With fairkit-learn, users can select models that are up to 67% more fair and 10% more accurate than the models they are likely to train with scikit-learn.",,
FairCVtest Demo: Understanding Bias in Multimodal Learning with a Testbed in Fair Automatic Recruitment,The presence of decision-making algorithms in society is rapidly increasing nowadays.,Search,2020,3,"Alejandro  Pena, Ignacio  Serna, Aythami  Morales, Julian  Fierrez",ICMI,,10.1145/3382507.3421165,https://doi.org/10.1145/3382507.3421165,https://semanticscholar.org/paper/225b951b1480d0be26ce65e845fc8ee617d994fc,,"With the aim of studying how current multimodal AI algorithms based on heterogeneous sources of information are affected by sensitive elements and inner biases in the data, this demonstrator experiments over an automated recruitment testbed based on Curriculum Vitae: FairCVtest. The presence of decision-making algorithms in society is rapidly increasing nowadays, while concerns about their transparency and the possibility of these algorithms becoming new sources of discrimination are arising. This demo shows the capacity of the Artificial Intelligence (AI) behind a recruitment tool to extract sensitive information from unstructured data, and exploit it in combination to data biases in undesirable (unfair) ways. Aditionally, the demo includes a new algorithm (SensitiveNets) for discrimination-aware learning which eliminates sensitive information in our multimodal AI framework.",,
Fairway: a way to build fair ML software,Fairway removes ethical bias from training data and trained models without damaging the predictive performance.,Search,2020,25,"Joymallya  Chakraborty, Suvodeep  Majumder, Zhe  Yu, Tim  Menzies",ESEC/SIGSOFT FSE,,10.1145/3368089.3409697,https://doi.org/10.1145/3368089.3409697,https://semanticscholar.org/paper/cafef47bdd70002ba9c599b5ab28ed056940a35e,http://arxiv.org/pdf/2003.10354,"Machine learning software is increasingly being used to make decisions that affect people's lives. But sometimes, the core part of this software (the learned model), behaves in a biased manner that gives undue advantages to a specific group of people (where those groups are determined by sex, race, etc.). This ""algorithmic discrimination"" in the AI software systems has become a matter of serious concern in the machine learning and software engineering community. There have been works done to find ""algorithmic bias"" or ""ethical bias"" in the software system. Once the bias is detected in the AI software system, the mitigation of bias is extremely important. In this work, we a)explain how ground-truth bias in training data affects machine learning model fairness and how to find that bias in AI software,b)propose a method Fairway which combines pre-processing and in-processing approach to remove ethical bias from training data and trained model. Our results show that we can find bias and mitigate bias in a learned model, without much damaging the predictive performance of that model. We propose that (1) testing for bias and (2) bias mitigation should be a routine part of the machine learning software development life cycle. Fairway offers much support for these two purposes.",,
Fairway: SE Principles for Building Fairer Software,Fairway removes ethical bias from training data and trained model without much damaging the predictive performance.,Search,2020,1,"Joymallya  Chakraborty, Suvodeep  Majumder, Zhe  Wu, Tim  Menzies",ArXiv,,,,https://semanticscholar.org/paper/21e8e6da482cf1aeb18e8a449d84b1c6da8770b6,,"Machine learning software is increasingly being used to make decisions that affect people’s lives. But sometimes, the core part of this software (the learned model), behaves in a biased manner that gives undue advantages to a specific group of people (where those groups are determined by sex, race, etc.). This “algorithmic discrimination” in the AI software systems has become a matter of serious concern in the machine learning and software engineering community. There have been works done to find “algorithmic bias” or “ethical bias” in software system. Once the bias is detected in the AI software system, mitigation of bias is extremely important. In this work, we a) explain how ground truth bias in training data affects machine learning model fairness and how to find that bias in AI software, b) propose a method Fairway which combines preprocessing and in-processing approach to remove ethical bias from training data and trained model. Our results show that we can find bias and mitigate bias in a learned model, without much damaging the predictive performance of that model. We propose that (1) testing for bias and (2) bias mitigation should be a routine part of the machine learning software development life cycle. Fairway offers much support for these two purposes.",,
Algorithmic Fairness,Algorithms now touch on many aspects of people's lives and must be fair and objective.,Search,2020,146,"Dana  Pessach, Erez  Shmueli",ArXiv,,,,https://semanticscholar.org/paper/27cd5a3eb55d2df2a4c06e96247b79f215516a67,,"An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence (AI) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop AI algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision-making may be inherently prone to unfairness, even when there is no intention for it. This paper presents an overview of the main concepts of identifying, measuring and improving algorithmic fairness when using AI algorithms. The paper begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, towards a better understanding of which mechanisms should be used in different scenarios. The paper then describes the most commonly used fairness-related datasets in this field. Finally, the paper ends by reviewing several emerging research sub-fields of algorithmic fairness.",,Review
Tools and Practices for Responsible AI Engineering,"Hydra-zen simplifies the process of making complex AI applications configurable, and their behaviors reproducible.",Search,2022,,"Ryan  Soklaski, Justin  Goodwin, Olivia  Brown, Michael  Yee, Jason  Matterer",,,,,https://semanticscholar.org/paper/9a15c604696e672d2ccbcc07d36bf38ba3817bba,,"Responsible Artificial Intelligence (AI)—the practice of developing, evaluating, and maintaining accurate AI systems that also exhibit essential properties such as robustness and explainability—represents a multifaceted challenge that often stretches standard machine learning tooling, frameworks, and testing methods beyond their limits. In this paper, we present two new software libraries—hydra-zen and the rAI-toolbox—that address critical needs for responsible AI engineering. hydra-zen dramatically simplifies the process of making complex AI applications configurable, and their behaviors reproducible. The rAI-toolbox is designed to enable methods for evaluating and enhancing the robustness of AI-models in a way that is scalable and that composes naturally with other popular ML frameworks. We describe the design principles and methodologies that make these tools effective, including the use of property-based testing to bolster the reliability of the tools themselves. Finally, we demonstrate the composability and flexibility of the tools by showing how various use cases from adversarial robustness and explainable AI can be concisely implemented with familiar APIs.",,
FAIR AI: A Conceptual Framework for Democratisation of 21st Century AI,New methods can automatically create interpretable neural network models without the help of AI-experts.,Search,2021,,Saman  Halgamuge,"2021 International Conference on Instrumentation, Control, and Automation (ICA)",,10.1109/ICA52848.2021.9625672,https://doi.org/10.1109/ICA52848.2021.9625672,https://semanticscholar.org/paper/cae45a30452713cacad1e5ec04adfb4a065f6a8e,,"Popular models of AI have two significant deficiencies: 1) they are mostly manually designed using the experience of AI-experts 2) they lack human interpretability, i.e., users cannot make sense of the functionality of neural network architectures either semantically/linguistically or mathematically. This lack of interpretability is a main inhibitor of broader use of 21st century AI, e.g., Deep Neural Networks (DNN). The dependence on AI experts to create AI hinders the democratisation of AI and therefore the accessibility to AI. Addressing these deficiencies would provide answers to some of the valid questions about traceability, accountability and the ability to integrate existing knowledge (scientific or linguistically articulated human experience) into the model. This keynote abstract addresses these two significant deficiencies that inhibit the democratisation of AI by developing new methods that can automatically create interpretable neural network models without the help of AI-experts. The proposed cross-fertilised innovation will have a profound impact on the society through the increased accessibility and trustworthiness of AI beneficial to almost all areas of sciences, engineering, and humanities.",,