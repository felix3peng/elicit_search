Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Opening the software engineering toolbox for the assessment of trustworthy AI,Software engineering and testing practices can be used for the assessment of trustworthy AI.,Search,2020,2,"Mohit Kumar Ahuja, Mohamed-Bachir  Belaid, Pierre  Bernab'e, Mathieu  Collet, Arnaud  Gotlieb, Chhagan  Lal, Dusica  Marijan, Sagar  Sen, Aizaz  Sharif, Helge  Spieker",NeHuAI@ECAI,,,,https://semanticscholar.org/paper/3b0ff6bd000e9c615d024614343f2c1cf12bf124,,"Trustworthiness is a central requirement for the acceptance and success of human-centered artificial intelligence (AI). To deem an AI system as trustworthy, it is crucial to assess its behaviour and characteristics against a gold standard of Trustworthy AI, consisting of guidelines, requirements, or only expectations. While AI systems are highly complex, their implementations are still based on software. The software engineering community has a long-established toolbox for the assessment of software systems, especially in the context of software testing. In this paper, we argue for the application of software engineering and testing practices for the assessment of trustworthy AI. We make the connection between the seven key requirements as defined by the European Commission's AI high-level expert group and established procedures from software engineering and raise questions for future work.",,
On Assessing Trustworthy AI in Healthcare. Machine Learning as a Supportive Tool to Recognize Cardiac Arrest in Emergency Calls,Artificial Intelligence (AI) brings potential risks that may cause unintended harm.,Search,2021,1,"Roberto V. Zicari, James  Brusseau, Stig Nikolaj Blomberg, Helle Collatz Christensen, Megan  Coffee, Marianna B. Ganapini, Sara  Gerke, Thomas Krendl Gilbert, Eleanore  Hickman, Elisabeth  Hildt, Sune  Holm, Ulrich  Kühne, Vince I. Madai, Walter  Osika, Andy  Spezzatti, Eberhard  Schnebel, Jesmin Jahan Tithi, Dennis  Vetter, Magnus  Westerlund, Renee  Wurth, Julia  Amann, Vegard  Antun, Valentina  Beretta, Frédérick  Bruneault, Erik  Campano, Boris  Düdder, Alessio  Gallucci, Emmanuel  Goffi, Christoffer Bjerre Haase, Thilo  Hagendorff, Pedro  Kringen, Florian  Möslein, Davi  Ottenheimer, Matiss  Ozols, Laura  Palazzani, Martin  Petrin, Karin  Tafur, Jim  Tørresen, Holger  Volland, Georgios  Kararigas",Frontiers in Human Dynamics,,10.3389/fhumd.2021.673104,https://doi.org/10.3389/fhumd.2021.673104,https://semanticscholar.org/paper/20ed0b58d386edfd2867279815fed7b3ddac4b6a,https://www.frontiersin.org/articles/10.3389/fhumd.2021.673104/pdf,"Artificial Intelligence (AI) has the potential to greatly improve the delivery of healthcare and other services that advance population health and wellbeing. However, the use of AI in healthcare also brings potential risks that may cause unintended harm. To guide future developments in AI, the High-Level Expert Group on AI set up by the European Commission (EC), recently published ethics guidelines for what it terms “trustworthy” AI. These guidelines are aimed at a variety of stakeholders, especially guiding practitioners toward more ethical and more robust applications of AI. In line with efforts of the EC, AI ethics scholarship focuses increasingly on converting abstract principles into actionable recommendations. However, the interpretation, relevance, and implementation of trustworthy AI depend on the domain and the context in which the AI system is used. The main contribution of this paper is to demonstrate how to use the general AI HLEG trustworthy AI guidelines in practice in the healthcare domain. To this end, we present a best practice of assessing the use of machine learning as a supportive tool to recognize cardiac arrest in emergency calls. The AI system under assessment is currently in use in the city of Copenhagen in Denmark. The assessment is accomplished by an independent team composed of philosophers, policy makers, social scientists, technical, legal, and medical experts. By leveraging an interdisciplinary team, we aim to expose the complex trade-offs and the necessity for such thorough human review when tackling socio-technical applications of AI in healthcare. For the assessment, we use a process to assess trustworthy AI, called 1 Z-Inspection® to identify specific challenges and potential ethical trade-offs when we consider AI in practice.",,Review
Traceability for Trustworthy AI: A Review of Models and Tools,A common approach and shared semantics are lacking for reproducibility tools.,Search,2021,3,"Marçal Mora Cantallops, Salvador  Sánchez-Alonso, Elena García Barriocanal, Miguel-Ángel  Sicilia",Big Data Cogn. Comput.,,10.3390/BDCC5020020,https://doi.org/10.3390/BDCC5020020,https://semanticscholar.org/paper/a8defe1d2e28879ff86e3cb5c6c86b2e78901c6f,https://www.mdpi.com/2504-2289/5/2/20/pdf,"Traceability is considered a key requirement for trustworthy artificial intelligence (AI), related to the need to maintain a complete account of the provenance of data, processes, and artifacts involved in the production of an AI model. Traceability in AI shares part of its scope with general purpose recommendations for provenance as W3C PROV, and it is also supported to different extents by specific tools used by practitioners as part of their efforts in making data analytic processes reproducible or repeatable. Here, we review relevant tools, practices, and data models for traceability in their connection to building AI models and systems. We also propose some minimal requirements to consider a model traceable according to the assessment list of the High-Level Expert Group on AI. Our review shows how, although a good number of reproducibility tools are available, a common approach is currently lacking, together with the need for shared semantics. Besides, we have detected that some tools have either not achieved full maturity, or are already falling into obsolescence or in a state of near abandonment by its developers, which might compromise the reproducibility of the research trusted to them.",,Review
Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence,The reliability of smart industrial systems can be assessed online with machine learning.,Search,2021,81,"Zhihan  Lv, Yang  Han, Amit Kumar Singh, Gunasekaran  Manogaran, Haibin  Lv",IEEE Transactions on Industrial Informatics,,10.1109/TII.2020.2994747,https://doi.org/10.1109/TII.2020.2994747,https://semanticscholar.org/paper/79086f67c5d7413a05305478b1b38781588ed19d,,"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.",,
Establishing the rules for building trustworthy AI,The European Commission’s report ‘Ethics guidelines for trustworthy AI’ facilitates international support for AI solutions that are good for the environment.,Search,2019,67,Luciano  Floridi,Nature Machine Intelligence,,10.1038/S42256-019-0055-Y,https://doi.org/10.1038/S42256-019-0055-Y,https://semanticscholar.org/paper/dc44e2be0f85b6225f05390c570885337a99ef83,https://philpapers.org/archive/FLOETR.pdf,"The European Commission’s report ‘Ethics guidelines for trustworthy AI’ provides a clear benchmark to evaluate the responsible development of AI systems, and facilitates international support for AI solutions that are good for humanity and the environment, says Luciano Floridi.",,
"A Review of Trust in Artificial Intelligence: Challenges, Vulnerabilities and Future Directions","Artificial Intelligence (AI) can benefit society, but it is also fraught with risks.",Search,2021,4,"Steven  Lockey, Nicole M. Gillespie, Daniel  Holm, Ida Asadi Someh",HICSS,,10.24251/HICSS.2021.664,https://doi.org/10.24251/HICSS.2021.664,https://semanticscholar.org/paper/1f11e05c5484ff5a066eab37c6268ad7aceaac8d,http://scholarspace.manoa.hawaii.edu/bitstream/10125/71284/1/0534.pdf,"Artificial Intelligence (AI) can benefit society, but it is also fraught with risks. Societal adoption of AI is recognized to depend on stakeholder trust in AI, yet the literature on trust in AI is fragmented, and little is known about the vulnerabilities faced by different stakeholders, making it is difficult to draw on this evidence-base to inform practice and policy. We undertake a literature review to take stock of what is known about the antecedents of trust in AI, and organize our findings around five trust challenges unique to or exacerbated by AI. Further, we develop a concept matrix identifying the key vulnerabilities to stakeholders raised by each of the challenges, and propose a multi-stakeholder approach to future research.",,Review
Trustworthy AI: From Principles to Practices,"Comprehensive trustworthy AI requires lifecycle considerations, from data acquisition to model development.",Search,2021,1,"Bo  Li, Peng  Qi, Bo  Liu, Shuai  Di, Jingen  Liu, Jiquan  Pei, Jinfeng  Yi, Bowen  Zhou",ArXiv,,,,https://semanticscholar.org/paper/c3689493757f90267908e776aeada9194fce55c7,,"Fast developing artificial intelligence (AI) technology has enabled various applied systems deployed in the real world, impacting people’s everyday lives. However, many current AI systems were found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection, etc., which not only degrades user experience but erodes the society’s trust in all AI systems. In this review, we strive to provide AI practitioners a comprehensive guide towards building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, alignment with human values, and accountability. We then survey leading approaches in these aspects in the industry. To unify the current fragmented approaches towards trustworthy AI, we propose a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items to practitioners and societal stakeholders (e.g., researchers and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges in the future development of trustworthy AI systems, where we identify the need for paradigm shift towards comprehensive trustworthy AI systems.",,Review
Trusting artificial intelligence in cybersecurity is a double-edged sword,National cybersecurity and defense strategies mention AI capabilities.,Search,2019,28,"Mariarosaria  Taddeo, Tom  McCutcheon, Luciano  Floridi",Nat. Mach. Intell.,,10.1038/s42256-019-0109-1,https://doi.org/10.1038/s42256-019-0109-1,https://semanticscholar.org/paper/a485c4c937a4b229455d611129ff4cd6a628f259,https://ora.ox.ac.uk/objects/uuid:8188813d-138e-409d-8686-d2d7d5fa0879/download_file?safe_filename=Trusting%2BAI%2Bin%2BCybersecurity.pdf&file_format=application%2Fpdf&type_of_work=Journal+article,"Applications of artificial intelligence (AI) for cybersecurity tasks are attracting greater attention from the private and the public sectors. Estimates indicate that the market for AI in cybersecurity will grow from US$1 billion in 2016 to a US$34.8 billion net worth by 2025. The latest national cybersecurity and defence strategies of several governments explicitly mention AI capabilities. At the same time, initiatives to define new standards and certification procedures to elicit users’ trust in AI are emerging on a global scale. However, trust in AI (both machine learning and neural networks) to deliver cybersecurity tasks is a double-edged sword: it can improve substantially cybersecurity practices, but can also facilitate new forms of attacks to the AI applications themselves, which may pose severe security threats. We argue that trust in AI for cybersecurity is unwarranted and that, to reduce security risks, some form of control to ensure the deployment of ‘reliable AI’ for cybersecurity is necessary. To this end, we offer three recommendations focusing on the design, development and deployment of AI for cybersecurity.Current national cybersecurity and defence strategies of several governments mention explicitly the use of AI. However, it will be important to develop standards and certification procedures, which involves continuous monitoring and assessment of threats. The focus should be on the reliability of AI-based systems, rather than on eliciting users’ trust in AI.",,
Exploring the Assessment List for Trustworthy AI in the Context of Advanced Driver-Assistance Systems,The European Commission appointed experts to a High-Level Expert Group on Artificial Intelligence (AI-HLEG).,Search,2021,,"Markus  Borg, Joshua  Bronson, Linus  Christensson, Fredrik  Olsson, Olof  Lennartsson, Elias  Sonnsjö, Hamid  Ebabi, Martin  Karsberg",2021 IEEE/ACM 2nd International Workshop on Ethics in Software Engineering Research and Practice (SEthics),,10.1109/SEthics52569.2021.00009,https://doi.org/10.1109/SEthics52569.2021.00009,https://semanticscholar.org/paper/3063caad889735cd1279054c09a8cd3befea9848,http://arxiv.org/pdf/2103.09051,"Artificial Intelligence (AI) is increasingly used in critical applications. Thus, the need for dependable AI systems is rapidly growing. In 2018, the European Commission appointed experts to a High-Level Expert Group on AI (AI-HLEG). AI- HLEG defined Trustworthy AI as 1) lawful, 2) ethical, and 3) robust and specified seven corresponding key requirements. To help development organizations, AI-HLEG recently published the Assessment List for Trustworthy AI (ALTAI). We present an illustrative case study from applying ALTAI to an ongoing development project of an Advanced Driver-Assistance System (ADAS) that relies on Machine Learning (ML). Our experience shows that ALTAI is largely applicable to ADAS development, but specific parts related to human agency and transparency can be disregarded. Moreover, bigger questions related to societal and environmental impact cannot be tackled by an ADAS supplier in isolation. We present how we plan to develop the ADAS to ensure ALTAI-compliance. Finally, we provide three recommendations for the next revision of ALTAI, i.e., life-cycle variants, domainspecific adaptations, and removed redundancy.",,
Trusted Artificial Intelligence: Towards Certification of Machine Learning Applications,,Search,2021,1,"Philip Matthias Winter, Sebastian  Eder, Johannes  Weissenbock, Christoph  Schwald, Thomas  Doms, Tom  Vogt, Sepp  Hochreiter, Bernhard  Nessler",ArXiv,,,,https://semanticscholar.org/paper/86819a944dbd083caa0f968e5fdb533ac9b5dcc7,,"Artificial Intelligence is one of the fastest growing technologies of the 21st century and accompanies us in our daily lives when interacting with technical applications. However, reliance on such technical systems is crucial for their widespread applicability and acceptance. The societal tools to express reliance are usually formalized by lawful regulations, i.e., standards, norms, accreditations, and certificates. Therefore, the TÜV AUSTRIA Group in cooperation with the Institute for Machine Learning at the Johannes Kepler University Linz, proposes a certification process and an audit catalog for Machine Learning applications. We are convinced that our approach can serve as the foundation for the certification of applications that use Machine Learning and Deep Learning, the techniques that drive the current revolution in Artificial Intelligence. While certain high-risk areas, such as fully autonomous robots in workspaces shared with humans, are still some time away from certification, we aim to cover low-risk applications with our certification procedure. Our holistic approach attempts to analyze Machine Learning applications from multiple perspectives to evaluate and verify the aspects of secure software development, functional requirements, data quality, data protection, and ethics. Inspired by existing work, we introduce four criticality levels to map the criticality of a Machine Learning application regarding the impact of its decisions on people, environment, and organizations. Currently, the audit catalog can be applied to low-risk applications within the scope of supervised learning as commonly encountered in industry. Guided by field experience, scientific developments, and market demands, the audit catalog will be extended and modified accordingly.",,
Trust in Artificial Intelligence: Australian Insights,Australia has low trust in AI but generally accepts or tolerates AI.,Search,2020,5,"Steve  Lockey, Nicole  Gillespie, Caitlin  Curtis",,,10.14264/b32f129,https://doi.org/10.14264/b32f129,https://semanticscholar.org/paper/6c2714f81bc323851052920d240bc21b98e2a9f8,https://espace.library.uq.edu.au/view/UQ:b32f129/LockeyGillespieCurtis_Trust_In_AI.pdf,"Artificial Intelligence (AI) is the cornerstone technology of the Fourth Industrial Revolution and is enabling rapid innovation with many potential benefits for Australian society (e.g. enhanced healthcare diagnostics, transportation optimisation) and business (e.g. enhanced efficiency and competitiveness). The COVID-19 pandemic has accelerated the uptake of advanced technology, and investment in AI continues to grow exponentially.AI also poses considerable risks and challenges to society which raises concerns about whether AI systems are worthy of trust. These concerns have been fuelled by high profile cases of AI use that were biased, discriminatory, manipulative, unlawful, or violated privacy or other human rights. Without public confidence that AI is being developed and used in an ethical and trustworthy manner, it will not be trusted and its full potential will not be realised. To echo the sentiment of Dr Alan Finkel AO, Australia’s Chief Scientist, acceptance of AI rests on “the essential foundation of trust”. Are we capable of extending our trust to AI? This national survey is the first to take a deep dive into answering this question and understanding community trust and expectations in relation to AI. To do this, we surveyed a nationally representative sample of over 2,500 Australian citizens in June to July 2020. Our findings provide important and timely research insights into the public’s trust and attitudes towards AI and lay out a pathway for strengthening trust and acceptance of AI systems.Key findings include:              - Trust is central to the acceptance of AI, and is influenced by four key drivers;              - Australians have low trust in AI systems but generally ‘accept’ or ‘tolerate’ AI;              - Australians expect AI to be regulated and carefully managed;              - Australians expect organisations to uphold the principles of trustworthy AI;              - Australians feel comfortable with some but not all uses of AI at work;              - Australians want to know more about AI but currently have low awareness and understanding of AI and its uses.We draw out the implications of the findings for government, business and NGOs and provide a roadmap to enhancing public trust in AI highlighting three key actions:              - Live up to Australian’s expectations of trustworthy AI              - Strengthen the regulatory framework for governing AI              - Strengthen Australia’s AI literacy",,
Trustworthy artificial intelligence (AI) in education,"AI applications are still nascent but can accelerate personalised learning, support students with special needs.",Search,2020,8,"Stéphan  Vincent-Lancrin, Reyer van der Vlies",,,10.1787/a6c90fa9-en,https://doi.org/10.1787/a6c90fa9-en,https://semanticscholar.org/paper/42d1eacdc929c74f2c0ebfb342e4e8a94b0f099a,https://www.oecd-ilibrary.org/deliver/a6c90fa9-en.pdf?itemId=%2Fcontent%2Fpaper%2Fa6c90fa9-en&mimeType=pdf,"This paper was written to support the G20 artificial intelligence (AI) dialogue. With the rise of artificial intelligence (AI), education faces two challenges: reaping the benefits of AI to improve education processes, both in the classroom and at the system level; and preparing students for new skillsets for increasingly automated economies and societies. AI applications are often still nascent, but there are many examples of promising uses that foreshadow how AI might transform education. With regard to the classroom, this paper highlights how AI can accelerate personalised learning, the support of students with special needs. At the system level, promising uses include predictive analysis to reduce dropout, and assessing new skillsets. A new demand for complex skills that are less easy to automate (e.g. higher cognitive skills like creativity and critical thinking) is also the consequence of AI and digitalisation. Reaching the full potential of AI requires that stakeholders trust not only the technology, but also its use by humans. This raises new policy challenges around “trustworthy AI”, encompassing the privacy and security of data, but also possible wrongful uses of data leading to biases against individuals or groups.",,
Trustworthiness of Artificial Intelligence,"A AI mistake can be disastrous, especially when human lives are involved.",Search,2020,4,"Sonali  Jain, Manan  Luthra, Shagun  Sharma, Mehtab  Fatima",2020 6th International Conference on Advanced Computing and Communication Systems (ICACCS),,10.1109/ICACCS48705.2020.9074237,https://doi.org/10.1109/ICACCS48705.2020.9074237,https://semanticscholar.org/paper/2efae53ba8d84c6f11d3f7151f23b9e22ca806e4,,"This paper discusses the need for a trustworthy AI, along with the ethics which are required to keep that trust intact. AI has a lot of benefits when it comes to societal, individual or cultural development. But any mistake in either the development or in the working phase of the AI system can be disastrous, especially when human lives are involved. The main goal of this paper is to understand what really makes an Artificial Intelligence system trustworthy.",,
Trustworthy Acceptance: A New Metric for Trustworthy Artificial Intelligence Used in Decision Making in Food-Energy-Water Sectors,A trustworthy acceptance metric and its measurement methodology can be used to evaluate the trustworthiness of AI systems.,Search,2021,1,"Suleyman  Uslu, Davinder  Kaur, Samuel J. Rivera, Arjan  Durresi, Mimoza  Durresi, Meghna  Babbar-Sebens",AINA,,10.1007/978-3-030-75100-5_19,https://doi.org/10.1007/978-3-030-75100-5_19,https://semanticscholar.org/paper/5d6a347ac08ff55ee9ac0234c36d9f41b0d7088c,,"We propose, for the first time, a trustworthy acceptance metric and its measurement methodology to evaluate the trustworthiness of AI-based systems used in decision making in Food Energy Water (FEW) management. The proposed metric is a significant step forward in the standardization process of AI systems. It is essential to standardize the AI systems’ trustworthiness, but until now, the standardization efforts remain at the level of high-level principles. The measurement methodology of the proposed includes human experts in the loop, and it is based on our trust management system. Our metric captures and quantifies the system’s transparent evaluation by field experts on as many control points as desirable by the users. We illustrate the trustworthy acceptance metric and its measurement methodology using AI in decision-making scenarios of Food-Energy-Water sectors. However, the proposed metric and its methodology can be easily adapted to other fields of AI applications. We show that our metric successfully captures the aggregated acceptance of any number of experts, can be used to do multiple measurements on various points of the system, and provides confidence values for the measured acceptance. Suleyman Uslu Indiana University-Purdue University Indianapolis, Indianapolis, IN, USA e-mail: suslu@iu.edu Davinder Kaur Indiana University-Purdue University Indianapolis, Indianapolis, IN, USA e-mail: davikaur@iu.edu Samuel J Rivera Oregon State University, Corvallis, OR, USA e-mail: sammy.rivera@oregonstate.edu Arjan Durresi Indiana University-Purdue University Indianapolis, Indianapolis, IN, USA e-mail: adurresi@iupui.edu Mimoza Durresi European University of Tirana e-mail: mimoza.durresi@uet.edu.al Meghna Babbar-Sebens Oregon State University, Corvallis, OR, USA e-mail: meghna@oregonstate.edu",,
Trust in Artificial Intelligence,"AI calibration involves dependability, perceived risks and benefits, reputation of the AI, reliability, consistency, and user experience with similar agents.",Search,2018,14,Arathi  Sethumadhavan,Ergonomics in Design: The Quarterly of Human Factors Applications,,10.1177/1064804618818592,https://doi.org/10.1177/1064804618818592,https://semanticscholar.org/paper/94a43052c729c8e7e3f28c50c76e46d09ec95f62,https://journals.sagepub.com/doi/pdf/10.1177/1064804618818592,"Trust is fundamental to creating a lasting relationship with another human being. In our daily lives, we encounter several situations where we place trust on other humans such as bus drivers, colleagues, and even strangers. Trust in human-human teams is initially founded on the predictability of the trustee, and as the relationship between the trustor and the trustee progresses, dependability or integrity replaces predictability as the basis of trust (Hoff & Bashir, 2015). As artificial intelligence (AI) gets smarter and smarter, it is becoming an integral part of human lives. For example, AI is being used for scheduling appointments, powering smart homes, recognizing people’s faces in photos, making health diagnoses, providing investment advice, and several other things. As with a humanhuman team, trust is a necessary ingredient for human-AI partnerships. However, trust in human-human teams progresses in the reverse order from human-human teams (e.g., Hoff & Bashir, 2015). This is because humans initially assume that AI is near perfect. Therefore, in the initial stages, faith forms the essential constituent of trust, and as the number of exchanges between the human and the AI increases, faith is replaced by dependability and predictability. Trust in AI is considered a twodimensional construct comprising trust and distrust, where trust is associated with feelings of calmness and security and distrust involves fear and worry (Lyons, Stokes, Eschleman, Alarcon, & Barelka, 2011). Unarguably, trust is a complex social process with a variety of factors determining the extent to which humans trust AI agents (Hoff & Bashir, 2015; Lee & See, 2004). Specifically, in order to calibrate the right level of trust in AI, consider dispositional (i.e., user characteristics such as age, culture, gender, and personality), internal (e.g., user characteristics such as workload, mood, self-confidence, and working memory capacity), environmental (e.g., task difficulty, perceived risks and benefits, organizational setting), and learned factors (e.g., reputation of the AI, reliability, consistency, type and timing of errors made by the AI, and users’ experience with similar agents). Design factors (e.g., appearance, ease of use, communication style, and transparency of the AI) also affect perceptions of trust. For example, anthrophomorphizing is an effective way of establishing a long-term social bond between humans and AI agents, which is driven by the neurotransmitter and hormone, oxytocin (e.g., de Visser et al., 2017). Further, anthropomorphic agents also resist breakdown in trust compared to their counterpart non-anthropomorphic agents, presumably because anthropomorphic agents remind users of humans who are forgiven more easily for being imperfect in comparison to machine-like agents. Interestingly, there is also evidence that humans tend to disclose more information to AI therapists than human therapists. Transparency of the AI also helps to calibrate the right level of trust by enabling users to develop accurate mental models of the AI underpinnings (e.g., Balfe, Sharples, & Wilson, 2018). 818592 ERGXXX10.1177/1064804618818592ergonomics in designergonomics in design research-article2018",,
Requirements for Trustworthy Artificial Intelligence - A Review,"The field of algorithmic decision-making, particularly Artificial Intelligence (AI), has been drastically changing.",Search,2020,8,"Davinder  Kaur, Suleyman  Uslu, Arjan  Durresi",NBiS,,10.1007/978-3-030-57811-4_11,https://doi.org/10.1007/978-3-030-57811-4_11,https://semanticscholar.org/paper/ab7f6628cbbafae1ce994929af2482efbd092d61,https://scholarworks.iupui.edu/bitstream/1805/28055/1/Kaur2020Requirements-AAM.pdf,"The field of algorithmic decision-making, particularly Artificial Intelligence (AI), has been drastically changing. With the availability of a massive amount of data and an increase in the processing power, AI systems have been used in a vast number of high-stake applications. So, it becomes vital to make these systems reliable and trustworthy. Different approaches have been proposed to make theses systems trustworthy. In this paper, we have reviewed these approaches and summarized them based on the principles proposed by the European Union for trustworthy AI. This review provides an overview of different principles that are important to make AI trustworthy.",,Review