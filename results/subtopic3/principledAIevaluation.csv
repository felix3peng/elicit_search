Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI,There is a growing consensus around eight key thematic trends in AI principles documents.,Search,2020,137,"Jessica  Fjeld, Nele  Achten, Hannah  Hilligoss, Adam  Nagy, Madhulika  Srikumar",SSRN Electronic Journal,,10.2139/ssrn.3518482,https://doi.org/10.2139/ssrn.3518482,https://semanticscholar.org/paper/58bb24b72fea6d0ce172bdaf9c2f16c2bd7649e9,https://dash.harvard.edu/bitstream/1/42160420/1/HLS%20White%20Paper%20Final_v3.pdf,"The rapid spread of artificial intelligence (AI) systems has precipitated a rise in ethical and human rights-based frameworks intended to guide the development and use of these technologies. Despite the proliferation of these ""AI principles,"" there has been little scholarly focus on understanding these efforts either individually or as contextualized within an expanding universe of principles with discernible trends.

To that end, this white paper and its associated data visualization compare the contents of thirty-six prominent AI principles documents side-by-side. This effort uncovered a growing consensus around eight key thematic trends: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values. Underlying this “normative core,” our analysis examined the forty-seven individual principles that make up the themes, detailing notable similarities and differences in interpretation found across the documents. In sharing these observations, it is our hope that policymakers, advocates, scholars, and others working to maximize the benefits and minimize the harms of AI will be better positioned to build on existing efforts and to push the fractured, global conversation on the future of AI toward consensus.",,
Unifying Principles and Metrics for Safe and Assistive AI,AI safety focuses on designing AI systems that allow humans to safely instruct and control AI systems.,Search,2021,4,Siddharth  Srivastava,AAAI,,,,https://semanticscholar.org/paper/967cba5849f93869655af8d3d8fda9b0a3304a1d,,"The prevalence and success of AI applications have been tempered by concerns about the controllability of AI systems about AI’s impact on the future of work. These concerns reflect two aspects of a central question: how would humans work with AI systems? While research on AI safety focuses on designing AI systems that allow humans to safely instruct and control AI systems, research on AI and the future of work focuses on the impact of AI on humans who may be unable to do so. This Blue Sky Ideas paper proposes a unifying set of declarative principles that enable a more uniform evaluation of arbitrary AI systems along multiple dimensions of the extent to which they are suitable for use by specific classes of human operators. It leverages recent AI research and the unique strengths of the field to develop human-centric principles for AI systems that address the concerns noted above.",,
Multisource AI Scorecard Table for System Evaluation,A Multisource AI Scorecard Table can help develop more understandable artificial intelligence systems and engender trust in AI outputs.,Search,2021,6,"Erik  Blasch, James  Sung, Tao  Nguyen",ArXiv,,,,https://semanticscholar.org/paper/a45c902c144113708cb050edb67fca0dee303e3e,,"The paper describes a Multisource AI Scorecard Table (MAST) that provides the developer and user of an artificial intelligence (AI)/machine learning (ML) system with a standard checklist focused on the principles of good analysis adopted by the intelligence community (IC) to help promote the development of more understandable systems and engender trust in AI outputs. Such a scorecard enables a transparent, consistent, and meaningful understanding of AI tools applied for commercial and government use. A standard is built on compliance and agreement through policy, which requires buy-in from the stakeholders. While consistency for testing might only exist across a standard data set, the community requires discussion on verification and validation approaches which can lead to interpretability, explainability, and proper use. The paper explores how the analytic tradecraft standards outlined in Intelligence Community Directive (ICD) 203 can provide a framework for assessing the performance of an AI system supporting various operational needs. These include sourcing, uncertainty, consistency, accuracy, and visualization. Three use cases are presented as notional examples that support security for comparative analysis.",,
Enhanced well-being assessment as basis for the practical implementation of ethical and rights-based normative principles for AI,A well-being impact assessment framework could enable a human-centered AI impact assessment.,Search,2020,3,"Marek  Havrda, Bogdana  Rakova","2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",,10.1109/SMC42975.2020.9283137,https://doi.org/10.1109/SMC42975.2020.9283137,https://semanticscholar.org/paper/867ba0f0e4843b889ec1d96ae52bd7f3c49f7603,http://arxiv.org/pdf/2007.14826,"Artificial Intelligence (AI) has an increasing impact on all areas of people’s livelihoods. A detailed look at existing interdisciplinary and transdisciplinary metrics frameworks could bring new insights and enable practitioners to navigate the challenge of understanding and assessing the impact of Autonomous and Intelligent Systems (A/IS). There has been emerging consensus on fundamental ethical and rights-based AI principles proposed by scholars, governments, civil rights organizations, and technology companies. In order to move from principles to real-world implementation, we adopt a lens motivated by regulatory impact assessments and the well-being movement in public policy. Similar to public policy interventions, outcomes of AI systems implementation may have far-reaching complex impacts. In public policy, indicators are only part of a broader toolbox, as metrics inherently lead to gaming and dissolution of incentives and objectives. Similarly, in the case of A/IS, there’s a need for a larger toolbox that allows for the iterative assessment of identified impacts, inclusion of new impacts in the analysis, and identification of emerging trade-offs. In this paper, we propose the practical application of an enhanced well-being impact assessment framework for A/IS that could be employed to address ethical and rights-based normative principles in AI. This process could enable a human-centered algorithmically-supported approach to the understanding of the impacts of AI systems. Finally, we propose a new testing infrastructure which would allow for governments, civil rights organizations, and others, to engage in cooperating with A/IS developers towards implementation of enhanced well-being impact assessments.",,
"AI Evaluation: past, present and future",AI systems are becoming more complex and unpredictable black-box behavioral evaluation becoming increasingly common.,Search,2014,24,José  Hernández-Orallo,ArXiv,,,,https://semanticscholar.org/paper/8f2d78fb9293eac038a6a431d23ca5a18df7bfb4,,"Artificial intelligence develops techniques and systems whose performance must be evaluated on a regular basis in order to certify and foster progress in the discipline. We will describe and critically assess the different ways AI systems are evaluated. We first focus on the traditional task-oriented evaluation approach. We see that black-box (behavioural evaluation) is becoming more and more common, as AI systems are becoming more complex and unpredictable. We identify three kinds of evaluation: Human discrimination, problem benchmarks and peer confrontation. We describe the limitations of the many evaluation settings and competitions in these three categories and propose several ideas for a more systematic and robust evaluation. We then focus on a less customary (and challenging) ability-oriented evaluation approach, where a system is characterised by its (cognitive) abilities, rather than by the tasks it is designed to solve. We discuss several possibilities: the adaptation of cognitive tests used for humans and animals, the development of tests derived from algorithmic information theory or more general approaches under the perspective of universal psychometrics.",,
A Unified Framework of Five Principles for AI in Society,"A new principle, explicability, is needed in addition to the core principles of ethical AI.",Search,2019,152,"Luciano  Floridi, Josh  Cowls",Issue 1,,10.1162/99608F92.8CD550D1,https://doi.org/10.1162/99608F92.8CD550D1,https://semanticscholar.org/paper/8499e44d42b12518495069a54ae4400baccb7546,https://assets.pubpub.org/ukqte8ry/c8d3cba5-8f10-4a00-894c-3a3b886ad844.pdf,"Artificial Intelligence (AI) is already having a major impact on society. As a result, many organizations have launched a wide range of initiatives to establish ethical principles for the adoption of socially beneficial AI. Unfortunately, the sheer volume of proposed principles threatens to overwhelm and confuse. How might this problem of ‘principle proliferation’ be solved? In this paper, we report the results of a fine-grained analysis of several of the highest-profile sets of ethical principles for AI. We assess whether these principles converge upon a set of agreed-upon principles, or diverge, with significant disagreement over what constitutes ‘ethical AI.’ Our analysis finds a high degree of overlap among the sets of principles we analyze. We then identify an overarching framework consisting of five core principles for ethical AI. Four of them are core principles commonly used in bioethics: beneficence, non-maleficence, autonomy, and justice. On the basis of our comparative analysis, we argue that a new principle is needed in addition: explicability, understood as incorporating both the epistemological sense of intelligibility (as an answer to the question ‘how does it work?’) and in the ethical sense of accountability (as an answer to the question: ‘who is responsible for the way it works?’). In the ensuing discussion, we note the limitations and assess the implications of this ethical framework for future efforts to create laws, rules, technical standards, and best practices for ethical AI in a wide range of contexts.KeywordsAccountability; Autonomy; Artificial Intelligence; Beneficence; Ethics; Explicability; Fairness; Intelligibility; Justice; Non-maleficence.",,
Evaluation in artificial intelligence: from task-oriented to ability-oriented measurement,Traditional AI evaluation schemes and competitions have many limitations.,Search,2016,80,José  Hernández-Orallo,Artificial Intelligence Review,,10.1007/s10462-016-9505-7,https://doi.org/10.1007/s10462-016-9505-7,https://semanticscholar.org/paper/1a48604fbc09f88a89e551ed70dda07bdb6f01e5,https://riunet.upv.es/bitstream/10251/83598/3/AIRE-Evaluation-of-AI.pdf,"The evaluation of artificial intelligence systems and components is crucial for the progress of the discipline. In this paper we describe and critically assess the different ways AI systems are evaluated, and the role of components and techniques in these systems. We first focus on the traditional task-oriented evaluation approach. We identify three kinds of evaluation: human discrimination, problem benchmarks and peer confrontation. We describe some of the limitations of the many evaluation schemes and competitions in these three categories, and follow the progression of some of these tests. We then focus on a less customary (and challenging) ability-oriented evaluation approach, where a system is characterised by its (cognitive) abilities, rather than by the tasks it is designed to solve. We discuss several possibilities: the adaptation of cognitive tests used for humans and animals, the development of tests derived from algorithmic information theory or more integrated approaches under the perspective of universal psychometrics. We analyse some evaluation tests from AI that are better positioned for an ability-oriented evaluation and discuss how their problems and limitations can possibly be addressed with some of the tools and ideas that appear within the paper. Finally, we enumerate a series of lessons learnt and generic guidelines to be used when an AI evaluation scheme is under consideration.",,
Toward a Methodology for AI Architecture Evaluation: Comparing Soar and CLIPS,Comparing and evaluating Artificial Intelligence architectures is motivated by fundamental properties required by general intelligent systems.,Search,1999,12,"Scott A. Wallace, John E. Laird",ATAL,,10.1007/10719619_9,https://doi.org/10.1007/10719619_9,https://semanticscholar.org/paper/3ef693e7d8c247d72af86752819e91399bc6a010,,We propose a methodology that can be used to compare and evaluate Artificial Intelligence architectures and is motivated by fundamental properties required by general intelligent systems. We examine an initial application of this method used to compare Soar and CLIPS in two simple domains. Results gathered from our tests indicate both qualitative and quantitative differences in these architectures and are used to explore how aspects of the architectures may affect the agent design process and the performance of agents implemented within each architecture.,,
A Survey on Ethical Principles of AI and Implementations,Ethical principles are required in every stage of the AI lifecycle to ensure that AI systems are ethical.,Search,2020,2,"Jianlong  Zhou, Fang  Chen, Adam  Berry, Mike  Reed, Shujia  Zhang, Siobhan  Savage",2020 IEEE Symposium Series on Computational Intelligence (SSCI),,10.1109/SSCI47803.2020.9308437,https://doi.org/10.1109/SSCI47803.2020.9308437,https://semanticscholar.org/paper/66a07e8609b74a0967828bb179e84444c2d312a8,https://opus.lib.uts.edu.au/bitstream/10453/146673/2/IEEE_ETHAI2020_ethicalAI_Survey.pdf,"AI has powerful capabilities in prediction, automation, planning, targeting, and personalisation. Generally, it is assumed that AI can enable machines to exhibit human-like intelligence, and is claimed to benefit to different areas of our lives. Since AI is fueled by data and is a distinct form of autonomous and self-learning agency, we are seeing increasing ethical concerns related to AI uses. In order to mitigate various ethical concerns, national and international organisations including governmental organisations, private sectors as well as research institutes have made extensive efforts by drafting ethical principles of AI, and having active discussions on ethics of AI within and beyond the AI community. This paper investigates these efforts with a focus on the identification of fundamental ethical principles of AI and their implementations. The review found that there is a convergence around limited principles and the most prevalent principles are transparency, justice and fairness, responsibility, non-maleficence, and privacy. The investigation suggests that ethical principles need to be combined with every stages of the AI lifecycle in the implementation to ensure that the AI system is designed, implemented and deployed in an ethical manner. Similar to ethical framework used in biomedical and clinical research, this paper suggests checklist-style questionnaires as benchmarks for the implementation of ethical principles of AI.",,Review
Requirements for AI Assessment,"The development of AI systems represents a significant investment and to realize the promise of that investment, performance assessment is necessary.",Search,2021,,"Gary Klein Mohammad Jalaeian Macrocognition, Robert R. Hoffman",,,,,https://semanticscholar.org/paper/5cfa64c13d88cf571a2a395862664b4cd7948207,,"The development of AI systems represents a significant investment. But to realize the promise of that investment, performance assessment is necessary. Empirical evaluation of Human-AI work systems must adduce convincing empirical evidence that the work method and its AI technology are learnable, usable, and useful. The theme to this Report is the notion that AI assessment must be effective but must also be efficient. Bench testing of a prototype of an AI system cannot require extensive series of experiments with complex designs. Thus, the empirical requirements that are presented in this Report involve escaping some of the constraints that are imposed in traditional laboratory research. Also, there is a recognition of new constraints that are unique to AI evaluation contexts. Empirical requirements are presented covering study design, research methods, statistical analyses, and online experimentation. The 15 requirements presented in this Report should be applicable to all research intended to evaluate the effectivity of AI systems.",,
Evaluation Platform for Artificial Intelligence Algorithms,"A platform helps to quickly train, test, and evaluate several artificial intelligence algorithms.",Search,2018,2,"Zoltan  Czako, Gheorghe  Sebestyen, Anca  Hangan",IJCCI,,10.5220/0006888900390046,https://doi.org/10.5220/0006888900390046,https://semanticscholar.org/paper/84e157edf26507db899356e67d6c24b4f901dc7f,,"Currently, artificial intelligence (AI) algorithms have been receiving a lot of attention from researchers as well as from commercial product developers. Hundreds of different AI algorithms aiming different kind of real-life problems have qualitatively different results, based on the nature of data, the nature of the problems and based on the context in which they are used. Choosing the most appropriate algorithm to solve a particular problem is not a trivial task. The goal of our research is to create a platform, which can be used in the early stage of problem solving. With this platform, the user could be able to quickly train, test and evaluate several artificial intelligence algorithms and also they will be able to find out which is the algorithm that performs best for a specific problem. Moreover, this platform will help developers to tune the parameters of the chosen algorithm in order to get better results on their problem. We will demonstrate our approach by running different types of algorithms initially in the case of breast cancer sample dataset and after that we will use the platform for solving an anomaly detection problem.",,
Toward AI research methodology: three case studies in evaluation,"The roles of evaluation in empirical artificial intelligence (AI) research are described, in an idealized cyclic model.",Search,1989,55,"Paul R. Cohen, Adele E. Howe",IEEE Trans. Syst. Man Cybern.,,10.1109/21.31069,https://doi.org/10.1109/21.31069,https://semanticscholar.org/paper/06e8b97451e9be65bd96eab7cb1ffe75dc3b082a,,"The roles of evaluation in empirical artificial intelligence (AI) research are described, in an idealized cyclic model and in the context of three case studies. The case studies illustrate the pitfalls in evaluation and the contributions of evaluation at all stages of the research cycle. Evaluation methods are contrasted with those of the behavioral sciences, and it is concluded that AI must define and refine its own methods. To this end, several experiment schemas and many specific evaluation criteria are described. Recommendations are offered in the hope of encouraging the development and practice of evaluation methods in AI. The first case study illustrates problems with evaluating knowledge-based systems, specifically a portfolio management expert system called FOLIO. The second study focuses on the relationship between evaluation and the evolution of the GRANT system, specifically, how the evaluations changed as GRANT's knowledge base was sealed up. Third, the cyclic nature of a given research model is examined. >",,
Analysis of first prototype universal intelligence tests: evaluating and comparing AI algorithms and humans,"Today, available methods that assess AI systems are focused on using empirical techniques to measure the performance of algorithms in some specific tasks.",Search,2011,2,"Javier  Insa-Cabrera, José  Hernández-Orallo",ArXiv,,,,https://semanticscholar.org/paper/2a26ccff62cc2d416e2ecba7b79e32c0ed5bcb3f,,"Today, available methods that assess AI systems are focused on using empirical techniques to measure the performance of algorithms in some specific tasks (e.g., playing chess, solving mazes or land a helicopter). However, these methods are not appropriate if we want to evaluate the general intelligence of AI and, even less, if we compare it with human intelligence. The ANYNT project has designed a new method of evaluation that tries to assess AI systems using well known computational notions and problems which are as general as possible. This new method serves to assess general intelligence (which allows us to learn how to solve any new kind of problem we face) and not only to evaluate performance on a set of specific tasks. This method not only focuses on measuring the intelligence of algorithms, but also to assess any intelligent system (human beings, animals, AI, aliens?,...), and letting us to place their results on the same scale and, therefore, to be able to compare them. This new approach will allow us (in the future) to evaluate and compare any kind of intelligent system known or even to build/find, be it artificial or biological. This master thesis aims at ensuring that this new method provides consistent results when evaluating AI algorithms, this is done through the design and implementation of prototypes of universal intelligence tests and their application to different intelligent systems (AI algorithms and humans beings). From the study we analyze whether the results obtained by two different intelligent systems are properly located on the same scale and we propose changes and refinements to these prototypes in order to, in the future, being able to achieve a truly universal intelligence test.",,
"Review of ""Principles of Artificial Intelligence by Nils J. Nilsson"", Tioga Publishing Co.","Nils Nilsson's book, ""Principles of Artificial Intelligence"", aims to fill a gap between theory and practice.",Search,1980,3,Elaine  Kant,SGAR,,10.1145/1056447.1056452,https://doi.org/10.1145/1056447.1056452,https://semanticscholar.org/paper/4dc1b87fd521864e675dc0aae403edd0a56e11c9,,"Nils Nilsson's new book, (Principles of Artificial Intelligence) (Tioga Publishing Co., 1980) discusses some basic ideas underlying different applications of AI. The book, designed as a text for a senior or first-year graduate course, aims to fill a gap between theory and practice. It succeeds in building a good solid arch outward from the shore of theory, but only a few support beams anchor the bridge to the shore of practice.",,Review
AI Evaluation: On Broken Yardsticks and Measurement Scales,"AI measurement suffers from a moving-target phenomenon, trying to catch up with accelerating AI research.",Search,2020,5,José  Hernández-Orallo,,,,,https://semanticscholar.org/paper/71381ee5ef204d539fc3daa6f7195dee47d9c6ff,,"AI measurement suffers from a moving-target phenomenon, trying to catch up with accelerating AI research. This is partly caused by the “AI effect” (whenever something is automated, intelligence is no longer considered necessary for it), the “superhuman abyss” (once an AI system reaches superhuman performance for a task, the human yardstick does not extend further), the “resource neglect” (breakthroughs are achieved and celebrated independently of the resources involved), the “specialisation drift” (specific solutions for particular benchmarks are favoured by performance-focused competitions) and the “cognitive-judge problem” (failure to distinguish the cognitive effort that is necessary for producing and verifying instances, frequently relying on human labellers or crowdsourcing). The situation is usually associated with a ‘challenge-solve-and-replace’ dynamics of AI benchmarks: measurement yardsticks are broken once their upper edge is reached, and replaced by a new benchmark. A deeper understanding of these issues reveals lack of measurement invariance, ill-defined scales, measures without unit, and other critical problems we should identify and address, looking at measurement theory and other disciplines. In this paper we examine the moving-target phenomenon and the causes mentioned above, and several research directions towards better AI measurement, focusing on dimensions, scales and units.",,
General valuation framework for artificial intelligence models,Artificial Intelligence models are useful every time heuristics can be successfully applied.,Search,2016,,"Florin B. Manolache, Octavian  Rusu",2016 15th RoEduNet Conference: Networking in Education and Research,,10.1109/ROEDUNET.2016.7753204,https://doi.org/10.1109/ROEDUNET.2016.7753204,https://semanticscholar.org/paper/18eb32d2cb289355e960908501b88fc397c311c4,,"Artificial Intelligence (AI) is a fast developing area that is applied to many daily problems, replacing the tried and true heuristics used by society for a long time. This paper provides a framework to estimate the value added by different steps and components used to create and apply AI models. Such estimations are useful to decide if deploying AI models makes economical sense, reported to the specific problem, data availability, existing domain expertise, and deployment costs. This framework leads to analysis of interesting subtle concepts like the difference between knowing the future and being able to predict the future, even if the prediction would be perfect. Such concepts are useful for benchmarking the efficiency of the model. As an example of applying the framework, real data from a bank was used to determine ways to decrease the operating costs by convincing new customers to opt for electronic statements. The results show that AI models are useful every time when heuristics can be successfully applied, but the improvement brought by AI may not always be spectacular, even in the case of complete information and perfect model. So estimating the real value of adding AI is important because it allows comparison between savings and deployment costs, as well as comparison between models.",,