Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI,There is a growing consensus around eight key thematic trends in AI principles documents.,Search,2020,137,"Jessica  Fjeld, Nele  Achten, Hannah  Hilligoss, Adam  Nagy, Madhulika  Srikumar",SSRN Electronic Journal,,10.2139/ssrn.3518482,https://doi.org/10.2139/ssrn.3518482,https://semanticscholar.org/paper/58bb24b72fea6d0ce172bdaf9c2f16c2bd7649e9,https://dash.harvard.edu/bitstream/1/42160420/1/HLS%20White%20Paper%20Final_v3.pdf,"The rapid spread of artificial intelligence (AI) systems has precipitated a rise in ethical and human rights-based frameworks intended to guide the development and use of these technologies. Despite the proliferation of these ""AI principles,"" there has been little scholarly focus on understanding these efforts either individually or as contextualized within an expanding universe of principles with discernible trends.

To that end, this white paper and its associated data visualization compare the contents of thirty-six prominent AI principles documents side-by-side. This effort uncovered a growing consensus around eight key thematic trends: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values. Underlying this “normative core,” our analysis examined the forty-seven individual principles that make up the themes, detailing notable similarities and differences in interpretation found across the documents. In sharing these observations, it is our hope that policymakers, advocates, scholars, and others working to maximize the benefits and minimize the harms of AI will be better positioned to build on existing efforts and to push the fractured, global conversation on the future of AI toward consensus.",,
Unifying Principles and Metrics for Safe and Assistive AI,AI safety focuses on designing AI systems that allow humans to safely instruct and control AI systems.,Search,2021,4,Siddharth  Srivastava,AAAI,,,,https://semanticscholar.org/paper/967cba5849f93869655af8d3d8fda9b0a3304a1d,,"The prevalence and success of AI applications have been tempered by concerns about the controllability of AI systems about AI’s impact on the future of work. These concerns reflect two aspects of a central question: how would humans work with AI systems? While research on AI safety focuses on designing AI systems that allow humans to safely instruct and control AI systems, research on AI and the future of work focuses on the impact of AI on humans who may be unable to do so. This Blue Sky Ideas paper proposes a unifying set of declarative principles that enable a more uniform evaluation of arbitrary AI systems along multiple dimensions of the extent to which they are suitable for use by specific classes of human operators. It leverages recent AI research and the unique strengths of the field to develop human-centric principles for AI systems that address the concerns noted above.",,
Principles and business processes for responsible AI,Organizations must undertake risk assessment from the perspectives of other stakeholders to responsibly manage AI.,Search,2019,25,Roger  Clarke,Comput. Law Secur. Rev.,,10.1016/J.CLSR.2019.04.007,https://doi.org/10.1016/J.CLSR.2019.04.007,https://semanticscholar.org/paper/9042677e67305726a4511ef3e9a7a00239af2a79,,"Abstract The first article in this series examined why the world wants controls over Artificial Intelligence (AI). This second article discusses how an organisation can manage AI responsibly, in order to protect its own interests, but also those of its stakeholders and society as a whole. A limited amount of guidance is provided by ethical analysis. A much more effective approach is to apply adapted forms of the established techniques of risk assessment and risk management. Critically, risk assessment needs to be undertaken not only with the organisation's own interests in focus, but also from the perspectives of other stakeholders. To underpin this new form of business process, a set of Principles for Responsible AI is presented, consolidating proposals put forward by a diverse collection of 30 organisations.",,
A Unified Framework of Five Principles for AI in Society,AI already has a major impact on society and is developing at an increasingly rapid pace.,Search,2019,152,"Luciano  Floridi, Josh  Cowls",Issue 1,,10.1162/99608F92.8CD550D1,https://doi.org/10.1162/99608F92.8CD550D1,https://semanticscholar.org/paper/8499e44d42b12518495069a54ae4400baccb7546,https://assets.pubpub.org/ukqte8ry/c8d3cba5-8f10-4a00-894c-3a3b886ad844.pdf,"Artificial Intelligence (AI) is already having a major impact on society. As a result, many organizations have launched a wide range of initiatives to establish ethical principles for the adoption of socially beneficial AI. Unfortunately, the sheer volume of proposed principles threatens to overwhelm and confuse. How might this problem of ‘principle proliferation’ be solved? In this paper, we report the results of a fine-grained analysis of several of the highest-profile sets of ethical principles for AI. We assess whether these principles converge upon a set of agreed-upon principles, or diverge, with significant disagreement over what constitutes ‘ethical AI.’ Our analysis finds a high degree of overlap among the sets of principles we analyze. We then identify an overarching framework consisting of five core principles for ethical AI. Four of them are core principles commonly used in bioethics: beneficence, non-maleficence, autonomy, and justice. On the basis of our comparative analysis, we argue that a new principle is needed in addition: explicability, understood as incorporating both the epistemological sense of intelligibility (as an answer to the question ‘how does it work?’) and in the ethical sense of accountability (as an answer to the question: ‘who is responsible for the way it works?’). In the ensuing discussion, we note the limitations and assess the implications of this ethical framework for future efforts to create laws, rules, technical standards, and best practices for ethical AI in a wide range of contexts.KeywordsAccountability; Autonomy; Artificial Intelligence; Beneficence; Ethics; Explicability; Fairness; Intelligibility; Justice; Non-maleficence.",,
Principles to Practices for Responsible AI: Closing the Gap,Companies have considered adoption of various high-level artificial intelligence (AI) principles for responsible AI.,Search,2020,10,"Daniel  Schiff, Bogdana  Rakova, Aladdin  Ayesh, Anat  Fanti, Michael  Lennon",ArXiv,,,,https://semanticscholar.org/paper/973b72984abceec59d1a97622b1a34946a9d89ef,,"Companies have considered adoption of various high-level artificial intelligence (AI) principles for responsible AI, but there is less clarity on how to implement these principles as organizational practices. This paper reviews the principles-to-practices gap. We outline five explanations for this gap ranging from a disciplinary divide to an overabundance of tools. In turn, we argue that an impact assessment framework which is broad, operationalizable, flexible, iterative, guided, and participatory is a promising approach to close the principles-to-practices gap. Finally, to help practitioners with applying these recommendations, we review a case study of AI's use in forest ecosystem restoration, demonstrating how an impact assessment framework can translate into effective and responsible AI practices.",,Review
Principles alone cannot guarantee ethical AI,AI ethics initiatives may not yet celebrate consensus around high-level principles that hide deep political and normative disagreement.,Search,2019,162,Brent  Mittelstadt,Nat. Mach. Intell.,,10.1038/s42256-019-0114-4,https://doi.org/10.1038/s42256-019-0114-4,https://semanticscholar.org/paper/2014d6036276b470d010060c86cedd4c92e0958c,http://arxiv.org/pdf/1906.06668,"Artificial intelligence (AI) ethics is now a global topic of discussion in academic and policy circles. At least 84 public–private initiatives have produced statements describing high-level principles, values and other tenets to guide the ethical development, deployment and governance of AI. According to recent meta-analyses, AI ethics has seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics. Despite the initial credibility granted to a principled approach to AI ethics by the connection to principles in medical ethics, there are reasons to be concerned about its future impact on AI development and governance. Significant differences exist between medicine and AI development that suggest a principled approach for the latter may not enjoy success comparable to the former. Compared to medicine, AI development lacks (1) common aims and fiduciary duties, (2) professional history and norms, (3) proven methods to translate principles into practice, and (4) robust legal and professional accountability mechanisms. These differences suggest we should not yet celebrate consensus around high-level principles that hide deep political and normative disagreement.AI ethics initiatives have seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics. Despite this, Brent Mittelstadt highlights important differences between medical practice and AI development that suggest a principled approach may not work in the case of AI.",,
Principles Alone Cannot Guarantee Ethical AI,AI ethics initiatives have seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics.,Search,2019,63,Brent  Mittelstadt,,,10.2139/ssrn.3391293,https://doi.org/10.2139/ssrn.3391293,https://semanticscholar.org/paper/ddc2d6d51b2f00a70320e293473035b993192949,,"Artificial intelligence (AI) ethics is now a global topic of discussion in academic and policy circles. At least 84 public–private initiatives have produced statements describing high-level principles, values and other tenets to guide the ethical development, deployment and governance of AI. According to recent meta-analyses, AI ethics has seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics. Despite the initial credibility granted to a principled approach to AI ethics by the connection to principles in medical ethics, there are reasons to be concerned about its future impact on AI development and governance. Significant differences exist between medicine and AI development that suggest a principled approach for the latter may not enjoy success comparable to the former. Compared to medicine, AI development lacks (1) common aims and fiduciary duties, (2) professional history and norms, (3) proven methods to translate principles into practice, and (4) robust legal and professional accountability mechanisms. These differences suggest we should not yet celebrate consensus around high-level principles that hide deep political and normative disagreement. AI ethics initiatives have seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics. Despite this, Brent Mittelstadt highlights important differences between medical practice and AI development that suggest a principled approach may not work in the case of AI.",,
Lessons learned from AI ethics principles for future actions,AI ethics is urgently calling for tangible action to move from high-level abstractions and conceptual arguments towards applying ethics in practice and creating accountability mechanisms.,Search,2021,17,Merve  Hickok,AI Ethics,,10.1007/s43681-020-00008-1,https://doi.org/10.1007/s43681-020-00008-1,https://semanticscholar.org/paper/eb22e505d84b31f52a5bd8bd973f89711891e836,https://link.springer.com/content/pdf/10.1007/s43681-020-00008-1.pdf,"As the use of artificial intelligence (AI) systems became significantly more prevalent in recent years, the concerns on how these systems collect, use and process big data also increased. To address these concerns and advocate for ethical and responsible development and implementation of AI, non-governmental organizations (NGOs), research centers, private companies, and governmental agencies published more than 100 AI ethics principles and guidelines. This first wave was followed by a series of suggested frameworks, tools, and checklists that attempt a technical fix to issues brought up in the high-level principles. Principles are important to create a common understanding for priorities and are the groundwork for future governance and opportunities for innovation. However, a review of these documents based on their country of origin and funding entities shows that private companies from US-West axis dominate the conversation. Several cases surfaced in the meantime which demonstrate biased algorithms and their impact on individuals and society. The field of AI ethics is urgently calling for tangible action to move from high-level abstractions and conceptual arguments towards applying ethics in practice and creating accountability mechanisms. However, lessons must be learned from the shortcomings of AI ethics principles to ensure the future investments, collaborations, standards, codes or legislation reflect the diversity of voices and incorporate the experiences of those who are already impacted by the biased algorithms.",,Review
Basic principles for the development of an AI-based tool for assistive technology decision making.,Artificial intelligence can help facilitate optimal assistive technology decisions based on user preferences.,Search,2020,1,"Moran  Ran, David  Banes, Marcia J Scherer",Disability and rehabilitation. Assistive technology,,10.1080/17483107.2020.1817163,https://doi.org/10.1080/17483107.2020.1817163,https://semanticscholar.org/paper/b63fea7caa5fdc5b0d7cdb8b660dd5678d97a2ab,,"INTRODUCTION

The impact of assistive technology use on the lives of people with disabilities has long been demonstrated in the literature. Despite the need for assistive technologies, and a wealth of innovative, afford-able, and accessible products, a low rate of assistive technology uptake is globally maintained. One of the reasons for this gap is related to data and knowledge formation and management. Low access to information and a lack of assessment services is evident. Fragmentation of data, inconsistency in assessment methodology and heterogeneity in the competence of assistive technology professionals, has led to a growing interest in the opportunities that data sciences, including AI, hold for the future of the assistive technology sector, as a supportive and constructive mechanism in any decision-making process.

OBJECTIVES

In this short paper, we seek to describe some of the principles that such an AI-based recommendation system should be built upon, using the Atvisor platform as a case study. Atvisor.ai is an AI-based digital platform that supports assistive technology assessments and the decision-making process.

RECOMMENDATIONS

Our recommendations represent the aggregated insights from two pilots held in Israel, testing the platform in multiple environments and with different stakeholders. These recommendations include ensuring the continuum of care and providing a full user journey, incorporating shared decision making and self-assessment features, providing data personalisation and a holistic approach, building a market network infrastructure and designing the tool within a wider service delivery model design. Assessment and decision-making processes, crucial to optimal uptake, cab be leveraged by technology to become more accessible and personalised. IMPLICATIONS FOR REHABILITATION Provides principles for the development of an AI-based recommendation system for assistive technology decision making. Promotes the use of artificial intelligence to support users and professionals in the assistive technology decision making process. Personalization of data regarding assistive technology, according to functional, holistic and client centered profiles of users, ensures optimal match and better use of assistive technology. Self-assessment and professional assessment components are important for enabling multiple access points to the assistive technology decision making process, based on the preferences and needs of users.",,
Explicability of humanitarian AI: a matter of principles,"Artificial intelligence, automated decision-making, and predictive analytics pose ethical and human rights-related implications.",Search,2021,,"Giulio  Coppi, Rebeca  Moreno Jimenez, Sofia  Kyriazi",Journal of International Humanitarian Action,,10.1186/s41018-021-00096-6,https://doi.org/10.1186/s41018-021-00096-6,https://semanticscholar.org/paper/023b4617cfdbcca17c1eb2d938ce8a2106e3ff3d,https://jhumanitarianaction.springeropen.com/track/pdf/10.1186/s41018-021-00096-6,"In the debate on how to improve efficiencies in the humanitarian sector and better meet people’s needs, the argument for the use of artificial intelligence (AI) and automated decision-making (ADMs) systems has gained significant traction and ignited controversy for its ethical and human rights-related implications. Setting aside the implications of introducing unmanned and automated systems in warfare, we focus instead on the impact of the adoption of AI-based ADMs in humanitarian response. In order to maintain the status and protection conferred by the humanitarian mandate, aid organizations are called to abide by a broad set of rules condensed in the humanitarian principles and notably the principles of humanity, neutrality, impartiality, and independence. But how do these principles operate when decision-making is automated? This article opens with an overview of AI and ADMs in the humanitarian sector, with special attention to the concept of algorithmic opacity. It then explores the transformative potential of these systems on the complex power dynamics between humanitarians, principled assistance, and affected communities during acute crises. Our research confirms that the existing flaws in accountability and epistemic processes can be also found in the mathematical and statistical formulas and in the algorithms used for automation, artificial intelligence, predictive analytics, and other efficiency-gaining-related processes. In doing so, our analysis highlights the potential harm to people resulting from algorithmic opacity, either through removal or obfuscation of the causal connection between triggering events and humanitarian services through the so-called black box effect (algorithms are often described as black boxes, as their complexity and technical opacity hide and obfuscate their inner workings (Diakopoulos, Tow Center for Digital Journ, 2017 ). Recognizing the need for a humanitarian ethics dimension in the analysis of automation, AI, and ADMs used in humanitarian action, we endorse the concept of “explicability” as developed within the ethical framework of machine learning and human-computer interaction, together with a set of proxy metrics. Finally, we stress the need for developing auditable standards, as well as transparent guidelines and frameworks to rein in the risks of what has been defined as humanitarian experimentation (Sandvik, Jacobsen, and McDonald, Int. Rev. Red Cross 99(904), 319–344, 2017). This article concludes that accountability mechanisms for AI-based systems and ADMs used to respond to the needs of populations in situation of vulnerability should be an essential feature by default, in order to preserve the respect of the do no harm principle even in the digital dimension of aid. In conclusion, while we confirm existing concerns related to the adoption of AI-based systems and ADMs in humanitarian action, we also advocate for a roadmap towards humanitarian AI for the sector and introduce a tentative ethics framework as basis for future research.",,Review
Responsible nudging for social good: new healthcare skills for AI-driven digital personal assistants,The AI for Social Good (AI4SG) factors are adopted as the norms constraining maleficence.,Search,2021,,"Marianna  Capasso, Steven  Umbrello","Medicine, health care, and philosophy",,10.1007/s11019-021-10062-z,https://doi.org/10.1007/s11019-021-10062-z,https://semanticscholar.org/paper/08a87c6ac392cb21497da65de51866297396bf80,https://link.springer.com/content/pdf/10.1007/s11019-021-10062-z.pdf,"Traditional medical practices and relationships are changing given the widespread adoption of AI-driven technologies across the various domains of health and healthcare. In many cases, these new technologies are not specific to the field of healthcare. Still, they are existent, ubiquitous, and commercially available systems upskilled to integrate these novel care practices. Given the widespread adoption, coupled with the dramatic changes in practices, new ethical and social issues emerge due to how these systems nudge users into making decisions and changing behaviours. This article discusses how these AI-driven systems pose particular ethical challenges with regards to nudging. To confront these issues, the value sensitive design (VSD) approach is adopted as a principled methodology that designers can adopt to design these systems to avoid harming and contribute to the social good. The AI for Social Good (AI4SG) factors are adopted as the norms constraining maleficence. In contrast, higher-order values specific to AI, such as those from the EU High-Level Expert Group on AI and the United Nations Sustainable Development Goals, are adopted as the values to be promoted as much as possible in design. The use case of Amazon Alexa's Healthcare Skills is used to illustrate this design approach. It provides an exemplar of how designers and engineers can begin to orientate their design programs of these technologies towards the social good.",,
Solidarity should be a core ethical principle of AI,Solidarity is almost never included as a principle in ethical guidelines for the development of AI.,Search,2019,8,Miguel  Luengo-Oroz,Nat. Mach. Intell.,,10.1038/s42256-019-0115-3,https://doi.org/10.1038/s42256-019-0115-3,https://semanticscholar.org/paper/ac0b0d62ce89f7afc881a96e4baf5d8fffafb129,https://www.nature.com/articles/s42256-019-0115-3.pdf,"Solidarity is one of the fundamental values at the heart of the construction of peaceful societies and present in more than one third of world's constitutions. Still, solidarity is almost never included as a principle in ethical guidelines for the development of AI. Solidarity as an AI principle (1) shares the prosperity created by AI, implementing mechanisms to redistribute the augmentation of productivity for all; and shares the burdens, making sure that AI does not increase inequality and no human is left behind. Solidarity as an AI principle (2) assesses the long term implications before developing and deploying AI systems so no groups of humans become irrelevant because of AI systems. Considering solidarity as a core principle for AI development will provide not just an human-centric but a more humanity-centric approach to AI.",,
Enhanced well-being assessment as basis for the practical implementation of ethical and rights-based normative principles for AI,AI impacts can be assessed using a human-centered algorithmically-supported approach.,Search,2020,3,"Marek  Havrda, Bogdana  Rakova","2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",,10.1109/SMC42975.2020.9283137,https://doi.org/10.1109/SMC42975.2020.9283137,https://semanticscholar.org/paper/867ba0f0e4843b889ec1d96ae52bd7f3c49f7603,http://arxiv.org/pdf/2007.14826,"Artificial Intelligence (AI) has an increasing impact on all areas of people’s livelihoods. A detailed look at existing interdisciplinary and transdisciplinary metrics frameworks could bring new insights and enable practitioners to navigate the challenge of understanding and assessing the impact of Autonomous and Intelligent Systems (A/IS). There has been emerging consensus on fundamental ethical and rights-based AI principles proposed by scholars, governments, civil rights organizations, and technology companies. In order to move from principles to real-world implementation, we adopt a lens motivated by regulatory impact assessments and the well-being movement in public policy. Similar to public policy interventions, outcomes of AI systems implementation may have far-reaching complex impacts. In public policy, indicators are only part of a broader toolbox, as metrics inherently lead to gaming and dissolution of incentives and objectives. Similarly, in the case of A/IS, there’s a need for a larger toolbox that allows for the iterative assessment of identified impacts, inclusion of new impacts in the analysis, and identification of emerging trade-offs. In this paper, we propose the practical application of an enhanced well-being impact assessment framework for A/IS that could be employed to address ethical and rights-based normative principles in AI. This process could enable a human-centered algorithmically-supported approach to the understanding of the impacts of AI systems. Finally, we propose a new testing infrastructure which would allow for governments, civil rights organizations, and others, to engage in cooperating with A/IS developers towards implementation of enhanced well-being impact assessments.",,
The Role and Limits of Principles in AI Ethics: Towards a Focus on Tensions,AI ethics is facing several challenges and must focus on resolving practical tensions to aid ethical decisions.,Search,2019,72,"Jess  Whittlestone, Rune  Nyrup, Anna  Alexandrova, Stephen  Cave",AIES,,10.1145/3306618.3314289,https://doi.org/10.1145/3306618.3314289,https://semanticscholar.org/paper/304f2d1cf97ac8fb071b5357e97691d64f4e8f6a,https://dl.acm.org/doi/pdf/10.1145/3306618.3314289,"The last few years have seen a proliferation of principles for AI ethics. There is substantial overlap between different sets of principles, with widespread agreement that AI should be used for the common good, should not be used to harm people or undermine their rights, and should respect widely held values such as fairness, privacy, and autonomy. While articulating and agreeing on principles is important, it is only a starting point. Drawing on comparisons with the field of bioethics, we highlight some of the limitations of principles: in particular, they are often too broad and high-level to guide ethics in practice. We suggest that an important next step for the field of AI ethics is to focus on exploring the tensions that inevitably arise as we try to implement principles in practice. By explicitly recognising these tensions we can begin to make decisions about how they should be resolved in specific cases, and develop frameworks and guidelines for AI ethics that are rigorous and practically relevant. We discuss some different specific ways that tensions arise in AI ethics, and what processes might be needed to resolve them.",,
AI virtues - The missing link in putting AI ethics into practice,AI ethics underwent a practical turn but without deviating from the principled approach and the many shortcomings associated with it.,Search,2020,2,Thilo  Hagendorff,ArXiv,,,,https://semanticscholar.org/paper/883eea577a485ddf39697cce549e05234ef85827,,"Several seminal ethics initiatives have stipulated sets of principles and standards for good technology development in the AI sector. However, widespread criticism has pointed out a lack of practical realization of these principles. Following that, AI ethics underwent a practical turn, but without deviating from the principled approach and the many shortcomings associated with it. This paper proposes a different approach. It defines four basic AI virtues, namely justice, honesty, responsibility and care, all of which represent specific motivational settings that constitute the very precondition for ethical decision making in the AI field. Moreover, it defines two second-order AI virtues, prudence and fortitude, that bolster achieving the basic virtues by helping with overcoming bounded ethicality or the many hidden psychological forces that impair ethical decision making and that are hitherto completely disregarded in AI ethics. Lastly, the paper describes measures for successfully cultivating the mentioned virtues in organizations dealing with AI research and development.",,
From principles to practice: How can we make AI ethics measurable?,Organizations that develop and deploy AI systems should implement precepts of ethical AI development.,Search,2021,,William  Warby,,,,,https://semanticscholar.org/paper/6629598bf1d16983bdffa9cade2216088b6e94d6,,"are omnipresent. A growing number of guidelines for the ethical development of socalled arti cial intelligence (AI) have been put forward by stakeholders from the private sector, civil society, and the scienti c and policymaking spheres. The Bertelsmann Stiftung’s Algo.Rules are among this body of proposals. However, it remains unclear how organizations that develop and deploy AI systems should implement precepts of this kind. In cooperation with the nonpro t VDE standardssetting organization, we are seeking to bridge this gap with a new working paper that demonstrates how AI ethics principles can be put into practice.",,