Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Safe AI Systems,AI must meet with the evil effects of systems by applying safety measures.,Search,2018,,Ananta  Bhatt,International Journal of Computer Applications,,10.5120/ijca2018918205,https://doi.org/10.5120/ijca2018918205,https://semanticscholar.org/paper/97aba8083bc5ad49af27a5c26c2bd40e6a17d6a3,,"As you know, it is inevitable to cease the advancement in AI because of the quest for developing systems that learns from experience and comply with the human capabilities. The leading ground-breaking impacts offers both positive and negative aspirations and are addressed as part of this research. To meet with the evil effects of the AI systems, is now the need of the hour to direct the focus on creating safe AI systems. The safe system must satisfy with 2 majorsapplying the security measures and analysing the system. Moreover, system must fulfil the risk and safety valuesintelligence, goals and safety in order to minimise the destruction that may occur due AI technology. However, further research is needed on the actual implementation of the safe AI systems in order to proceed with this research.",,
Safe Artificial Intelligence and Formal Methods - (Position Paper),Formal methods can assist in building safer and reliable AI.,Search,2016,9,Emil  Vassev,ISoLA,,10.1007/978-3-319-47166-2_49,https://doi.org/10.1007/978-3-319-47166-2_49,https://semanticscholar.org/paper/796dcb12b96e5d0c2f358b1b67fdfba36d023acc,https://ulir.ul.ie/bitstream/10344/5407/2/Vassev_2016_safe.pdf,"In one aspect of our life or another, today we all live with AI. For example, the mechanisms behind the search engines operating on the Internet do not just retrieve information, but also constantly learn how to respond more rapidly and usefully to our requests. Although framed by its human inventors, this AI is getting stronger and more powerful every day to go beyond the original human intentions in the future. One of the major questions emerging along with the propagation of AI in both technology and life is about safety in AI. This paper presents the author’s view about how formal methods can assist us in building safer and reliable AI.",,
Decision Support for Safe AI Design,There is considerable interest in ethical designs for artificial intelligence that do not pose risks to humans.,Search,2012,4,Bill  Hibbard,AGI,,10.1007/978-3-642-35506-6_13,https://doi.org/10.1007/978-3-642-35506-6_13,https://semanticscholar.org/paper/c40cad8cacaa47aac6d4cab8f80749beaebd690c,http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_57.pdf,"There is considerable interest in ethical designs for artificial intelligence (AI) that do not pose risks to humans. This paper proposes using elements of Hutter's agent-environment framework to define a decision support system for simulating, visualizing and analyzing AI designs to understand their consequences. The simulations do not have to be accurate predictions of the future; rather they show the futures that an agent design predicts will fulfill its motivations and that can be explored by AI designers to find risks to humans. In order to safely create a simulation model this paper shows that the most probable finite stochastic program to explain a finite history is finitely computable, and that there is an agent that makes such a computation without any unintended instrumental actions. It also discusses the risks of running an AI in a simulated environment.",,
Unifying Principles and Metrics for Safe and Assistive AI,AI safety focuses on designing AI systems that allow humans to safely instruct and control AI systems.,Search,2021,4,Siddharth  Srivastava,AAAI,,,,https://semanticscholar.org/paper/967cba5849f93869655af8d3d8fda9b0a3304a1d,,"The prevalence and success of AI applications have been tempered by concerns about the controllability of AI systems about AI’s impact on the future of work. These concerns reflect two aspects of a central question: how would humans work with AI systems? While research on AI safety focuses on designing AI systems that allow humans to safely instruct and control AI systems, research on AI and the future of work focuses on the impact of AI on humans who may be unable to do so. This Blue Sky Ideas paper proposes a unifying set of declarative principles that enable a more uniform evaluation of arbitrary AI systems along multiple dimensions of the extent to which they are suitable for use by specific classes of human operators. It leverages recent AI research and the unique strengths of the field to develop human-centric principles for AI systems that address the concerns noted above.",,
Designing a Safe Autonomous Artificial Intelligence Agent based on Human Self-Regulation,AI agents must be designed to follow initial programming intentions as the program grows in complexity.,Search,2017,,Mark  Muraven,ArXiv,,,,https://semanticscholar.org/paper/cbe8428141197ce9e9daf4e0952ab58b67cfcee6,,"There is a growing focus on how to design safe artificial intelligent (AI) agents. As systems become more complex, poorly specified goals or control mechanisms may cause AI agents to engage in unwanted and harmful outcomes. Thus it is necessary to design AI agents that follow initial programming intentions as the program grows in complexity. How to specify these initial intentions has also been an obstacle to designing safe AI agents. Finally, there is a need for the AI agent to have redundant safety mechanisms to ensure that any programming errors do not cascade into major problems. Humans are autonomous intelligent agents that have avoided these problems and the present manuscript argues that by understanding human self-regulation and goal setting, we may be better able to design safe AI agents. Some general principles of human self-regulation are outlined and specific guidance for AI design is given.",,
Safe and sound - artificial intelligence in hazardous applications,Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making.,Search,2000,272,"John  Fox, Subrata Kumar Das",,,,,https://semanticscholar.org/paper/f62d215d4c40d299929d2e7fa105d54623a9e1bf,,"Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making, where small faults or errors can spell human catastrophe. This book describes, from both practical and theoretical perspectives, an AI technology for supporting sound clinical decision making and safe patient management. The technology combines techniques from conventional software engineering with a systematic method for building intelligent agents. Although the focus is on medicine, many of the ideas can be applied to AI systems in other hazardous settings. The book also covers a number of general AI problems, including knowledge representation and expertise modeling, reasoning and decision making under uncertainty, planning and scheduling, and the design and implementation of intelligent agents.The book, written in an informal style, begins with the medical background and motivations, technical challenges, and proposed solutions. It then turns to a wide-ranging discussion of intelligent and autonomous agents, with particular reference to safety and hazard management. The final section provides a detailed discussion of the knowledge representation and other aspects of the agent model developed in the book, along with a formal logical semantics for the language.",,
Structured access to AI capabilities: an emerging paradigm for safe AI deployment,Structured capability access (“SCA”) is an emerging paradigm for the safe deployment of artificial intelligence (AI).,Search,2022,,Toby  Shevlane,ArXiv,,,,https://semanticscholar.org/paper/6dc9d4caa30112a85df613d1992563354c010011,,"Structured capability access (“SCA”) is an emerging paradigm for the safe deployment of artificial intelligence (AI). Instead of openly disseminating AI systems, developers facilitate controlled, arm’s length interactions with their AI systems. The aim is to prevent dangerous AI capabilities from being widely accessible, whilst preserving access to AI capabilities that can be used safely. The developer must both restrict how the AI system can be used, and prevent the user from circumventing these restrictions through modification or reverse engineering of the AI system. SCA is most effective when implemented through cloud-based AI services, rather than disseminating AI software that runs locally on users’ hardware. Cloud-based interfaces provide the AI developer greater scope for controlling how the AI system is used, and for protecting against unauthorized modifications to the system’s design. This chapter expands the discussion of “publication norms” in the AI community, which to date has focused on the question of how the informational content of AI research projects should be disseminated (e.g., code and models). Although this is an important question, there are limits to what can be achieved through the control of information flows. SCA views AI software not only as information that can be shared but also as a tool with which users can have arm’s length interactions. There are early examples of SCA being practiced by AI developers, but there is much room for further development, both in the functionality of cloud-based interfaces and in the wider institutional framework.",,
Safe AI for CPS (Invited Paper),Safety can be ensured by formal verification which rules out errant behavior at design time.,Search,2018,5,"Nathan  Fulton, André  Platzer",2018 IEEE International Test Conference (ITC),,10.1109/TEST.2018.8624774,https://doi.org/10.1109/TEST.2018.8624774,https://semanticscholar.org/paper/03fce03060f6110b2772958f58539320bab519ed,,"Autonomous cyber-physical systems-such as self-driving cars and autonomous drones-often leverage artificial intelligence and machine learning algorithms to act well in open environments. Although testing plays an important role in ensuring safety and robustness, modern autonomous systems have grown so complex that achieving safety via testing alone is intractable. Formal verification reduces this testing burden by ruling out large classes of errant behavior at design time. This paper reviews recent work toward developing formal methods for cyber-physical systems that use AI for planning and control by combining the rigor of formal proofs with the flexibility of reinforcement learning.",,Review
Controlling Safety of Artificial Intelligence-Based Systems in Healthcare,A safety controlling system could guide and control the implementation of AI systems in the healthcare industry.,Search,2021,6,"Mohammad Reza Davahli, Waldemar  Karwowski, Krzysztof  Fiok, Thomas T. H. Wan, Hamid R. Parsaei",Symmetry,,10.3390/sym13010102,https://doi.org/10.3390/sym13010102,https://semanticscholar.org/paper/f725733c4a497ef80f089d391c7b9f5304206d28,https://www.mdpi.com/2073-8994/13/1/102/pdf,"In response to the need to address the safety challenges in the use of artificial intelligence (AI), this research aimed to develop a framework for a safety controlling system (SCS) to address the AI black-box mystery in the healthcare industry. The main objective was to propose safety guidelines for implementing AI black-box models to reduce the risk of potential healthcare-related incidents and accidents. The system was developed by adopting the multi-attribute value model approach (MAVT), which comprises four symmetrical parts: extracting attributes, generating weights for the attributes, developing a rating scale, and finalizing the system. On the basis of the MAVT approach, three layers of attributes were created. The first level contained 6 key dimensions, the second level included 14 attributes, and the third level comprised 78 attributes. The key first level dimensions of the SCS included safety policies, incentives for clinicians, clinician and patient training, communication and interaction, planning of actions, and control of such actions. The proposed system may provide a basis for detecting AI utilization risks, preventing incidents from occurring, and developing emergency plans for AI-related risks. This approach could also guide and control the implementation of AI systems in the healthcare industry.",,
Guidelines for Artificial Intelligence Containment,AI safety researchers need reliable sandboxing software to contain intelligent programs.,Search,2019,20,"James  Babcock, János  Kramár, Roman V. Yampolskiy",Next-Generation Ethics,,10.1017/9781108616188.008,https://doi.org/10.1017/9781108616188.008,https://semanticscholar.org/paper/b0c820f9d7fa4d8c56a7887548834b7ba65ddfa5,http://www.vetta.org/documents/Machine_Super_Intelligence.pdf,"With almost daily improvements in capabilities of artificial intelligence it is more important than ever to develop safety software for use by the AI research community. Building on our previous work on AI Containment Problem we propose a number of guidelines which should help AI safety researchers to develop reliable sandboxing software for intelligent programs of all levels. Such safety container software will make it possible to study and analyze intelligent artificial agent while maintaining certain level of safety against information leakage, social engineering attacks and cyberattacks from within the container.",,
"Hybrid Strategies Towards Safe ""Self-Aware"" Superintelligent Systems",A hybrid framework combining AI Safety measures with AGI “Self-awareness” could contribute to the AI alignment problem.,Search,2018,7,"Nadisha-Marie  Aliman, Leon  Kester",AGI,,10.1007/978-3-319-97676-1_1,https://doi.org/10.1007/978-3-319-97676-1_1,https://semanticscholar.org/paper/925f98c25614450d9384f10da1b1f7ed8947b6e1,,"Against the backdrop of increasing progresses in AI research paired with a rise of AI applications in decision-making processes, security-critical domains as well as in ethically relevant frames, a large-scale debate on possible safety measures encompassing corresponding long-term and short-term issues has emerged across different disciplines. One pertinent topic in this context which has been addressed by various AI Safety researchers is e.g. the AI alignment problem for which no final consensus has been achieved yet. In this paper, we present a multidisciplinary toolkit of AI Safety strategies combining considerations from AI and Systems Engineering as well as from Cognitive Science with a security mindset as often relevant in Cybersecurity. We elaborate on how AGI “Self-awareness” could complement different AI Safety measures in a framework extended by a jointly performed Human Enhancement procedure. Our analysis suggests that this hybrid framework could contribute to undertake the AI alignment problem from a new holistic perspective through security-building synergetic effects emerging thereof and could help to increase the odds of a possible safe future transition towards superintelligent systems.",,
"Human-Centered Artificial Intelligence: Reliable, Safe & Trustworthy",The methods of Human-Centered Artificial Intelligence are more likely to produce RST technologies.,Search,2020,105,Ben  Shneiderman,Int. J. Hum. Comput. Interact.,,10.1080/10447318.2020.1741118,https://doi.org/10.1080/10447318.2020.1741118,https://semanticscholar.org/paper/e49f67fa5c946ad24afcf59699a9cacf1ca53924,http://arxiv.org/pdf/2002.04087,"ABSTRACT Well-designed technologies that offer high levels of human control and high levels of computer automation can increase human performance, leading to wider adoption. The Human-Centered Artificial Intelligence (HCAI) framework clarifies how to (1) design for high levels of human control and high levels of computer automation so as to increase human performance, (2) understand the situations in which full human control or full computer control are necessary, and (3) avoid the dangers of excessive human control or excessive computer control. The methods of HCAI are more likely to produce designs that are Reliable, Safe & Trustworthy (RST). Achieving these goals will dramatically increase human performance, while supporting human self-efficacy, mastery, creativity, and responsibility.",,
Safe AI - IS This Possible?,"The inherently self-learning, self-adaptive nature of AI techniques may require functionally deterministic controllers.",Search,1994,1,M. G. Rodd,,,10.1016/S1474-6670(17)45702-8,https://doi.org/10.1016/S1474-6670(17)45702-8,https://semanticscholar.org/paper/a9ac4c8bd3611e530f5315b54abefe100ba79ae0,,"Abstract This deliberately provocative paper poses the question as to whether it is professionally-acceptable to use AI-based control systems in real-time automation. The premise is that the increasing demands for improved control of industrial processes are resulting in a search for new control techniques which, it is suggested, will be based on various AI methodologies. However, in the face of the inherently self-learning, self-adaptive nature of such techniques, it is pointed out that to produce safe, predictable systems, the controllers should be functionally deterministic! In posing this dilemma, the paper suggests possible ways in which it may be resolved.",,
Safe AI—is this possible?☆,"The inherently self-learning, self-adaptive nature of AI control systems makes them inherently unsafe.",Search,1995,11,M. G. Rodd,,,10.1016/0952-1976(95)00010-X,https://doi.org/10.1016/0952-1976(95)00010-X,https://semanticscholar.org/paper/c53376e4e49ca954f4f4f2ca520b6042967f7dda,,"Abstract This deliberately provocative paper poses the question as to whether it is professionally acceptable to use AI-based control systems in real-time automation. The premise is that the increasing demands for improved control of industrial processes are resulting in a search for new control techniques which, it is suggested, will be based on various AI methodologies. However, in the face of the inherently self-learning, self-adaptive nature of such techniques, it is pointed out that to produce safe, predictable systems, the controllers should be functionally deterministic! In posing this dilemma, the paper suggests possible ways in which it may be resolved.",,
Safe AI: creating a health care expert system,,Search,2015,,"Marcos  Pinto, Mamun  Hasam, Hsinrong  Wei",,,,,https://semanticscholar.org/paper/b7f122b8683bd3f1efc2bf9e1b189fe26785dbde,,"This paper describes the process of creating a health care expert system (ES) step-by-step. An expert system, a branch of Artificial Intelligence (AI), will be designed using rule-based method to indicate the most possible diagnosis for upper respiratory infections (URI) such as common cold, flu (influenza), croup, and sinus infection. The expert system presents the user with a list of possible diagnoses and their corresponding explanations depending on certain user's information: age, gender, procedence region, and medical history. It is a hands-on example that can be incorporated into college courses on Web services and/or mobile applications.",,
Governing the safety of artificial intelligence in healthcare,Artificial intelligence technologies will introduce new risks to healthcare and amplify existing ones.,Search,2019,28,Carl  Macrae,BMJ Quality & Safety,,10.1136/bmjqs-2019-009484,https://doi.org/10.1136/bmjqs-2019-009484,https://semanticscholar.org/paper/b91e4c2234c950a8a639ba1d3f97dcd51220520a,https://nottingham-repository.worktribe.com/preview/2062727/MacraeGoverningAIsafety.pdf,"Artificial intelligence (AI) has enormous potential to improve the safety of healthcare, from increasing diagnostic accuracy,1 to optimising treatment planning,2 to forecasting outcomes of care.3 However, integrating AI technologies into the delivery of healthcare is likely to introduce a range of new risks and amplify existing ones. For instance, failures in widely used software have the potential to quickly affect large numbers of patients4; hidden assumptions in underlying data and models can lead to AI systems delivering dangerous recommendations that are insensitive to local care processes,5 6 and opaque AI techniques such as deep learning can make explaining and learning from failure extremely difficult.7 8 To maximise the benefits of AI in healthcare and to build trust among patients and practitioners, it will therefore be essential to robustly govern the risks that AI poses to patient safety.

In a recent review in this journal, Challen and colleagues present an important and timely analysis of some of the key technological risks associated with the application of machine learning in clinical settings.9 Machine learning is a subfield of AI that focuses on the development of algorithms that are automatically derived and optimised through exposure to large quantities of exemplar ‘training’ data.10 The outputs of machine learning algorithms are essentially classifications of patterns that provide some sort of prediction—for instance, predicting whether an image shows a malignant melanoma or a benign mole.11 Some of the basic techniques of machine learning have existed for half a century or more, but progress in the field has accelerated rapidly due to advances in the development of ‘deep’ artificial neural networks12 combined with huge increases in computational power and the availability of enormous quantities of data. These techniques have underpinned recent public demonstrations of AI systems …",,Review