Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Opening the software engineering toolbox for the assessment of trustworthy AI,Software engineering and testing practices can be used for the assessment of trustworthy AI.,Search,2020,2,"Mohit Kumar Ahuja, Mohamed-Bachir  Belaid, Pierre  Bernab'e, Mathieu  Collet, Arnaud  Gotlieb, Chhagan  Lal, Dusica  Marijan, Sagar  Sen, Aizaz  Sharif, Helge  Spieker",NeHuAI@ECAI,,,,https://semanticscholar.org/paper/3b0ff6bd000e9c615d024614343f2c1cf12bf124,,"Trustworthiness is a central requirement for the acceptance and success of human-centered artificial intelligence (AI). To deem an AI system as trustworthy, it is crucial to assess its behaviour and characteristics against a gold standard of Trustworthy AI, consisting of guidelines, requirements, or only expectations. While AI systems are highly complex, their implementations are still based on software. The software engineering community has a long-established toolbox for the assessment of software systems, especially in the context of software testing. In this paper, we argue for the application of software engineering and testing practices for the assessment of trustworthy AI. We make the connection between the seven key requirements as defined by the European Commission's AI high-level expert group and established procedures from software engineering and raise questions for future work.",,
Z-Inspection®: A Process to Assess Trustworthy AI,Z-Inspection® is a process to assess trustworthy AI.,Search,2021,7,"Roberto V. Zicari, John  Brodersen, James  Brusseau, Boris  Düdder, Timo  Eichhorn, Todor  Ivanov, Georgios  Kararigas, Pedro  Kringen, Melissa  McCullough, Florian  Möslein, Naveed  Mushtaq, Gemma  Roig, Norman  Stürtz, Karsten  Tolle, Jesmin Jahan Tithi, Irmhild van Halem, Magnus  Westerlund",IEEE Transactions on Technology and Society,,10.1109/TTS.2021.3066209,https://doi.org/10.1109/TTS.2021.3066209,https://semanticscholar.org/paper/129782466966ed199864a280f69a740d3f6a4c50,https://ieeexplore.ieee.org/ielx7/8566059/9459493/09380498.pdf,"The ethical and societal implications of artificial intelligence systems raise concerns. In this article, we outline a novel process based on applied ethics, namely, Z-Inspection®, to assess if an AI system is trustworthy. We use the definition of trustworthy AI given by the high-level European Commission’s expert group on AI. Z-Inspection® is a general inspection process that can be applied to a variety of domains where AI systems are used, such as business, healthcare, and public sector, among many others. To the best of our knowledge, Z-Inspection® is the first process to assess trustworthy AI in practice.",,
Trustworthy Acceptance: A New Metric for Trustworthy Artificial Intelligence Used in Decision Making in Food-Energy-Water Sectors,A trustworthy acceptance metric can capture and quantify the system’s transparent evaluation by field experts on as many control points as desirable by the users.,Search,2021,1,"Suleyman  Uslu, Davinder  Kaur, Samuel J. Rivera, Arjan  Durresi, Mimoza  Durresi, Meghna  Babbar-Sebens",AINA,,10.1007/978-3-030-75100-5_19,https://doi.org/10.1007/978-3-030-75100-5_19,https://semanticscholar.org/paper/5d6a347ac08ff55ee9ac0234c36d9f41b0d7088c,,"We propose, for the first time, a trustworthy acceptance metric and its measurement methodology to evaluate the trustworthiness of AI-based systems used in decision making in Food Energy Water (FEW) management. The proposed metric is a significant step forward in the standardization process of AI systems. It is essential to standardize the AI systems’ trustworthiness, but until now, the standardization efforts remain at the level of high-level principles. The measurement methodology of the proposed includes human experts in the loop, and it is based on our trust management system. Our metric captures and quantifies the system’s transparent evaluation by field experts on as many control points as desirable by the users. We illustrate the trustworthy acceptance metric and its measurement methodology using AI in decision-making scenarios of Food-Energy-Water sectors. However, the proposed metric and its methodology can be easily adapted to other fields of AI applications. We show that our metric successfully captures the aggregated acceptance of any number of experts, can be used to do multiple measurements on various points of the system, and provides confidence values for the measured acceptance. Suleyman Uslu Indiana University-Purdue University Indianapolis, Indianapolis, IN, USA e-mail: suslu@iu.edu Davinder Kaur Indiana University-Purdue University Indianapolis, Indianapolis, IN, USA e-mail: davikaur@iu.edu Samuel J Rivera Oregon State University, Corvallis, OR, USA e-mail: sammy.rivera@oregonstate.edu Arjan Durresi Indiana University-Purdue University Indianapolis, Indianapolis, IN, USA e-mail: adurresi@iupui.edu Mimoza Durresi European University of Tirana e-mail: mimoza.durresi@uet.edu.al Meghna Babbar-Sebens Oregon State University, Corvallis, OR, USA e-mail: meghna@oregonstate.edu",,
Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence,An automatic online assessment method for the reliability of CPS was proposed in this article.,Search,2021,81,"Zhihan  Lv, Yang  Han, Amit Kumar Singh, Gunasekaran  Manogaran, Haibin  Lv",IEEE Transactions on Industrial Informatics,,10.1109/TII.2020.2994747,https://doi.org/10.1109/TII.2020.2994747,https://semanticscholar.org/paper/79086f67c5d7413a05305478b1b38781588ed19d,,"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.",,
Trustworthiness and IT Suspicion: An Evaluation of the Nomological Network,Researchers may consider using separate measures for trust and distrust in future studies.,Search,2011,39,"Joseph B. Lyons, Charlene K. Stokes, Kevin J. Eschleman, Gene M. Alarcon, Alexander J. Barelka",Hum. Factors,,10.1177/0018720811406726,https://doi.org/10.1177/0018720811406726,https://semanticscholar.org/paper/41d9807d75bfce13ec996f26f552953058f070e2,http://journals.sagepub.com/doi/pdf/10.1177/0018720811406726,"Objective: The authors evaluated the validity of trust in automation and information technology (IT) suspicion by examining their factor structure and relationship with decision confidence. Background: Research on trust has burgeoned, yet the dimensionality of trust remains elusive. Researchers suggest that trust is a unidimensional construct, whereas others believe it is multidimensional. Additionally, novel constructs, such as IT suspicion, have yet to be distinguished from trust in automation. Research is needed to examine the overlap between these constructs and to determine the dimensionality of trust in automation. Method: Participants (N = 72) engaged in a computer-based convoy scenario involving an automated decision aid. The aid fused real-time sensor data and provided route recommendations to participants who selected a route based on (a) a map with historical enemy information, (b) sensor inputs, and (c) automation suggestions. Measures for trust in automation and IT suspicion were administered after individuals interacted with the automation. Results: Results indicated three orthogonal factors: trust, distrust, and IT suspicion. Each variable was explored as a predictor of decision confidence. Distrust and trust evidenced unique influences on decision confidence, albeit at different times. Higher distrust related to less confidence, whereas trust related to greater confidence. Conclusion: The current study found that trust in automation was best characterized by two orthogonal dimensions (trust and distrust). Both trust and distrust were found to be independent from IT suspicion, and both distrust and trust uniquely predicted decision confidence. Application: Researchers may consider using separate measures for trust and distrust in future studies.",,
Trustworthiness of Artificial Intelligence,A mistake in the development or working phase of an artificial intelligence system can be disastrous.,Search,2020,4,"Sonali  Jain, Manan  Luthra, Shagun  Sharma, Mehtab  Fatima",2020 6th International Conference on Advanced Computing and Communication Systems (ICACCS),,10.1109/ICACCS48705.2020.9074237,https://doi.org/10.1109/ICACCS48705.2020.9074237,https://semanticscholar.org/paper/2efae53ba8d84c6f11d3f7151f23b9e22ca806e4,,"This paper discusses the need for a trustworthy AI, along with the ethics which are required to keep that trust intact. AI has a lot of benefits when it comes to societal, individual or cultural development. But any mistake in either the development or in the working phase of the AI system can be disastrous, especially when human lives are involved. The main goal of this paper is to understand what really makes an Artificial Intelligence system trustworthy.",,
Traceability for Trustworthy AI: A Review of Models and Tools,A common approach and shared semantics are lacking for traceability tools.,Search,2021,3,"Marçal Mora Cantallops, Salvador  Sánchez-Alonso, Elena García Barriocanal, Miguel-Ángel  Sicilia",Big Data Cogn. Comput.,,10.3390/BDCC5020020,https://doi.org/10.3390/BDCC5020020,https://semanticscholar.org/paper/a8defe1d2e28879ff86e3cb5c6c86b2e78901c6f,https://www.mdpi.com/2504-2289/5/2/20/pdf,"Traceability is considered a key requirement for trustworthy artificial intelligence (AI), related to the need to maintain a complete account of the provenance of data, processes, and artifacts involved in the production of an AI model. Traceability in AI shares part of its scope with general purpose recommendations for provenance as W3C PROV, and it is also supported to different extents by specific tools used by practitioners as part of their efforts in making data analytic processes reproducible or repeatable. Here, we review relevant tools, practices, and data models for traceability in their connection to building AI models and systems. We also propose some minimal requirements to consider a model traceable according to the assessment list of the High-Level Expert Group on AI. Our review shows how, although a good number of reproducibility tools are available, a common approach is currently lacking, together with the need for shared semantics. Besides, we have detected that some tools have either not achieved full maturity, or are already falling into obsolescence or in a state of near abandonment by its developers, which might compromise the reproducibility of the research trusted to them.",,Review
Trustworthy AI,The pursuit of responsible AI raises the ante on both the trustworthy computing and formal methods communities.,Search,2021,7,Jeannette M. Wing,Commun. ACM,,10.1145/3448248,https://doi.org/10.1145/3448248,https://semanticscholar.org/paper/33cf9b4d6c76f988380b1adff2c06c30010f93d3,https://dl.acm.org/doi/pdf/10.1145/3448248,The pursuit of responsible AI raises the ante on both the trustworthy computing and formal methods communities.,,
In AI We Trust? Factors That Influence Trustworthiness of AI-infused Decision-Making Processes,"Factors such as decision stakes, decision authority, model trainer, model interpretability, social transparency, and model confidence influence ratings of trust in AI-infused decision-making processes.",Search,2019,7,"Maryam  Ashoori, Justin D. Weisz",ArXiv,,,,https://semanticscholar.org/paper/4155178a77ec7f8775c814c6216002a82d0cab97,,"Many decision-making processes have begun to incorporate an AI element, including prison sentence recommendations, college admissions, hiring, and mortgage approval. In all of these cases, AI models are being trained to help human decision makers reach accurate and fair judgments, but little is known about what factors influence the extent to which people consider an AI-infused decision-making process to be trustworthy. We aim to understand how different factors about a decision-making process, and an AI model that supports that process, influences peoples' perceptions of the trustworthiness of that process. We report on our evaluation of how seven different factors -- decision stakes, decision authority, model trainer, model interpretability, social transparency, and model confidence -- influence ratings of trust in a scenario-based study.",,
Trustworthy AI: From Principles to Practices,Comprehensive trustworthy AI systems require monitoring and governance.,Search,2021,1,"Bo  Li, Peng  Qi, Bo  Liu, Shuai  Di, Jingen  Liu, Jiquan  Pei, Jinfeng  Yi, Bowen  Zhou",ArXiv,,,,https://semanticscholar.org/paper/c3689493757f90267908e776aeada9194fce55c7,,"Fast developing artificial intelligence (AI) technology has enabled various applied systems deployed in the real world, impacting people’s everyday lives. However, many current AI systems were found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection, etc., which not only degrades user experience but erodes the society’s trust in all AI systems. In this review, we strive to provide AI practitioners a comprehensive guide towards building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, alignment with human values, and accountability. We then survey leading approaches in these aspects in the industry. To unify the current fragmented approaches towards trustworthy AI, we propose a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items to practitioners and societal stakeholders (e.g., researchers and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges in the future development of trustworthy AI systems, where we identify the need for paradigm shift towards comprehensive trustworthy AI systems.",,Review
Automating the Evaluation of Trustworthiness,A potential trustor can use a framework to evaluate the potential trustworthiness of a potential trustee.,Search,2021,,"Marc  Sel, Chris J. Mitchell",TrustBus,,10.1007/978-3-030-86586-3_2,https://doi.org/10.1007/978-3-030-86586-3_2,https://semanticscholar.org/paper/89b871acca977719e6c49f8f7720af7edfef85f0,,"Digital services have a significant impact on the lives of many people and organisations. Trust influences decisions regarding potential service providers, and continues to do so once a service provider has been selected. There is no globally accepted model to describe trust in the context of digital services, nor to evaluate the trustworthiness of entities. We present a formal framework to partially fill this gap. It is based on four building blocks: a data model, rulebooks, trustworthiness evaluation functions and instance data. An implementation of this framework can be used by a potential trustor to evaluate the trustworthiness of a potential trustee.",,
Enhancing trustworthiness evaluation in internetware with similarity and non-negative constraints,Trustworthiness evaluation mechanisms can help reduce uncertainty and boost collaborations by providing trustworthy partners/entities.,Search,2013,2,"Guo  Yan, Feng  Xu, Yuan  Yao, Jian  Lu",Internetware,,10.1145/2532443.2532459,https://doi.org/10.1145/2532443.2532459,https://semanticscholar.org/paper/93395325980fb3b911ed8695fec908b18b4f368f,,"Internetware is envisioned as a new software paradigm where software developers usually need to interact with unknown partners as well as the software entities developed by them. To reduce uncertainty and boost collaborations in such setting, it is important to provide trustworthiness evaluation mechanisms so that trustworthy partners/entities can be easily found. In this work, we propose a novel trustworthiness evaluation mechanism by enhancing existing mechanisms with similarity and non-negative constraints. To be specific, we first extend an existing multi-aspect trust inference model by incorporating the non-negative constraint. One of the advantages of such constraint is its strong interpretability. Second, we incorporate similarity into two neighborhood models borrowed from recommender systems. When computing similarity, we make use of the intermediate results from the first step. Finally, these models are combined under a machine learning framework. To show the effectiveness of our method, we conduct experiments on a real data-set. The results show that: both our non-negativity extension and similarity computation improve the evaluation accuracy of the original methods, and the combined method outperforms several state-of-the-art methods.",,
Trustworthy AI Inference Systems: An Industry Research View,"Trends in AI inference systems require the global collective attention of industry, academia, and government researchers to sustain.",Search,2020,2,"Rosario  Cammarota, Matthias  Schunter, Anand  Rajan, Fabian  Boemer, 'Agnes  Kiss, Amos  Treiber, Christian  Weinert, Thomas  Schneider, Emmanuel  Stapf, Ahmad-Reza  Sadeghi, Daniel  Demmler, Huili  Chen, Siam Umar Hussain, Sadegh  Riazi, Farinaz  Koushanfar, Saransh  Gupta, Tajan Simunic Rosing, Kamalika  Chaudhuri, Hamid  Nejatollahi, Nikil  Dutt, Mohsen  Imani, Kim  Laine, Anuj  Dubey, Aydin  Aysu, Fateme Sadat Hosseini, Chengmo  Yang, Eric  Wallace, Pamela  Norton",ArXiv,,,,https://semanticscholar.org/paper/828947e3e9e06c506e6a30e3eb7e176c0b8b953d,,"In this work, we provide an industry research view for approaching the design, deployment, and operation of trustworthy Artificial Intelligence (AI) inference systems. Such systems provide customers with timely, informed, and customized inferences to aid their decision, while at the same time utilizing appropriate security protection mechanisms for AI models. Additionally, such systems should also use Privacy-Enhancing Technologies (PETs) to protect customers' data at any time.

To approach the subject, we start by introducing trends in AI inference systems. We continue by elaborating on the relationship between Intellectual Property (IP) and private data protection in such systems. Regarding the protection mechanisms, we survey the security and privacy building blocks instrumental in designing, building, deploying, and operating private AI inference systems. For example, we highlight opportunities and challenges in AI systems using trusted execution environments combined with more recent advances in cryptographic techniques to protect data in use. Finally, we outline areas of further development that require the global collective attention of industry, academia, and government researchers to sustain the operation of trustworthy AI inference systems.",,
A subjective model for trustworthiness evaluation in the social Internet of Things,The benefits of the Social Internet of Things (SIoT) are those of improving information/service discovery.,Search,2012,147,"Michele  Nitti, Roberto  Girau, Luigi  Atzori, Antonio  Iera, Giacomo  Morabito","2012 IEEE 23rd International Symposium on Personal, Indoor and Mobile Radio Communications - (PIMRC)",,10.1109/pimrc.2012.6362662,https://doi.org/10.1109/pimrc.2012.6362662,https://semanticscholar.org/paper/03d32c3226b58feb3bcf0a2d0b24684220224e2e,https://iris.unica.it/bitstream/11584/105601/1/A%20Subjective%20Model%20for%20Trustworthiness%20Evaluation%20in%20the%20Social%20Internet%20of%20Things.pdf,"The integration of social networking concepts into the Internet of Things (IoT) has led to the so called Social Internet of Things (SIoT) paradigm, according to which the objects are capable of establishing social relationships in an autonomous way with respect to their owners. The benefits are those of improving scalability in information/service discovery when the SIoT is made of huge numbers of heterogeneous nodes, similarly to what happens with social networks among humans. In this paper we focus on the problem of understanding how the information provided by the other members of the SIoT has to be processed so as to build a reliable system on the basis of the behavior of the objects. We define a subjective model for the management of trustworthiness which builds upon the solutions proposed for P2P networks. Each node computes the trustworthiness of its friends on the basis of its own experience and on the opinion of the common friends with the potential service providers. We employ a feedback system and we combine the credibility and centrality of the nodes to evaluate the trust level. Preliminary simulations show the benefits of the proposed model towards the isolation of almost any malicious node in the network.",,
Trustworthy AI in the Age of Pervasive Computing and Big Data,"Trust in AI is intrinsically linked to ethics, including the ethics of algorithms, the ethics of data, or the ethics of practice.",Search,2020,15,"Abhishek  Kumar, Tristan  Braud, Sasu  Tarkoma, Pan  Hui",2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),,10.1109/percomworkshops48775.2020.9156127,https://doi.org/10.1109/percomworkshops48775.2020.9156127,https://semanticscholar.org/paper/f92cedfdf08f7c92ddefebd06fa763d7a8359c1f,http://repository.ust.hk/ir/bitstream/1783.1-101388/1/neutral-ai.pdf,"The era of pervasive computing has resulted in countless devices that continuously monitor users and their environment, generating an abundance of user behavioural data. Such data may support improving the quality of service, but may also lead to adverse usages such as surveillance and advertisement. In parallel, Artificial Intelligence (AI) systems are being applied to sensitive fields such as healthcare, justice, or human resources, raising multiple concerns on the trustworthiness of such systems. Trust in AI systems is thus intrinsically linked to ethics, including the ethics of algorithms, the ethics of data, or the ethics of practice. In this paper, we formalise the requirements of trustworthy AI systems through an ethics perspective. We specifically focus on the aspects that can be integrated into the design and development of AI systems. After discussing the state of research and the remaining challenges, we show how a concrete use-case in smart cities can benefit from these methods.",,
Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims,"Ten mechanisms can provide evidence about the safety, security, fairness, and privacy protection of AI systems.",Search,2020,90,"Miles  Brundage, Shahar  Avin, Jasmine  Wang, Haydn  Belfield, Gretchen  Krueger, Gillian  Hadfield, Heidy  Khlaaf, Jingying  Yang, Helen  Toner, Ruth  Fong, Tegan  Maharaj, Pang Wei Koh, Sara  Hooker, Jade  Leung, Andrew  Trask, Emma  Bluemke, Jonathan  Lebensbold, Cullen  O'Keefe, Mark  Koren, Th'eo  Ryffel, JB  Rubinovitz, Tamay  Besiroglu, Federica  Carugati, Jack  Clark, Peter  Eckersley, Sarah de Haas, Maritza  Johnson, Ben  Laurie, Alex  Ingerman, Igor  Krawczuk, Amanda  Askell, Rosario  Cammarota, Andrew  Lohn, David  Krueger, Charlotte  Stix, Peter  Henderson, Logan  Graham, Carina  Prunkl, Bianca  Martin, Elizabeth  Seger, Noa  Zilberman, Se'an 'O h'Eigeartaigh, Frens  Kroeger, Girish  Sastry, Rebecca  Kagan, Adrian  Weller, Brian  Tse, Elizabeth  Barnes, Allan  Dafoe, Paul  Scharre, Ariel  Herbert-Voss, Martijn  Rasser, Shagun  Sodhani, Carrick  Flynn, Thomas Krendl Gilbert, Lisa  Dyer, Saif  Khan, Yoshua  Bengio, Markus  Anderljung",ArXiv,,,,https://semanticscholar.org/paper/62c3142956d54db158d190ce691e3c13e7897412,,"With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.",,