Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Safe AI Systems,AI must meet with the evil effects of systems by complying with the human capabilities.,Search,2018,,Ananta  Bhatt,International Journal of Computer Applications,,10.5120/ijca2018918205,https://doi.org/10.5120/ijca2018918205,https://semanticscholar.org/paper/97aba8083bc5ad49af27a5c26c2bd40e6a17d6a3,,"As you know, it is inevitable to cease the advancement in AI because of the quest for developing systems that learns from experience and comply with the human capabilities. The leading ground-breaking impacts offers both positive and negative aspirations and are addressed as part of this research. To meet with the evil effects of the AI systems, is now the need of the hour to direct the focus on creating safe AI systems. The safe system must satisfy with 2 majorsapplying the security measures and analysing the system. Moreover, system must fulfil the risk and safety valuesintelligence, goals and safety in order to minimise the destruction that may occur due AI technology. However, further research is needed on the actual implementation of the safe AI systems in order to proceed with this research.",,
Decision Support for Safe AI Design,Simulations do not have to be accurate predictions of the future to be useful for AI safety.,Search,2012,4,Bill  Hibbard,AGI,,10.1007/978-3-642-35506-6_13,https://doi.org/10.1007/978-3-642-35506-6_13,https://semanticscholar.org/paper/c40cad8cacaa47aac6d4cab8f80749beaebd690c,http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_57.pdf,"There is considerable interest in ethical designs for artificial intelligence (AI) that do not pose risks to humans. This paper proposes using elements of Hutter's agent-environment framework to define a decision support system for simulating, visualizing and analyzing AI designs to understand their consequences. The simulations do not have to be accurate predictions of the future; rather they show the futures that an agent design predicts will fulfill its motivations and that can be explored by AI designers to find risks to humans. In order to safely create a simulation model this paper shows that the most probable finite stochastic program to explain a finite history is finitely computable, and that there is an agent that makes such a computation without any unintended instrumental actions. It also discusses the risks of running an AI in a simulated environment.",,
Safe Artificial Intelligence and Formal Methods - (Position Paper),Formal methods can assist in building safer and reliable artificial intelligence.,Search,2016,9,Emil  Vassev,ISoLA,,10.1007/978-3-319-47166-2_49,https://doi.org/10.1007/978-3-319-47166-2_49,https://semanticscholar.org/paper/796dcb12b96e5d0c2f358b1b67fdfba36d023acc,https://ulir.ul.ie/bitstream/10344/5407/2/Vassev_2016_safe.pdf,"In one aspect of our life or another, today we all live with AI. For example, the mechanisms behind the search engines operating on the Internet do not just retrieve information, but also constantly learn how to respond more rapidly and usefully to our requests. Although framed by its human inventors, this AI is getting stronger and more powerful every day to go beyond the original human intentions in the future. One of the major questions emerging along with the propagation of AI in both technology and life is about safety in AI. This paper presents the author’s view about how formal methods can assist us in building safer and reliable AI.",,
Designing a Safe Autonomous Artificial Intelligence Agent based on Human Self-Regulation,AI agents must be designed to follow initial programming intentions as the program grows in complexity.,Search,2017,,Mark  Muraven,ArXiv,,,,https://semanticscholar.org/paper/cbe8428141197ce9e9daf4e0952ab58b67cfcee6,,"There is a growing focus on how to design safe artificial intelligent (AI) agents. As systems become more complex, poorly specified goals or control mechanisms may cause AI agents to engage in unwanted and harmful outcomes. Thus it is necessary to design AI agents that follow initial programming intentions as the program grows in complexity. How to specify these initial intentions has also been an obstacle to designing safe AI agents. Finally, there is a need for the AI agent to have redundant safety mechanisms to ensure that any programming errors do not cascade into major problems. Humans are autonomous intelligent agents that have avoided these problems and the present manuscript argues that by understanding human self-regulation and goal setting, we may be better able to design safe AI agents. Some general principles of human self-regulation are outlined and specific guidance for AI design is given.",,
Safe and sound - artificial intelligence in hazardous applications,Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making.,Search,2000,272,"John  Fox, Subrata Kumar Das",,,,,https://semanticscholar.org/paper/f62d215d4c40d299929d2e7fa105d54623a9e1bf,,"Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making, where small faults or errors can spell human catastrophe. This book describes, from both practical and theoretical perspectives, an AI technology for supporting sound clinical decision making and safe patient management. The technology combines techniques from conventional software engineering with a systematic method for building intelligent agents. Although the focus is on medicine, many of the ideas can be applied to AI systems in other hazardous settings. The book also covers a number of general AI problems, including knowledge representation and expertise modeling, reasoning and decision making under uncertainty, planning and scheduling, and the design and implementation of intelligent agents.The book, written in an informal style, begins with the medical background and motivations, technical challenges, and proposed solutions. It then turns to a wide-ranging discussion of intelligent and autonomous agents, with particular reference to safety and hazard management. The final section provides a detailed discussion of the knowledge representation and other aspects of the agent model developed in the book, along with a formal logical semantics for the language.",,
Guidelines for Artificial Intelligence Containment,AI safety researchers need to develop reliable sandboxing software to study and analyze intelligent programs.,Search,2019,20,"James  Babcock, János  Kramár, Roman V. Yampolskiy",Next-Generation Ethics,,10.1017/9781108616188.008,https://doi.org/10.1017/9781108616188.008,https://semanticscholar.org/paper/b0c820f9d7fa4d8c56a7887548834b7ba65ddfa5,http://www.vetta.org/documents/Machine_Super_Intelligence.pdf,"With almost daily improvements in capabilities of artificial intelligence it is more important than ever to develop safety software for use by the AI research community. Building on our previous work on AI Containment Problem we propose a number of guidelines which should help AI safety researchers to develop reliable sandboxing software for intelligent programs of all levels. Such safety container software will make it possible to study and analyze intelligent artificial agent while maintaining certain level of safety against information leakage, social engineering attacks and cyberattacks from within the container.",,
Safe AI for CPS (Invited Paper),Safety can be ensured by formal verification which rules out errant behavior at design time.,Search,2018,5,"Nathan  Fulton, André  Platzer",2018 IEEE International Test Conference (ITC),,10.1109/TEST.2018.8624774,https://doi.org/10.1109/TEST.2018.8624774,https://semanticscholar.org/paper/03fce03060f6110b2772958f58539320bab519ed,,"Autonomous cyber-physical systems-such as self-driving cars and autonomous drones-often leverage artificial intelligence and machine learning algorithms to act well in open environments. Although testing plays an important role in ensuring safety and robustness, modern autonomous systems have grown so complex that achieving safety via testing alone is intractable. Formal verification reduces this testing burden by ruling out large classes of errant behavior at design time. This paper reviews recent work toward developing formal methods for cyber-physical systems that use AI for planning and control by combining the rigor of formal proofs with the flexibility of reinforcement learning.",,Review
Governing the safety of artificial intelligence in healthcare,Artificial intelligence technologies will introduce new risks to healthcare and amplify existing ones.,Search,2019,28,Carl  Macrae,BMJ Quality & Safety,,10.1136/bmjqs-2019-009484,https://doi.org/10.1136/bmjqs-2019-009484,https://semanticscholar.org/paper/b91e4c2234c950a8a639ba1d3f97dcd51220520a,https://nottingham-repository.worktribe.com/preview/2062727/MacraeGoverningAIsafety.pdf,"Artificial intelligence (AI) has enormous potential to improve the safety of healthcare, from increasing diagnostic accuracy,1 to optimising treatment planning,2 to forecasting outcomes of care.3 However, integrating AI technologies into the delivery of healthcare is likely to introduce a range of new risks and amplify existing ones. For instance, failures in widely used software have the potential to quickly affect large numbers of patients4; hidden assumptions in underlying data and models can lead to AI systems delivering dangerous recommendations that are insensitive to local care processes,5 6 and opaque AI techniques such as deep learning can make explaining and learning from failure extremely difficult.7 8 To maximise the benefits of AI in healthcare and to build trust among patients and practitioners, it will therefore be essential to robustly govern the risks that AI poses to patient safety.

In a recent review in this journal, Challen and colleagues present an important and timely analysis of some of the key technological risks associated with the application of machine learning in clinical settings.9 Machine learning is a subfield of AI that focuses on the development of algorithms that are automatically derived and optimised through exposure to large quantities of exemplar ‘training’ data.10 The outputs of machine learning algorithms are essentially classifications of patterns that provide some sort of prediction—for instance, predicting whether an image shows a malignant melanoma or a benign mole.11 Some of the basic techniques of machine learning have existed for half a century or more, but progress in the field has accelerated rapidly due to advances in the development of ‘deep’ artificial neural networks12 combined with huge increases in computational power and the availability of enormous quantities of data. These techniques have underpinned recent public demonstrations of AI systems …",,Review
Entropic boundary conditions towards safe artificial superintelligence,Artificial superintelligent (ASI) agents that will not cause harm to humans or other organisms are central to mitigating a growing contemporary global safety concern.,Search,2021,,"Santiago  Núñez-Corrales, Eric  Jakobsson",Journal of Experimental & Theoretical Artificial Intelligence,,10.1080/0952813X.2021.1952653,https://doi.org/10.1080/0952813X.2021.1952653,https://semanticscholar.org/paper/026c8c82cf2e2b80e6c0fdf5986b5a6146baa384,,Artificial superintelligent (ASI) agents that will not cause harm to humans or other organisms are central to mitigating a growing contemporary global safety concern as artificial intelligent agent...,,
Artificial Intelligence Helps Protect Smart Homes against Thieves,A home with AI could be safer and more secure against theft.,Search,2020,1,"Zeydin  Pala, Orhan  Özkan",DÜMF Mühendislik Dergisi,,10.24012/dumf.700311,https://doi.org/10.24012/dumf.700311,https://semanticscholar.org/paper/4ffa0b3aa974fc723e88df56c1e5347c173430c4,https://dergipark.org.tr/tr/download/article-file/997310,"Interaction with the environments in which humans live is increasing more and more, and Artificial Intelligence (AI) offers significant contributions to this. Although the topic of smart homes has attracted a great deal of attention from researchers, the AI-based application in this area is still in its infancy. In this study, a home security automation system, which is quite simple, but smart and AI-based, is proposed. When the home-dwellers were not at home, the home lighting system tried to be managed with AI at night, as if life was still there. The AI-based smart home physical design was done using Arduino equipment and was tried to be adapted to the real-life environment with software support. As if there was someone at home, a special dataset, which was consisted of nine inputs, one output vector and about 5500 samples was created to turn on/off the home lights in a manner suitable for night life. The home lighting system was successfully managed using an AI-based system that learns nightlife lighting habits. The proposed system performance was tested in support of commonly used machine learning classification algorithms such as Multi-layer perceptron (MLP), Linear support vector machine (L-SVM), Gaussian Naive Bayes (NB), and linear discriminant analysis (LDA). The accuracy values of MLP, L-SVM and NB algorithms were 96.69%, 94.98% and 91.23%, respectively. Our results show that a home with AI could be safer and more secure against theft.",,
Application of Artificial Intelligence in the Health Care Safety Context: Opportunities and Challenges,Artificial intelligence can provide opportunities in diagnosis and treatment processes but may face safety challenges.,Search,2019,28,"Samer  Ellahham, Nour  Ellahham, Mecit Can Emre Simsekler",American journal of medical quality : the official journal of the American College of Medical Quality,,10.1177/1062860619878515,https://doi.org/10.1177/1062860619878515,https://semanticscholar.org/paper/a40761088d3afd1ad8fc698f813fe2c06d99c528,,"There is a growing awareness that artificial intelligence (AI) has been used in the analysis of complicated and big data to provide outputs without human input in various health care contexts, such as bioinformatics, genomics, and image analysis. Although this technology can provide opportunities in diagnosis and treatment processes, there still may be challenges and pitfalls related to various safety concerns. To shed light on such opportunities and challenges, this article reviews AI in health care along with its implication for safety. To provide safer technology through AI, this study shows that safe design, safety reserves, safe fail, and procedural safeguards are key strategies, whereas cost, risk, and uncertainty should be identified for all potential technical systems. It is also suggested that clear guidance and protocols should be identified and shared with all stakeholders to develop and adopt safer AI applications in the health care context.",,Review
"Safe and Sound: Artificial Intelligence in Hazardous Applications - John Fox, Subrata Das, AAAI Press, Menlo Park, CA, and MIT Press, Cambridge, MA/London, UK, 2000, 326 pp., References, Index, Illus., ISBN 0-262-06211-9",Artificial intelligence techniques can solve difficult problems in a more satisfactory way than conventional software.,Search,2003,2,Mar  Marcos,Artif. Intell. Medicine,,10.1016/S0933-3657(02)00082-9,https://doi.org/10.1016/S0933-3657(02)00082-9,https://semanticscholar.org/paper/0ab5618de22bfdb929e3a79248040d0476da2ea3,,"Since the inception of the Artificial Intelligence (AI) discipline, there have been plenty of developments of systems that can be labeled as intelligent. Intelligent systems have the potential of solving difficult problems in a more satisfactory way than conventional software would do, usually making use of heuristics or knowledge that the experts in the domain posses. As AI techniques improve, the range of tasks that intelligent systems are able to perform will grow. This will make them potentially suitable for a higher number of application fields. However, many fields are inherently safety-critical. If we want to put intelligent systems into use in those fields we should pay special attention to safety issues. In a scenario where an intelligent system operates in a complex environment, safety is mainly concerned with ensuring that the system responds adequately to any hazard that might arise, e.g. due to unexpected interactions of the actions it has performed. The book by John Fox and Subrata Das presents an approach to the improvement of safety in AI systems, combining ideas from software engineering, conventional system engineering and AI. It draws lessons from the extensive experience of the authors in the development of medical AI applications. Concerning the management of hazards, their proposal consists in using AI techniques similar to the ones employed for problem-solving. If the complexity of the environment makes it impossible to predict all possible hazards, as it is the case in Medicine, a promising alternative is to provide the system with additional capabilities to dynamically reason about hazards. As the title suggests, the book addresses the problem of making intelligent systems sound and safe. According to this, the two main topics treated in the book are, respectively, how to design intelligent systems that are able to perform complex, knowledge-intensive tasks, and how to make them safe. Both topics are illustrated with numerous examples from the field of Medicine. Nevertheless, the focus of attention is both on the practical achievements and the underlying theoretical framework. The book is structured in three parts. Part I introduces PROforma, a methodology and a technology for building intelligent systems. Part II discusses the safety issues that arise when deploying intelligent systems in complex and changing environments. Finally, Part III describes the theoretical concepts underlying the PROforma technology. A feature of this book is that there are different possible ways to read it, depending on the interest of the reader in the theoretical details of Part III. For those readers mainly interested in this part, a summary of the first two parts is Artificial Intelligence in Medicine 27 (2003) 103–106",,
Applications of Artificial Intelligence in Safe Human–Robot Interactions,"A self-organizing map (SOM) network can model, track, and predict human motions within a robot workspace.",Search,2011,38,"Nima  Najmaei, Mehrdad R. Kermani","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",,10.1109/TSMCB.2010.2058103,https://doi.org/10.1109/TSMCB.2010.2058103,https://semanticscholar.org/paper/0d41b7c8057df961ad1b0c00bf49d2a27425dec0,,"The integration of industrial robots into the human workspace presents a set of unique challenges. This paper introduces a new sensory system for modeling, tracking, and predicting human motions within a robot workspace. A reactive control scheme to modify a robot's operations for accommodating the presence of the human within the robot workspace is also presented. To this end, a special class of artificial neural networks, namely, self-organizing maps (SOMs), is employed for obtaining a superquadric-based model of the human. The SOM network receives information of the human's footprints from the sensory system and infers necessary data for rendering the human model. The model is then used in order to assess the danger of the robot operations based on the measured as well as predicted human motions. This is followed by the introduction of a new reactive control scheme that results in the least interferences between the human and robot operations. The approach enables the robot to foresee an upcoming danger and take preventive actions before the danger becomes imminent. Simulation and experimental results are presented in order to validate the effectiveness of the proposed method.",,
Safe AI—is this possible?☆,"The inherently self-learning, self-adaptive nature of artificial intelligence methods may require functionally deterministic controllers.",Search,1995,11,M. G. Rodd,,,10.1016/0952-1976(95)00010-X,https://doi.org/10.1016/0952-1976(95)00010-X,https://semanticscholar.org/paper/c53376e4e49ca954f4f4f2ca520b6042967f7dda,,"Abstract This deliberately provocative paper poses the question as to whether it is professionally acceptable to use AI-based control systems in real-time automation. The premise is that the increasing demands for improved control of industrial processes are resulting in a search for new control techniques which, it is suggested, will be based on various AI methodologies. However, in the face of the inherently self-learning, self-adaptive nature of such techniques, it is pointed out that to produce safe, predictable systems, the controllers should be functionally deterministic! In posing this dilemma, the paper suggests possible ways in which it may be resolved.",,
How Safe Is Artificial Intelligence?,Machine learning dramatically changes our civilization.,Search,2019,1,Klaus  Mainzer,Artificial intelligence - When do machines take over?,,10.1007/978-3-662-59717-0_11,https://doi.org/10.1007/978-3-662-59717-0_11,https://semanticscholar.org/paper/5493c1017f9c42565e7f01a170e12a99d128ea82,,"Machine learning dramatically changes our civilization. We rely more and more on efficient algorithms, because otherwise the complexity of our civilizing infrastructure would not be manageable: Our brains are too slow and hopelessly overwhelmed by the amount of data we have to deal with. But how secure are AI algorithms? In practical applications, learning algorithms refer to models of neural networks, which themselves are extremely complex. They are fed and trained with huge amounts of data. The number of necessary parameters explodes exponentially. Nobody knows exactly what happens in these “black boxes” in detail. A statistical trial-and-error procedure often remains. But how should questions of responsibility be decided in, e.g., autonomous driving or in medicine, if the methodological basics remain dark?",,
Towards Safe Artificial General Intelligence,"If humanity would find itself in conflict with a system of much greater intelligence than itself, then human society would likely lose.",Search,2018,21,Tom  Everitt,,,10.25911/5D134A2F8A7D3,https://doi.org/10.25911/5D134A2F8A7D3,https://semanticscholar.org/paper/8b1e149ae23ea7839c9e3a2bd063c354ff7075d0,,"The field of artificial intelligence has recently experienced a number of breakthroughs thanks to progress in deep learning and reinforcement learning. Computer algorithms now outperform humans at Go, Jeopardy, image classification, and lip reading, and are becoming very competent at driving cars and interpreting natural language. The rapid development has led many to conjecture that artificial intelligence with greater-thanhuman ability on a wide range of tasks may not be far. This in turn raises concerns whether we know how to control such systems, in case we were to successfully build them. Indeed, if humanity would find itself in conflict with a system of much greater intelligence than itself, then human society would likely lose. One way to make sure we avoid such a conflict is to ensure that any future AI system with potentially greater-thanhuman-intelligence has goals that are aligned with the goals of the rest of humanity. For example, it should not wish to kill humans or steal their resources. The main focus of this thesis will therefore be goal alignment, i.e. how to design artificially intelligent agents with goals coinciding with the goals of their designers. Focus will mainly be directed towards variants of reinforcement learning, as reinforcement learning currently seems to be the most promising path towards powerful artificial intelligence. We identify and categorize goal misalignment problems in reinforcement learning agents as designed today, and give examples of how these agents may cause catastrophes in the future. We also suggest a number of reasonably modest modifications that can be used to avoid or mitigate each identified misalignment problem. Finally, we also study various choices of decision algorithms, and conditions for when a powerful reinforcement learning system will permit us to shut it down. The central conclusion is that while reinforcement learning systems as designed today are inherently unsafe to scale to human levels of intelligence, there are ways to potentially address many of these issues without straying too far from the currently so successful reinforcement learning paradigm. Much work remains in turning the high-level proposals suggested in this thesis into practical algorithms, however. Central claim: There are a number of theoretically valid, partial solutions to the problem of keeping artificial general intelligence both safe and useful.",,