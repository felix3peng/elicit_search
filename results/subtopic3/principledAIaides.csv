Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI,There is a growing consensus around eight key thematic trends in AI principles documents.,Search,2020,137,"Jessica  Fjeld, Nele  Achten, Hannah  Hilligoss, Adam  Nagy, Madhulika  Srikumar",SSRN Electronic Journal,,10.2139/ssrn.3518482,https://doi.org/10.2139/ssrn.3518482,https://semanticscholar.org/paper/58bb24b72fea6d0ce172bdaf9c2f16c2bd7649e9,https://dash.harvard.edu/bitstream/1/42160420/1/HLS%20White%20Paper%20Final_v3.pdf,"The rapid spread of artificial intelligence (AI) systems has precipitated a rise in ethical and human rights-based frameworks intended to guide the development and use of these technologies. Despite the proliferation of these ""AI principles,"" there has been little scholarly focus on understanding these efforts either individually or as contextualized within an expanding universe of principles with discernible trends.

To that end, this white paper and its associated data visualization compare the contents of thirty-six prominent AI principles documents side-by-side. This effort uncovered a growing consensus around eight key thematic trends: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values. Underlying this “normative core,” our analysis examined the forty-seven individual principles that make up the themes, detailing notable similarities and differences in interpretation found across the documents. In sharing these observations, it is our hope that policymakers, advocates, scholars, and others working to maximize the benefits and minimize the harms of AI will be better positioned to build on existing efforts and to push the fractured, global conversation on the future of AI toward consensus.",,
Principles and business processes for responsible AI,"AI needs to be responsibly managed in order to protect its own interests, as well as those of its stakeholders.",Search,2019,25,Roger  Clarke,Comput. Law Secur. Rev.,,10.1016/J.CLSR.2019.04.007,https://doi.org/10.1016/J.CLSR.2019.04.007,https://semanticscholar.org/paper/9042677e67305726a4511ef3e9a7a00239af2a79,,"Abstract The first article in this series examined why the world wants controls over Artificial Intelligence (AI). This second article discusses how an organisation can manage AI responsibly, in order to protect its own interests, but also those of its stakeholders and society as a whole. A limited amount of guidance is provided by ethical analysis. A much more effective approach is to apply adapted forms of the established techniques of risk assessment and risk management. Critically, risk assessment needs to be undertaken not only with the organisation's own interests in focus, but also from the perspectives of other stakeholders. To underpin this new form of business process, a set of Principles for Responsible AI is presented, consolidating proposals put forward by a diverse collection of 30 organisations.",,
A Unified Framework of Five Principles for AI in Society,AI already has a major impact on society and is developing at an increasingly rapid pace.,Search,2019,152,"Luciano  Floridi, Josh  Cowls",Issue 1,,10.1162/99608F92.8CD550D1,https://doi.org/10.1162/99608F92.8CD550D1,https://semanticscholar.org/paper/8499e44d42b12518495069a54ae4400baccb7546,https://assets.pubpub.org/ukqte8ry/c8d3cba5-8f10-4a00-894c-3a3b886ad844.pdf,"Artificial Intelligence (AI) is already having a major impact on society. As a result, many organizations have launched a wide range of initiatives to establish ethical principles for the adoption of socially beneficial AI. Unfortunately, the sheer volume of proposed principles threatens to overwhelm and confuse. How might this problem of ‘principle proliferation’ be solved? In this paper, we report the results of a fine-grained analysis of several of the highest-profile sets of ethical principles for AI. We assess whether these principles converge upon a set of agreed-upon principles, or diverge, with significant disagreement over what constitutes ‘ethical AI.’ Our analysis finds a high degree of overlap among the sets of principles we analyze. We then identify an overarching framework consisting of five core principles for ethical AI. Four of them are core principles commonly used in bioethics: beneficence, non-maleficence, autonomy, and justice. On the basis of our comparative analysis, we argue that a new principle is needed in addition: explicability, understood as incorporating both the epistemological sense of intelligibility (as an answer to the question ‘how does it work?’) and in the ethical sense of accountability (as an answer to the question: ‘who is responsible for the way it works?’). In the ensuing discussion, we note the limitations and assess the implications of this ethical framework for future efforts to create laws, rules, technical standards, and best practices for ethical AI in a wide range of contexts.KeywordsAccountability; Autonomy; Artificial Intelligence; Beneficence; Ethics; Explicability; Fairness; Intelligibility; Justice; Non-maleficence.",,
Principles alone cannot guarantee ethical AI,AI ethics initiatives have seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics.,Search,2019,162,Brent  Mittelstadt,Nat. Mach. Intell.,,10.1038/s42256-019-0114-4,https://doi.org/10.1038/s42256-019-0114-4,https://semanticscholar.org/paper/2014d6036276b470d010060c86cedd4c92e0958c,http://arxiv.org/pdf/1906.06668,"Artificial intelligence (AI) ethics is now a global topic of discussion in academic and policy circles. At least 84 public–private initiatives have produced statements describing high-level principles, values and other tenets to guide the ethical development, deployment and governance of AI. According to recent meta-analyses, AI ethics has seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics. Despite the initial credibility granted to a principled approach to AI ethics by the connection to principles in medical ethics, there are reasons to be concerned about its future impact on AI development and governance. Significant differences exist between medicine and AI development that suggest a principled approach for the latter may not enjoy success comparable to the former. Compared to medicine, AI development lacks (1) common aims and fiduciary duties, (2) professional history and norms, (3) proven methods to translate principles into practice, and (4) robust legal and professional accountability mechanisms. These differences suggest we should not yet celebrate consensus around high-level principles that hide deep political and normative disagreement.AI ethics initiatives have seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics. Despite this, Brent Mittelstadt highlights important differences between medical practice and AI development that suggest a principled approach may not work in the case of AI.",,
Actionable Principles for Artificial Intelligence Policy: Three Pathways,AI Ethics Principles are often not actioned in governmental policy.,Search,2021,9,Charlotte  Stix,Sci. Eng. Ethics,,10.1007/s11948-020-00277-3,https://doi.org/10.1007/s11948-020-00277-3,https://semanticscholar.org/paper/16dc267a80a87e81e6a8ec4ed8c5a020657dd7da,https://link.springer.com/content/pdf/10.1007/s11948-020-00277-3.pdf,"In the development of governmental policy for artificial intelligence (AI) that is informed by ethics, one avenue currently pursued is that of drawing on “AI Ethics Principles”. However, these AI Ethics Principles often fail to be actioned in governmental policy. This paper proposes a novel framework for the development of ‘Actionable Principles for AI’. The approach acknowledges the relevance of AI Ethics Principles and homes in on methodological elements to increase their practical implementability in policy processes. As a case study, elements are extracted from the development process of the Ethics Guidelines for Trustworthy AI of the European Commission’s “High Level Expert Group on AI”. Subsequently, these elements are expanded on and evaluated in light of their ability to contribute to a prototype framework for the development of 'Actionable Principles for AI'. The paper proposes the following three propositions for the formation of such a prototype framework: (1) preliminary landscape assessments; (2) multi-stakeholder participation and cross-sectoral feedback; and, (3) mechanisms to support implementation and operationalizability.",,
Principles Alone Cannot Guarantee Ethical AI,AI ethics initiatives have seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics.,Search,2019,63,Brent  Mittelstadt,,,10.2139/ssrn.3391293,https://doi.org/10.2139/ssrn.3391293,https://semanticscholar.org/paper/ddc2d6d51b2f00a70320e293473035b993192949,,"Artificial intelligence (AI) ethics is now a global topic of discussion in academic and policy circles. At least 84 public–private initiatives have produced statements describing high-level principles, values and other tenets to guide the ethical development, deployment and governance of AI. According to recent meta-analyses, AI ethics has seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics. Despite the initial credibility granted to a principled approach to AI ethics by the connection to principles in medical ethics, there are reasons to be concerned about its future impact on AI development and governance. Significant differences exist between medicine and AI development that suggest a principled approach for the latter may not enjoy success comparable to the former. Compared to medicine, AI development lacks (1) common aims and fiduciary duties, (2) professional history and norms, (3) proven methods to translate principles into practice, and (4) robust legal and professional accountability mechanisms. These differences suggest we should not yet celebrate consensus around high-level principles that hide deep political and normative disagreement. AI ethics initiatives have seemingly converged on a set of principles that closely resemble the four classic principles of medical ethics. Despite this, Brent Mittelstadt highlights important differences between medical practice and AI development that suggest a principled approach may not work in the case of AI.",,
A Misdirected Principle with a Catch: Explicability for AI,The property of requiring explicability should attach to a particular action or decision rather than the entity making that decision.,Search,2019,38,Scott  Robbins,Minds and Machines,,10.1007/s11023-019-09509-3,https://doi.org/10.1007/s11023-019-09509-3,https://semanticscholar.org/paper/d83b033956e352559778c42b8997408fc8979e91,https://link.springer.com/content/pdf/10.1007/s11023-019-09509-3.pdf,"There is widespread agreement that there should be a principle requiring that artificial intelligence (AI) be ‘explicable’. Microsoft, Google, the World Economic Forum, the draft AI ethics guidelines for the EU commission, etc. all include a principle for AI that falls under the umbrella of ‘explicability’. Roughly, the principle states that “for AI to promote and not constrain human autonomy, our ‘decision about who should decide’ must be informed by knowledge of how AI would act instead of us” (Floridi et al. in Minds Mach 28(4):689–707, 2018). There is a strong intuition that if an algorithm decides, for example, whether to give someone a loan, then that algorithm should be explicable. I argue here, however, that such a principle is misdirected. The property of requiring explicability should attach to a particular action or decision rather than the entity making that decision. It is the context and the potential harm resulting from decisions that drive the moral need for explicability—not the process by which decisions are reached. Related to this is the fact that AI is used for many low-risk purposes for which it would be unnecessary to require that it be explicable. A principle requiring explicability would prevent us from reaping the benefits of AI used in these situations. Finally, the explanations given by explicable AI are only fruitful if we already know which considerations are acceptable for the decision at hand. If we already have these considerations, then there is no need to use contemporary AI algorithms because standard automation would be available. In other words, a principle of explicability for AI makes the use of AI redundant.",,
Toward an Ethics of AI Assistants: an Initial Framework,Personal AI assistants are now nearly ubiquitous and present ethical concerns.,Search,2018,28,John  Danaher,Philosophy & Technology,,10.1007/S13347-018-0317-3,https://doi.org/10.1007/S13347-018-0317-3,https://semanticscholar.org/paper/0eec7dad1f60c6802b88ba7637ab84c567e97c8d,,"Personal AI assistants are now nearly ubiquitous. Every leading smartphone operating system comes with a personal AI assistant that promises to help you with basic cognitive tasks: searching, planning, messaging, scheduling and so on. Usage of such devices is effectively a form of algorithmic outsourcing: getting a smart algorithm to do something on your behalf. Many have expressed concerns about this algorithmic outsourcing. They claim that it is dehumanising, leads to cognitive degeneration, and robs us of our freedom and autonomy. Some people have a more subtle view, arguing that it is problematic in those cases where its use may degrade important interpersonal virtues. In this article, I assess these objections to the use of AI assistants. I will argue that the ethics of their use is complex. There are no quick fixes or knockdown objections to the practice, but there are some legitimate concerns. By carefully analysing and evaluating the objections that have been lodged to date, we can begin to articulate an ethics of personal AI use that navigates those concerns. In the process, we can locate some paradoxes in our thinking about outsourcing and technological dependence, and we can think more clearly about what it means to live a good life in the age of smart machines.",,
Ethical Principles for Artificial Intelligence in National Defence,Efforts to develop or acquire artificial intelligence capabilities for defense are growing on a global scale.,Search,2021,,"Mariarosaria  Taddeo, David  McNeish, Alexander  Blanchard, Elizabeth  Edgar",Philosophy & Technology,,10.1007/s13347-021-00482-3,https://doi.org/10.1007/s13347-021-00482-3,https://semanticscholar.org/paper/5a1498414962642198712787a0708ad10e1b875f,https://link.springer.com/content/pdf/10.1007/s13347-021-00482-3.pdf,"Defence agencies across the globe identify artificial intelligence (AI) as a key technology to maintain an edge over adversaries. As a result, efforts to develop or acquire AI capabilities for defence are growing on a global scale. Unfortunately, they remain unmatched by efforts to define ethical frameworks to guide the use of AI in the defence domain. This article provides one such framework. It identifies five principles—justified and overridable uses, just and transparent systems and processes, human moral responsibility, meaningful human control and reliable AI systems—and related recommendations to foster ethically sound uses of AI for national defence purposes.",,
"Artificial Intelligence, Values and Alignment",The central challenge for theorists is to identify fair principles for AI alignment that are reflective endorsements despite widespread variation in moral beliefs.,Search,2020,53,Iason  Gabriel,Minds Mach.,,10.1007/S11023-020-09539-2,https://doi.org/10.1007/S11023-020-09539-2,https://semanticscholar.org/paper/7aa70e2c12c8ba2dcc828893adb8bb56e3766726,https://link.springer.com/content/pdf/10.1007/s11023-020-09539-2.pdf,"This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify 'true' moral principles for AI; rather, it is to identify fair principles for alignment, that receive reflective endorsement despite widespread variation in people's moral beliefs. The final part of the paper explores three ways in which fair principles for AI alignment could potentially be identified.",,
AI virtues - The missing link in putting AI ethics into practice,"Following widespread criticism, AI ethics underwent a practical turn but without deviating from the principled approach.",Search,2020,2,Thilo  Hagendorff,ArXiv,,,,https://semanticscholar.org/paper/883eea577a485ddf39697cce549e05234ef85827,,"Several seminal ethics initiatives have stipulated sets of principles and standards for good technology development in the AI sector. However, widespread criticism has pointed out a lack of practical realization of these principles. Following that, AI ethics underwent a practical turn, but without deviating from the principled approach and the many shortcomings associated with it. This paper proposes a different approach. It defines four basic AI virtues, namely justice, honesty, responsibility and care, all of which represent specific motivational settings that constitute the very precondition for ethical decision making in the AI field. Moreover, it defines two second-order AI virtues, prudence and fortitude, that bolster achieving the basic virtues by helping with overcoming bounded ethicality or the many hidden psychological forces that impair ethical decision making and that are hitherto completely disregarded in AI ethics. Lastly, the paper describes measures for successfully cultivating the mentioned virtues in organizations dealing with AI research and development.",,
AI in CAI : An artificial intelligence approach to computer-assisted instruction,SCHOLAR is a new and more powerful type of computer-assisted instruction using artificial intelligence techniques.,Search,1970,790,Jaime R. Carbonell,,,10.1109/TMMS.1970.299942,https://doi.org/10.1109/TMMS.1970.299942,https://semanticscholar.org/paper/f9441005143eac86fd045b194274cf8ea6b8169d,,"The main purpose of the research reported here is to show that a new and more powerful type of computer-assisted instruction (CAI), based on extensive application of artificial-intelligence (AI) techniques, is feasible, and to demonstrate some of its major capabilities. A set of computer programs was written and given the name SCHOLAR. Due to its complexity, only the conception and educational aspects of this system (including an actual on-line protocol) are presented in this paper. In what may be called conventional ad hoc-frame-oriented (AFO) CAI, the data base consists of many ""frames"" of specific pieces of text, questions, and anticipated answers entered in advance by the teacher. By contrast, an information-structure-oriented (ISO) CAI system is based on the utilization of an information network of facts, concepts, and procedures; it can generate text, questions, and corresponding answers. Because an ISO CAI system can also utilize its information network to answer questions formulated by the student, a mixed-initiative dialogue between student and computer is possible with questions and answers from both sides.",,
From principles to practice: How can we make AI ethics measurable?,Organizations that develop and deploy AI systems should implement precepts of ethical AI development.,Search,2021,,William  Warby,,,,,https://semanticscholar.org/paper/6629598bf1d16983bdffa9cade2216088b6e94d6,,"are omnipresent. A growing number of guidelines for the ethical development of socalled arti cial intelligence (AI) have been put forward by stakeholders from the private sector, civil society, and the scienti c and policymaking spheres. The Bertelsmann Stiftung’s Algo.Rules are among this body of proposals. However, it remains unclear how organizations that develop and deploy AI systems should implement precepts of this kind. In cooperation with the nonpro t VDE standardssetting organization, we are seeking to bridge this gap with a new working paper that demonstrates how AI ethics principles can be put into practice.",,
From the ground truth up: doing AI ethics from practice to principles,AI ethics is currently unbalanced toward theoretical principles and will benefit from increased exposure to grounded practices and dilemmas.,Search,2022,,James  Brusseau,AI & SOCIETY,,10.1007/s00146-021-01336-4,https://doi.org/10.1007/s00146-021-01336-4,https://semanticscholar.org/paper/d20db8aac79e84ac0e1cb753d69fb020aa7b13a3,http://arxiv.org/pdf/2201.01659,"Recent AI ethics has focused on applying abstract principles downward to practice. This paper moves in the other direction. Ethical insights are generated from the lived experiences of AI-designers working on tangible human problems, and then cycled upward to influence theoretical debates surrounding these questions: (1) Should AI as trustworthy be sought through explainability, or accurate performance? (2) Should AI be considered trustworthy at all, or is reliability a preferable aim? (3) Should AI ethics be oriented toward establishing protections for users, or toward catalyzing innovation? Specific answers are less significant than the larger demonstration that AI ethics is currently unbalanced toward theoretical principles, and will benefit from increased exposure to grounded practices and dilemmas.",,
"AI4People—An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations",AI should be developed and adopted under ethical principles.,Search,2018,472,"Luciano  Floridi, Josh  Cowls, Monica  Beltrametti, Raja  Chatila, Patrice  Chazerand, Virginia  Dignum, Christoph  Luetge, Robert  Madelin, Ugo  Pagallo, Francesca  Rossi, Burkhard  Schafer, Peggy  Valcke, Effy  Vayena",Minds and Machines,,10.1007/s11023-018-9482-5,https://doi.org/10.1007/s11023-018-9482-5,https://semanticscholar.org/paper/6b73b3a0a808a0c69bf7a89da168448b3bbdc3e3,https://link.springer.com/content/pdf/10.1007/s11023-018-9482-5.pdf,"This article reports the findings of AI4People, an Atomium—EISMD initiative designed to lay the foundations for a “Good AI Society”. We introduce the core opportunities and risks of AI for society; present a synthesis of five ethical principles that should undergird its development and adoption; and offer 20 concrete recommendations—to assess, to develop, to incentivise, and to support good AI—which in some cases may be undertaken directly by national or supranational policy makers, while in others may be led by other stakeholders. If adopted, these recommendations would serve as a firm foundation for the establishment of a Good AI Society.",,
Building Ethically Bounded AI,AI agents should be aware and follow appropriate ethical principles and should thus exhibit properties such as fairness or other virtues.,Search,2019,31,"Francesca  Rossi, Nicholas  Mattei",AAAI,,10.1609/aaai.v33i01.33019785,https://doi.org/10.1609/aaai.v33i01.33019785,https://semanticscholar.org/paper/bd318e959236b0d33a7567b6d3afc8d5e92b8ea3,https://ojs.aaai.org/index.php/AAAI/article/download/5051/4924,"The more AI agents are deployed in scenarios with possibly unexpected situations, the more they need to be flexible, adaptive, and creative in achieving the goal we have given them. Thus, a certain level of freedom to choose the best path to the goal is inherent in making AI robust and flexible enough. At the same time, however, the pervasive deployment of AI in our life, whether AI is autonomous or collaborating with humans, raises several ethical challenges. AI agents should be aware and follow appropriate ethical principles and should thus exhibit properties such as fairness or other virtues. These ethical principles should define the boundaries of AI’s freedom and creativity. However, it is still a challenge to understand how to specify and reason with ethical boundaries in AI agents and how to combine them appropriately with subjective preferences and goal specifications. Some initial attempts employ either a data-driven examplebased approach for both, or a symbolic rule-based approach for both. We envision a modular approach where any AI technique can be used for any of these essential ingredients in decision making or decision support systems, paired with a contextual approach to define their combination and relative weight. In a world where neither humans nor AI systems work in isolation, but are tightly interconnected, e.g., the Internet of Things, we also envision a compositional approach to building ethically bounded AI, where the ethical properties of each component can be fruitfully exploited to derive those of the overall system. In this paper we define and motivate the notion of ethically-bounded AI, we describe two concrete examples, and we outline some outstanding challenges.",,