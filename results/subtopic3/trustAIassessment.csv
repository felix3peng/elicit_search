Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Opening the software engineering toolbox for the assessment of trustworthy AI,Software engineering and testing practices can be used for the assessment of trustworthy AI.,Search,2020,2,"Mohit Kumar Ahuja, Mohamed-Bachir  Belaid, Pierre  Bernab'e, Mathieu  Collet, Arnaud  Gotlieb, Chhagan  Lal, Dusica  Marijan, Sagar  Sen, Aizaz  Sharif, Helge  Spieker",NeHuAI@ECAI,,,,https://semanticscholar.org/paper/3b0ff6bd000e9c615d024614343f2c1cf12bf124,,"Trustworthiness is a central requirement for the acceptance and success of human-centered artificial intelligence (AI). To deem an AI system as trustworthy, it is crucial to assess its behaviour and characteristics against a gold standard of Trustworthy AI, consisting of guidelines, requirements, or only expectations. While AI systems are highly complex, their implementations are still based on software. The software engineering community has a long-established toolbox for the assessment of software systems, especially in the context of software testing. In this paper, we argue for the application of software engineering and testing practices for the assessment of trustworthy AI. We make the connection between the seven key requirements as defined by the European Commission's AI high-level expert group and established procedures from software engineering and raise questions for future work.",,
Can we trust AI? An empirical investigation of trust requirements and guide to successful AI adoption,"The factors that increase overall trust in AI are access to knowledge, transparency, explainability, certification, and self-imposed standards and guidelines.",Search,2021,2,"Patrick  Bedué, Albrecht  Fritzsche",,,10.1108/JEIM-06-2020-0233,https://doi.org/10.1108/JEIM-06-2020-0233,https://semanticscholar.org/paper/2590e2a851c6c9fc61029377c1e451afdb408004,,"Purpose Artificial intelligence (AI) fosters economic growth and opens up new directions for innovation. However, the diffusion of AI proceeds very slowly and falls behind, especially in comparison to other technologies. An important path leading to better adoption rates identified is trust-building. Particular requirements for trust and their relevance for AI adoption are currently insufficiently addressed.Design/methodology/approachTo close this gap, the authors follow a qualitative approach, drawing on the extended valence framework by assessing semi-structured interviews with experts from various companies.FindingsThe authors contribute to research by finding several subcategories for the three main trust dimensions ability, integrity and benevolence, thereby revealing fundamental differences for building trust in AI compared to more traditional technologies. In particular, the authors find access to knowledge, transparency, explainability, certification, as well as self-imposed standards and guidelines to be important factors that increase overall trust in AI.Originality/valueThe results show how the valence framework needs to be elaborated to become applicable to the AI context and provide further structural orientation to better understand AI adoption intentions. This may help decision-makers to identify further requirements or strategies to increase overall trust in their AI products, creating competitive and operational advantage.",,
How to Evaluate Trust in AI-Assisted Decision Making? A Survey of Empirical Methodologies,Empirically investigating trust in AI-assisted decision making is difficult due to a lack of standard protocols.,Search,2021,1,"Oleksandra  Vereschak, Gilles  Bailly, Baptiste  Caramiaux",Proc. ACM Hum. Comput. Interact.,,10.1145/3476068,https://doi.org/10.1145/3476068,https://semanticscholar.org/paper/3041b15b1f2f08426585440eb52d2ea3156287bb,https://hal.sorbonne-universite.fr/hal-03280969v2/file/Authors.pdf,"The spread of AI-embedded systems involved in human decision making makes studying human trust in these systems critical. However, empirically investigating trust is challenging. One reason is the lack of standard protocols to design trust experiments. In this paper, we present a survey of existing methods to empirically investigate trust in AI-assisted decision making and analyse the corpus along the constitutive elements of an experimental protocol. We find that the definition of trust is not commonly integrated in experimental protocols, which can lead to findings that are overclaimed or are hard to interpret and compare across studies. Drawing from empirical practices in social and cognitive studies on human-human trust, we provide practical guidelines to improve the methodology of studying Human-AI trust in decision-making contexts. In addition, we bring forward research opportunities of two types: one focusing on further investigation regarding trust methodologies and the other on factors that impact Human-AI trust.",,
"A Review of Trust in Artificial Intelligence: Challenges, Vulnerabilities and Future Directions","Artificial Intelligence (AI) can benefit society, but it is also fraught with risks.",Search,2021,4,"Steven  Lockey, Nicole M. Gillespie, Daniel  Holm, Ida Asadi Someh",HICSS,,10.24251/HICSS.2021.664,https://doi.org/10.24251/HICSS.2021.664,https://semanticscholar.org/paper/1f11e05c5484ff5a066eab37c6268ad7aceaac8d,http://scholarspace.manoa.hawaii.edu/bitstream/10125/71284/1/0534.pdf,"Artificial Intelligence (AI) can benefit society, but it is also fraught with risks. Societal adoption of AI is recognized to depend on stakeholder trust in AI, yet the literature on trust in AI is fragmented, and little is known about the vulnerabilities faced by different stakeholders, making it is difficult to draw on this evidence-base to inform practice and policy. We undertake a literature review to take stock of what is known about the antecedents of trust in AI, and organize our findings around five trust challenges unique to or exacerbated by AI. Further, we develop a concept matrix identifying the key vulnerabilities to stakeholders raised by each of the challenges, and propose a multi-stakeholder approach to future research.",,Review
Trust in Artificial Intelligence: Australian Insights,Australians have low trust in AI but generally “accept” or “tolerate” AI.,Search,2020,5,"Steve  Lockey, Nicole  Gillespie, Caitlin  Curtis",,,10.14264/b32f129,https://doi.org/10.14264/b32f129,https://semanticscholar.org/paper/6c2714f81bc323851052920d240bc21b98e2a9f8,https://espace.library.uq.edu.au/view/UQ:b32f129/LockeyGillespieCurtis_Trust_In_AI.pdf,"Artificial Intelligence (AI) is the cornerstone technology of the Fourth Industrial Revolution and is enabling rapid innovation with many potential benefits for Australian society (e.g. enhanced healthcare diagnostics, transportation optimisation) and business (e.g. enhanced efficiency and competitiveness). The COVID-19 pandemic has accelerated the uptake of advanced technology, and investment in AI continues to grow exponentially.AI also poses considerable risks and challenges to society which raises concerns about whether AI systems are worthy of trust. These concerns have been fuelled by high profile cases of AI use that were biased, discriminatory, manipulative, unlawful, or violated privacy or other human rights. Without public confidence that AI is being developed and used in an ethical and trustworthy manner, it will not be trusted and its full potential will not be realised. To echo the sentiment of Dr Alan Finkel AO, Australia’s Chief Scientist, acceptance of AI rests on “the essential foundation of trust”. Are we capable of extending our trust to AI? This national survey is the first to take a deep dive into answering this question and understanding community trust and expectations in relation to AI. To do this, we surveyed a nationally representative sample of over 2,500 Australian citizens in June to July 2020. Our findings provide important and timely research insights into the public’s trust and attitudes towards AI and lay out a pathway for strengthening trust and acceptance of AI systems.Key findings include:              - Trust is central to the acceptance of AI, and is influenced by four key drivers;              - Australians have low trust in AI systems but generally ‘accept’ or ‘tolerate’ AI;              - Australians expect AI to be regulated and carefully managed;              - Australians expect organisations to uphold the principles of trustworthy AI;              - Australians feel comfortable with some but not all uses of AI at work;              - Australians want to know more about AI but currently have low awareness and understanding of AI and its uses.We draw out the implications of the findings for government, business and NGOs and provide a roadmap to enhancing public trust in AI highlighting three key actions:              - Live up to Australian’s expectations of trustworthy AI              - Strengthen the regulatory framework for governing AI              - Strengthen Australia’s AI literacy",,
Trust in Artificial Intelligence,"AI is being used for scheduling appointments, powering smart homes, recognizing people’s faces in photos, making health diagnoses, providing investment advice, and several other things.",Search,2018,14,Arathi  Sethumadhavan,Ergonomics in Design: The Quarterly of Human Factors Applications,,10.1177/1064804618818592,https://doi.org/10.1177/1064804618818592,https://semanticscholar.org/paper/94a43052c729c8e7e3f28c50c76e46d09ec95f62,https://journals.sagepub.com/doi/pdf/10.1177/1064804618818592,"Trust is fundamental to creating a lasting relationship with another human being. In our daily lives, we encounter several situations where we place trust on other humans such as bus drivers, colleagues, and even strangers. Trust in human-human teams is initially founded on the predictability of the trustee, and as the relationship between the trustor and the trustee progresses, dependability or integrity replaces predictability as the basis of trust (Hoff & Bashir, 2015). As artificial intelligence (AI) gets smarter and smarter, it is becoming an integral part of human lives. For example, AI is being used for scheduling appointments, powering smart homes, recognizing people’s faces in photos, making health diagnoses, providing investment advice, and several other things. As with a humanhuman team, trust is a necessary ingredient for human-AI partnerships. However, trust in human-human teams progresses in the reverse order from human-human teams (e.g., Hoff & Bashir, 2015). This is because humans initially assume that AI is near perfect. Therefore, in the initial stages, faith forms the essential constituent of trust, and as the number of exchanges between the human and the AI increases, faith is replaced by dependability and predictability. Trust in AI is considered a twodimensional construct comprising trust and distrust, where trust is associated with feelings of calmness and security and distrust involves fear and worry (Lyons, Stokes, Eschleman, Alarcon, & Barelka, 2011). Unarguably, trust is a complex social process with a variety of factors determining the extent to which humans trust AI agents (Hoff & Bashir, 2015; Lee & See, 2004). Specifically, in order to calibrate the right level of trust in AI, consider dispositional (i.e., user characteristics such as age, culture, gender, and personality), internal (e.g., user characteristics such as workload, mood, self-confidence, and working memory capacity), environmental (e.g., task difficulty, perceived risks and benefits, organizational setting), and learned factors (e.g., reputation of the AI, reliability, consistency, type and timing of errors made by the AI, and users’ experience with similar agents). Design factors (e.g., appearance, ease of use, communication style, and transparency of the AI) also affect perceptions of trust. For example, anthrophomorphizing is an effective way of establishing a long-term social bond between humans and AI agents, which is driven by the neurotransmitter and hormone, oxytocin (e.g., de Visser et al., 2017). Further, anthropomorphic agents also resist breakdown in trust compared to their counterpart non-anthropomorphic agents, presumably because anthropomorphic agents remind users of humans who are forgiven more easily for being imperfect in comparison to machine-like agents. Interestingly, there is also evidence that humans tend to disclose more information to AI therapists than human therapists. Transparency of the AI also helps to calibrate the right level of trust by enabling users to develop accurate mental models of the AI underpinnings (e.g., Balfe, Sharples, & Wilson, 2018). 818592 ERGXXX10.1177/1064804618818592ergonomics in designergonomics in design research-article2018",,
Trustworthy artificial intelligence (AI) in education,"AI applications are still nascent but can accelerate personalised learning, support students with special needs.",Search,2020,8,"Stéphan  Vincent-Lancrin, Reyer van der Vlies",,,10.1787/a6c90fa9-en,https://doi.org/10.1787/a6c90fa9-en,https://semanticscholar.org/paper/42d1eacdc929c74f2c0ebfb342e4e8a94b0f099a,https://www.oecd-ilibrary.org/deliver/a6c90fa9-en.pdf?itemId=%2Fcontent%2Fpaper%2Fa6c90fa9-en&mimeType=pdf,"This paper was written to support the G20 artificial intelligence (AI) dialogue. With the rise of artificial intelligence (AI), education faces two challenges: reaping the benefits of AI to improve education processes, both in the classroom and at the system level; and preparing students for new skillsets for increasingly automated economies and societies. AI applications are often still nascent, but there are many examples of promising uses that foreshadow how AI might transform education. With regard to the classroom, this paper highlights how AI can accelerate personalised learning, the support of students with special needs. At the system level, promising uses include predictive analysis to reduce dropout, and assessing new skillsets. A new demand for complex skills that are less easy to automate (e.g. higher cognitive skills like creativity and critical thinking) is also the consequence of AI and digitalisation. Reaching the full potential of AI requires that stakeholders trust not only the technology, but also its use by humans. This raises new policy challenges around “trustworthy AI”, encompassing the privacy and security of data, but also possible wrongful uses of data leading to biases against individuals or groups.",,
Trusting artificial intelligence in cybersecurity is a double-edged sword,Trust in AI for cybersecurity is unwarranted and some form of control to ensure the deployment of ‘reliable AI’ for cybersecurity is necessary.,Search,2019,28,"Mariarosaria  Taddeo, Tom  McCutcheon, Luciano  Floridi",Nat. Mach. Intell.,,10.1038/s42256-019-0109-1,https://doi.org/10.1038/s42256-019-0109-1,https://semanticscholar.org/paper/a485c4c937a4b229455d611129ff4cd6a628f259,https://ora.ox.ac.uk/objects/uuid:8188813d-138e-409d-8686-d2d7d5fa0879/download_file?safe_filename=Trusting%2BAI%2Bin%2BCybersecurity.pdf&file_format=application%2Fpdf&type_of_work=Journal+article,"Applications of artificial intelligence (AI) for cybersecurity tasks are attracting greater attention from the private and the public sectors. Estimates indicate that the market for AI in cybersecurity will grow from US$1 billion in 2016 to a US$34.8 billion net worth by 2025. The latest national cybersecurity and defence strategies of several governments explicitly mention AI capabilities. At the same time, initiatives to define new standards and certification procedures to elicit users’ trust in AI are emerging on a global scale. However, trust in AI (both machine learning and neural networks) to deliver cybersecurity tasks is a double-edged sword: it can improve substantially cybersecurity practices, but can also facilitate new forms of attacks to the AI applications themselves, which may pose severe security threats. We argue that trust in AI for cybersecurity is unwarranted and that, to reduce security risks, some form of control to ensure the deployment of ‘reliable AI’ for cybersecurity is necessary. To this end, we offer three recommendations focusing on the design, development and deployment of AI for cybersecurity.Current national cybersecurity and defence strategies of several governments mention explicitly the use of AI. However, it will be important to develop standards and certification procedures, which involves continuous monitoring and assessment of threats. The focus should be on the reliability of AI-based systems, rather than on eliciting users’ trust in AI.",,
Exploring the Assessment List for Trustworthy AI in the Context of Advanced Driver-Assistance Systems,The European Commission appointed experts to a High-Level Expert Group on Artificial Intelligence (AI-HLEG).,Search,2021,,"Markus  Borg, Joshua  Bronson, Linus  Christensson, Fredrik  Olsson, Olof  Lennartsson, Elias  Sonnsjö, Hamid  Ebabi, Martin  Karsberg",2021 IEEE/ACM 2nd International Workshop on Ethics in Software Engineering Research and Practice (SEthics),,10.1109/SEthics52569.2021.00009,https://doi.org/10.1109/SEthics52569.2021.00009,https://semanticscholar.org/paper/3063caad889735cd1279054c09a8cd3befea9848,http://arxiv.org/pdf/2103.09051,"Artificial Intelligence (AI) is increasingly used in critical applications. Thus, the need for dependable AI systems is rapidly growing. In 2018, the European Commission appointed experts to a High-Level Expert Group on AI (AI-HLEG). AI- HLEG defined Trustworthy AI as 1) lawful, 2) ethical, and 3) robust and specified seven corresponding key requirements. To help development organizations, AI-HLEG recently published the Assessment List for Trustworthy AI (ALTAI). We present an illustrative case study from applying ALTAI to an ongoing development project of an Advanced Driver-Assistance System (ADAS) that relies on Machine Learning (ML). Our experience shows that ALTAI is largely applicable to ADAS development, but specific parts related to human agency and transparency can be disregarded. Moreover, bigger questions related to societal and environmental impact cannot be tackled by an ADAS supplier in isolation. We present how we plan to develop the ADAS to ensure ALTAI-compliance. Finally, we provide three recommendations for the next revision of ALTAI, i.e., life-cycle variants, domainspecific adaptations, and removed redundancy.",,
Interdisciplinary Remarks on the Assessment List on Trustworthy AI (ALTAI) Applied to a P5 Medicine Tool,The self-assessment tool developed for the AI-based melanoma cancer diagnosis may have mandatory application in R&D.,Search,2021,,"Denise  Amram, Arianna  Cignoni, Tommaso  Banfi, Gastone  Ciuti",,,10.35248/2155-9627.21.12.374,https://doi.org/10.35248/2155-9627.21.12.374,https://semanticscholar.org/paper/ad0d5bd09ad965217738b1497612d2edcaeff870,,"The authors apply the Assessment List on Trustworthy AI (ALTAI) to a possible AI-based tool aiming at supporting

the melanoma cancer diagnosis. They take the opportunity to provide an interdisciplinary analysis of the proposed selfassessment

tool in light of its possible mandatory application in R&D&I. The presented empirical exercise highlights

some pros and cons of the adopted checklist, stimulating further remarks on the EU regulatory initiatives on AI.

Finally, we try to understand the improvement of procedures, medical knowledge and treatment collected and improved

during these months, that allowed for a lower mortality rate in the referring period.",,
Trust in Artificial Intelligence: Meta-Analytic Findings.,Factors such as AI reliability and anthropomorphism are significant predictors of trust in AI.,Search,2021,1,"Alexandra D Kaplan, Theresa T Kessler, J Christopher Brill, P A Hancock",Human factors,,10.1177/00187208211013988,https://doi.org/10.1177/00187208211013988,https://semanticscholar.org/paper/4ad12e5c49202187f2a85351d417f81dc86b8524,,"OBJECTIVE

The present meta-analysis sought to determine significant factors that predict trust in artificial intelligence (AI). Such factors were divided into those relating to (a) the human trustor, (b) the AI trustee, and (c) the shared context of their interaction.

BACKGROUND

There are many factors influencing trust in robots, automation, and technology in general, and there have been several meta-analytic attempts to understand the antecedents of trust in these areas. However, no targeted meta-analysis has been performed examining the antecedents of trust in AI.

METHOD

Data from 65 articles examined the three predicted categories, as well as the subcategories of human characteristics and abilities, AI performance and attributes, and contextual tasking. Lastly, four common uses for AI (i.e., chatbots, robots, automated vehicles, and nonembodied, plain algorithms) were examined as further potential moderating factors.

RESULTS

Results showed that all of the examined categories were significant predictors of trust in AI as well as many individual antecedents such as AI reliability and anthropomorphism, among many others.

CONCLUSION

Overall, the results of this meta-analysis determined several factors that influence trust, including some that have no bearing on AI performance. Additionally, we highlight the areas where there is currently no empirical research.

APPLICATION

Findings from this analysis will allow designers to build systems that elicit higher or lower levels of trust, as they require.",,Meta-Analysis
Trust in artificial intelligence for medical diagnoses.,People have comparable standards of performance for AI and human doctors and that trust in AI does not increase when people are told the AI outperforms the human doctor.,Search,2020,5,"Georgiana  Juravle, Andriana  Boudouraki, Miglena  Terziyska, Constantin  Rezlescu",Progress in brain research,,10.1016/bs.pbr.2020.06.006,https://doi.org/10.1016/bs.pbr.2020.06.006,https://semanticscholar.org/paper/06f0bb0b40507020a9866dc8f35f5a26700b33af,,"We present two online experiments investigating trust in artificial intelligence (AI) as a primary and secondary medical diagnosis tool and one experiment testing two methods to increase trust in AI. Participants in Experiment 1 read hypothetical scenarios of low and high-risk diseases, followed by two sequential diagnoses, and estimated their trust in the medical findings. In three between-participants groups, the first and second diagnoses were given by: human and AI, AI and human, and human and human doctors, respectively. In Experiment 2 we examined if people expected higher standards of performance from AI than human doctors, in order to trust AI treatment recommendations. In Experiment 3 we investigated the possibility to increase trust in AI diagnoses by: (i) informing our participants that the AI outperforms the human doctor, and (ii) nudging them to prefer AI diagnoses in a choice between AI and human doctors. Results indicate overall lower trust in AI, as well as for diagnoses of high-risk diseases. Participants trusted AI doctors less than humans for first diagnoses, and they were also less likely to trust a second opinion from an AI doctor for high risk diseases. Surprisingly, results highlight that people have comparable standards of performance for AI and human doctors and that trust in AI does not increase when people are told the AI outperforms the human doctor. Importantly, we find that the gap in trust between AI and human diagnoses is eliminated when people are nudged to select AI in a free-choice paradigm between human and AI diagnoses, with trust for AI diagnoses significantly increased when participants could choose their doctor. These findings isolate control over one's medical practitioner as a valid candidate for future trust-related medical diagnosis and highlight a solid potential path to smooth acceptance of AI diagnoses amongst patients.",,
Trust in Distributed Artificial Intelligence,Trust allows interactions between agents where there may have been no effective interaction possible before trust.,Search,1992,90,Stephen  Marsh,MAAMAW,,10.1007/3-540-58266-5_6,https://doi.org/10.1007/3-540-58266-5_6,https://semanticscholar.org/paper/6d296cd0ddaccb6d01aa197c9ba5a04afee2d399,,"A discussion of trust is presented which focuses on multiagent systems, from the point of view of one agent in a system. The roles trust plays in various forms of interaction are considered, with the view that trust allows interactions between agents where there may have been no effective interaction possible before trust. Trust allows parties to acknowledge that, whilst there is a risk in relationships with potentially malevolent agents, some form of interaction may produce benefits, where no interaction at all may not. In addition, accepting the risk allows the trusting agent to prepare itself for possibly irresponsible or untrustworthy behaviour, thus minimizing the potential damage caused. A formalism is introduced to clarify these notions, and to permit computer simulations. An important contribution of this work is that the formalism is not allen-compassing: there are some notions of trust that are excluded. What it describes is a specific view of trust.",,
Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims,"Ten mechanisms can provide evidence about the safety, security, fairness, and privacy protection of AI systems.",Search,2020,90,"Miles  Brundage, Shahar  Avin, Jasmine  Wang, Haydn  Belfield, Gretchen  Krueger, Gillian  Hadfield, Heidy  Khlaaf, Jingying  Yang, Helen  Toner, Ruth  Fong, Tegan  Maharaj, Pang Wei Koh, Sara  Hooker, Jade  Leung, Andrew  Trask, Emma  Bluemke, Jonathan  Lebensbold, Cullen  O'Keefe, Mark  Koren, Th'eo  Ryffel, JB  Rubinovitz, Tamay  Besiroglu, Federica  Carugati, Jack  Clark, Peter  Eckersley, Sarah de Haas, Maritza  Johnson, Ben  Laurie, Alex  Ingerman, Igor  Krawczuk, Amanda  Askell, Rosario  Cammarota, Andrew  Lohn, David  Krueger, Charlotte  Stix, Peter  Henderson, Logan  Graham, Carina  Prunkl, Bianca  Martin, Elizabeth  Seger, Noa  Zilberman, Se'an 'O h'Eigeartaigh, Frens  Kroeger, Girish  Sastry, Rebecca  Kagan, Adrian  Weller, Brian  Tse, Elizabeth  Barnes, Allan  Dafoe, Paul  Scharre, Ariel  Herbert-Voss, Martijn  Rasser, Shagun  Sodhani, Carrick  Flynn, Thomas Krendl Gilbert, Lisa  Dyer, Saif  Khan, Yoshua  Bengio, Markus  Anderljung",ArXiv,,,,https://semanticscholar.org/paper/62c3142956d54db158d190ce691e3c13e7897412,,"With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.",,
Requirements for Trustworthy Artificial Intelligence - A Review,"The field of algorithmic decision-making, particularly Artificial Intelligence (AI), has been drastically changing.",Search,2020,8,"Davinder  Kaur, Suleyman  Uslu, Arjan  Durresi",NBiS,,10.1007/978-3-030-57811-4_11,https://doi.org/10.1007/978-3-030-57811-4_11,https://semanticscholar.org/paper/ab7f6628cbbafae1ce994929af2482efbd092d61,https://scholarworks.iupui.edu/bitstream/1805/28055/1/Kaur2020Requirements-AAM.pdf,"The field of algorithmic decision-making, particularly Artificial Intelligence (AI), has been drastically changing. With the availability of a massive amount of data and an increase in the processing power, AI systems have been used in a vast number of high-stake applications. So, it becomes vital to make these systems reliable and trustworthy. Different approaches have been proposed to make theses systems trustworthy. In this paper, we have reviewed these approaches and summarized them based on the principles proposed by the European Union for trustworthy AI. This review provides an overview of different principles that are important to make AI trustworthy.",,Review
The relationship between trust in AI and trustworthy machine learning technologies,Trust can be impacted throughout the life cycle of AI-based systems.,Search,2020,50,"Ehsan  Toreini, Mhairi  Aitken, Kovila  Coopamootoo, Karen  Elliott, Carlos Gonzalez Zelaya, Aad van Moorsel",FAT*,,10.1145/3351095.3372834,https://doi.org/10.1145/3351095.3372834,https://semanticscholar.org/paper/bd4cf5a6987ef0f39b7e4e88d59b3a3365abcc70,http://arxiv.org/pdf/1912.00782,"To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.",,