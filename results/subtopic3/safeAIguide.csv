Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Understanding artificial intelligence ethics and safety,"The UK public sector can prevent harmful AI by putting in place governance processes that support the design and implementation of ethical, fair, and safe AI systems.",Search,2019,37,David  Leslie,ArXiv,,10.5281/zenodo.3240529,https://doi.org/10.5281/zenodo.3240529,https://semanticscholar.org/paper/adeb97576107e9f8e1140302cf614d1da709d523,,"A remarkable time of human promise has been ushered in by the convergence of the ever-expanding availability of big data, the soaring speed and stretch of cloud computing platforms, and the advancement of increasingly sophisticated machine learning algorithms. Innovations in AI are already leaving a mark on government by improving the provision of essential social goods and services from healthcare, education, and transportation to food supply, energy, and environmental management. These bounties are likely just the start. The prospect that progress in AI will help government to confront some of its most urgent challenges is exciting, but legitimate worries abound. As with any new and rapidly evolving technology, a steep learning curve means that mistakes and miscalculations will be made and that both unanticipated and harmful impacts will occur.

This guide, written for department and delivery leads in the UK public sector and adopted by the British Government in its publication, 'Using AI in the Public Sector,' identifies the potential harms caused by AI systems and proposes concrete, operationalisable measures to counteract them. It stresses that public sector organisations can anticipate and prevent these potential harms by stewarding a culture of responsible innovation and by putting in place governance processes that support the design and implementation of ethical, fair, and safe AI systems. It also highlights the need for algorithmically supported outcomes to be interpretable by their users and made understandable to decision subjects in clear, non-technical, and accessible ways. Finally, it builds out a vision of human-centred and context-sensitive implementation that gives a central role to communication, evidence-based reasoning, situational awareness, and moral justifiability.",,
Safe and sound - artificial intelligence in hazardous applications,Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making.,Search,2000,272,"John  Fox, Subrata Kumar Das",,,,,https://semanticscholar.org/paper/f62d215d4c40d299929d2e7fa105d54623a9e1bf,,"Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making, where small faults or errors can spell human catastrophe. This book describes, from both practical and theoretical perspectives, an AI technology for supporting sound clinical decision making and safe patient management. The technology combines techniques from conventional software engineering with a systematic method for building intelligent agents. Although the focus is on medicine, many of the ideas can be applied to AI systems in other hazardous settings. The book also covers a number of general AI problems, including knowledge representation and expertise modeling, reasoning and decision making under uncertainty, planning and scheduling, and the design and implementation of intelligent agents.The book, written in an informal style, begins with the medical background and motivations, technical challenges, and proposed solutions. It then turns to a wide-ranging discussion of intelligent and autonomous agents, with particular reference to safety and hazard management. The final section provides a detailed discussion of the knowledge representation and other aspects of the agent model developed in the book, along with a formal logical semantics for the language.",,
Establishing the rules for building trustworthy AI,The European Commission’s report ‘Ethics guidelines for trustworthy AI’ facilitates international support for AI solutions that are good for the environment.,Search,2019,67,Luciano  Floridi,Nature Machine Intelligence,,10.1038/S42256-019-0055-Y,https://doi.org/10.1038/S42256-019-0055-Y,https://semanticscholar.org/paper/dc44e2be0f85b6225f05390c570885337a99ef83,https://philpapers.org/archive/FLOETR.pdf,"The European Commission’s report ‘Ethics guidelines for trustworthy AI’ provides a clear benchmark to evaluate the responsible development of AI systems, and facilitates international support for AI solutions that are good for humanity and the environment, says Luciano Floridi.",,
Safe AI for CPS (Invited Paper),Combining formal proofs with reinforcement learning can develop safe AI for cyber-physical systems.,Search,2018,5,"Nathan  Fulton, André  Platzer",2018 IEEE International Test Conference (ITC),,10.1109/TEST.2018.8624774,https://doi.org/10.1109/TEST.2018.8624774,https://semanticscholar.org/paper/03fce03060f6110b2772958f58539320bab519ed,,"Autonomous cyber-physical systems-such as self-driving cars and autonomous drones-often leverage artificial intelligence and machine learning algorithms to act well in open environments. Although testing plays an important role in ensuring safety and robustness, modern autonomous systems have grown so complex that achieving safety via testing alone is intractable. Formal verification reduces this testing burden by ruling out large classes of errant behavior at design time. This paper reviews recent work toward developing formal methods for cyber-physical systems that use AI for planning and control by combining the rigor of formal proofs with the flexibility of reinforcement learning.",,Review
Safe Artificial Intelligence and Formal Methods - (Position Paper),Formal methods can assist in building safer and reliable AI.,Search,2016,9,Emil  Vassev,ISoLA,,10.1007/978-3-319-47166-2_49,https://doi.org/10.1007/978-3-319-47166-2_49,https://semanticscholar.org/paper/796dcb12b96e5d0c2f358b1b67fdfba36d023acc,https://ulir.ul.ie/bitstream/10344/5407/2/Vassev_2016_safe.pdf,"In one aspect of our life or another, today we all live with AI. For example, the mechanisms behind the search engines operating on the Internet do not just retrieve information, but also constantly learn how to respond more rapidly and usefully to our requests. Although framed by its human inventors, this AI is getting stronger and more powerful every day to go beyond the original human intentions in the future. One of the major questions emerging along with the propagation of AI in both technology and life is about safety in AI. This paper presents the author’s view about how formal methods can assist us in building safer and reliable AI.",,
AI Safety Gridworlds,A2C and Rainbow do not solve the AI safety problems well.,Search,2017,141,"Jan  Leike, Miljan  Martic, Victoria  Krakovna, Pedro A. Ortega, Tom  Everitt, Andrew  Lefrancq, Laurent  Orseau, Shane  Legg",ArXiv,,,,https://semanticscholar.org/paper/d09bec5af4eef5038e48b26b6c14098f95997114,,"We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.",,
"Visual book review 1 “ Safe and Sound , AI in hazardous applications",The critical component of the logic of argument (LA) is exposed in Axiom 8.,Search,2001,3,Boris  Kovalerchuk,,,,,https://semanticscholar.org/paper/41f895337f19cd042dd64b1942b63c48acc57128,,"A recent book “Safe and Sound, AI in hazardous applications” by John Fox and Subrata Das (AAAI Press/ The MIT Press, 2000, 293p., ISBN 0-262-06211-9) attracts attention of the research community and practitioners to the problem of safety of traditional and computer-aided medical diagnosis and treatment. The authors use medical applications as a focal point for the general safety problem through variety of hazardous applications. At first glance, the problem is already well known. However, they show that discovery and analysis of the sources of danger in hazardous applications are far from having rigorous solutions. The book uses an artificial intelligence (AI) approach, which allows one to express different types of statements in consistent logical fashion. Specifically the authors consider two main types of statements for making medical decisions: claims and their grounds. In addition, a confidence value is assigned to a claim using ground statements. For instance, the claim can be that Mr. P has a gastric ulcer and the grounds can be that Mr. P has pain after meals & ulcers are painful because of an increase in acidity. The word “support” can express the level of confidence. In the book, the safety issue for defining diagnosis and treatment is viewed as adding restrictions on inference. The goal of restrictions is to avoid dangerous actions for patients. This review should help a reader to see conditions for successful applications of logic of argument (LA), which is the central theme of the book. The book contains three parts (see Figure 1): Part 1: Method for building software agents, which produce Rigorously Engineered Decisions. Part 2: Technique for deploying agents in general and hazardous environment with medical examples. Part 3: Formal and logical aspect of the method. The spread of the central theme over the book is presented in Figure 1. Chapter 4 provides an informal description of LA, Chapters 13 and 15 contain formalisms and Chapter 15 implementation of LA in Prolog language. The critical component of LA is exposed in Axiom 8 (Chapter 13). This axiom is the central assumption of LA. It actually formalizes the requirement of independence of arguments used in LA in inferring diagnosis or treatment recommendation. The property is also known as truth-functionality in Artificial Intelligence literature [1] and was a subject of intensive discussions in AI community for years [1,2,3].",,Review
Safe AI Systems,The safe AI must satisfy with 2 majors applying the security measures and analyzing the system.,Search,2018,,Ananta  Bhatt,International Journal of Computer Applications,,10.5120/ijca2018918205,https://doi.org/10.5120/ijca2018918205,https://semanticscholar.org/paper/97aba8083bc5ad49af27a5c26c2bd40e6a17d6a3,,"As you know, it is inevitable to cease the advancement in AI because of the quest for developing systems that learns from experience and comply with the human capabilities. The leading ground-breaking impacts offers both positive and negative aspirations and are addressed as part of this research. To meet with the evil effects of the AI systems, is now the need of the hour to direct the focus on creating safe AI systems. The safe system must satisfy with 2 majorsapplying the security measures and analysing the system. Moreover, system must fulfil the risk and safety valuesintelligence, goals and safety in order to minimise the destruction that may occur due AI technology. However, further research is needed on the actual implementation of the safe AI systems in order to proceed with this research.",,
Safe and Ethical Artificial Intelligence in Radiotherapy - Lessons Learned From the Aviation Industry.,The Aletheia Framework from Rolls-Royce can guide organizations in developing safe AI systems.,Search,2021,1,"R  Hallows, L  Glazier, M S Katz, M  Aznar, M  Williams",Clinical oncology (Royal College of Radiologists (Great Britain)),,10.1016/j.clon.2021.11.019,https://doi.org/10.1016/j.clon.2021.11.019,https://semanticscholar.org/paper/518441aba1c5f08526564b51f7b516b9837ff964,,"Ethical artificial intelligence (AI) frameworks can be the catalyst in improving the safety and wellbeing of people when developing AI systems. In 2020 Rolls-Royce released its ethical and trustworthiness toolkit, The Aletheia FrameworkTM, which helps guide organisations as they consider the ethics around the use of AI. It covers three facets: social impact, accuracy and trust, and governance - which apply across all uses of AI. By adopting AI ethics and trust frameworks, oncologists can ensure the ratio between the benefit and harms of AI can be maximised. With AI transforming every sector, collaboration across industries to share ideas and learn from each other - even unlikely partnerships between engineering and oncology - could help optimise that transition.",,
Guidelines for Artificial Intelligence Containment,Safety container software can study and analyze intelligent artificial agents while maintaining safety levels.,Search,2019,20,"James  Babcock, János  Kramár, Roman V. Yampolskiy",Next-Generation Ethics,,10.1017/9781108616188.008,https://doi.org/10.1017/9781108616188.008,https://semanticscholar.org/paper/b0c820f9d7fa4d8c56a7887548834b7ba65ddfa5,http://www.vetta.org/documents/Machine_Super_Intelligence.pdf,"With almost daily improvements in capabilities of artificial intelligence it is more important than ever to develop safety software for use by the AI research community. Building on our previous work on AI Containment Problem we propose a number of guidelines which should help AI safety researchers to develop reliable sandboxing software for intelligent programs of all levels. Such safety container software will make it possible to study and analyze intelligent artificial agent while maintaining certain level of safety against information leakage, social engineering attacks and cyberattacks from within the container.",,
Governing AI safety through independent audits,Independent audit of AI systems is a pragmatic approach to a burdensome and unenforceable assurance challenge.,Search,2021,8,"Gregory  Falco, Ben  Shneiderman, Julia  Badger, Ryan  Carrier, A. T. Dahbura, David  Danks, Martin  Eling, Alwyn  Goodloe, Jerry  Gupta, Christopher  Hart, Marina  Jirotka, Henric  Johnson, Cara  LaPointe, Ashley J. Llorens, Alan K. Mackworth, Carsten  Maple, Sigurður Emil Pálsson, Frank A. Pasquale, Alan F. T. Winfield, Zee Kin Yeong",Nat. Mach. Intell.,,10.1038/S42256-021-00370-7,https://doi.org/10.1038/S42256-021-00370-7,https://semanticscholar.org/paper/93a04c8661ce96f9ab972a0ede4680232627467a,https://ora.ox.ac.uk/objects/uuid:900c78bf-4fe3-4afc-aaed-5a5fdffecbff/download_file?safe_filename=Falco_et_al_2021_Governing_AI_safety.pdf&file_format=pdf&type_of_work=Journal+article,"Highly automated systems are becoming omnipresent. They range in function from self-driving vehicles to advanced medical diagnostics and afford many benefits. However, there are assurance challenges that have become increasingly visible in high-profile crashes and incidents. Governance of such systems is critical to garner widespread public trust. Governance principles have been previously proposed offering aspirational guidance to automated system developers; however, their implementation is often impractical given the excessive costs and processes required to enact and then enforce the principles. This Perspective, authored by an international and multidisciplinary team across government organizations, industry and academia, proposes a mechanism to drive widespread assurance of highly automated systems: independent audit. As proposed, independent audit of AI systems would embody three ‘AAA’ governance principles of prospective risk Assessments, operation Audit trails and system Adherence to jurisdictional requirements. Independent audit of AI systems serves as a pragmatic approach to an otherwise burdensome and unenforceable assurance challenge. As highly automated systems become pervasive in society, enforceable governance principles are needed to ensure safe deployment. This Perspective proposes a pragmatic approach where independent audit of AI systems is central. The framework would embody three AAA governance principles: prospective risk Assessments, operation Audit trails and system Adherence to jurisdictional requirements.",,
Decision Support for Safe AI Design,Simulations do not have to be accurate predictions of the future to be useful for AI safety.,Search,2012,4,Bill  Hibbard,AGI,,10.1007/978-3-642-35506-6_13,https://doi.org/10.1007/978-3-642-35506-6_13,https://semanticscholar.org/paper/c40cad8cacaa47aac6d4cab8f80749beaebd690c,http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_57.pdf,"There is considerable interest in ethical designs for artificial intelligence (AI) that do not pose risks to humans. This paper proposes using elements of Hutter's agent-environment framework to define a decision support system for simulating, visualizing and analyzing AI designs to understand their consequences. The simulations do not have to be accurate predictions of the future; rather they show the futures that an agent design predicts will fulfill its motivations and that can be explored by AI designers to find risks to humans. In order to safely create a simulation model this paper shows that the most probable finite stochastic program to explain a finite history is finitely computable, and that there is an agent that makes such a computation without any unintended instrumental actions. It also discusses the risks of running an AI in a simulated environment.",,
Safe AI—is this possible?☆,"The inherently self-learning, self-adaptive nature of artificial intelligence methods may require functionally deterministic controllers.",Search,1995,11,M. G. Rodd,,,10.1016/0952-1976(95)00010-X,https://doi.org/10.1016/0952-1976(95)00010-X,https://semanticscholar.org/paper/c53376e4e49ca954f4f4f2ca520b6042967f7dda,,"Abstract This deliberately provocative paper poses the question as to whether it is professionally acceptable to use AI-based control systems in real-time automation. The premise is that the increasing demands for improved control of industrial processes are resulting in a search for new control techniques which, it is suggested, will be based on various AI methodologies. However, in the face of the inherently self-learning, self-adaptive nature of such techniques, it is pointed out that to produce safe, predictable systems, the controllers should be functionally deterministic! In posing this dilemma, the paper suggests possible ways in which it may be resolved.",,
Key Concepts in AI Safety: An Overview,“AI safety” is an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems.,Search,2021,,"Tim  Rudner, Helen  Toner",,,10.51593/20190040,https://doi.org/10.51593/20190040,https://semanticscholar.org/paper/f59ccf9ea5d7dd14ace4688503b8e15f3a2dd096,https://cset.georgetown.edu/wp-content/uploads/CSET-Key-Concepts-in-AI-Safety-An-Overview.pdf,"This paper is the first installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. In it, the authors introduce three categories of AI safety issues: problems of robustness, assurance, and specification. Other papers in this series elaborate on these and further key concepts.",,Review
A critical perspective on guidelines for responsible and trustworthy artificial intelligence,The ethical issues of AI technology vary from privacy and confidentiality of personal data to the ethical status and value of AI entities.,Search,2020,4,"Banu  Buruk, Perihan Elif Ekmekci, Berna  Arda","Medicine, health care, and philosophy",,10.1007/s11019-020-09948-1,https://doi.org/10.1007/s11019-020-09948-1,https://semanticscholar.org/paper/4990df10e6423ad5d44888697dd8bc523287fd14,,"Artificial intelligence (AI) is among the fastest developing areas of advanced technology in medicine. The most important qualia of AI which makes it different from other advanced technology products is its ability to improve its original program and decision-making algorithms via deep learning abilities. This difference is the reason that AI technology stands out from the ethical issues of other advanced technology artifacts. The ethical issues of AI technology vary from privacy and confidentiality of personal data to ethical status and value of AI entities in a wide spectrum, depending on their capability of deep learning and scope of the domains in which they operate. Developing ethical norms and guidelines for planning, development, production, and usage of AI technology has become an important issue to overcome these problems. In this respect three outstanding documents have been produced: 1. The Montréal Declaration for Responsible Development of Artificial Intelligence 2. Ethics Guidelines for Trustworthy AI 3. Asilomar Artificial Intelligence Principles In this study, these three documents will be analyzed with respect to the ethical principles and values they involve, their perspectives for approaching ethical issues, and their prospects for ethical reasoning when one or more of these values and principles are in conflict. Then, the sufficiency of these guidelines for addressing current or prospective ethical issues emerging from the existence of AI technology in medicine will be evaluated. The discussion will be pursued in terms of the ambiguity of interlocutors and efficiency for working out ethical dilemmas occurring in practical life.",,
AI safety: state of the field through quantitative lens,AI safety is the field under which we need to decide the direction of humanity’s future.,Search,2020,7,"Mislav  Juric, Agneza  Sandic, Mario  Brcic","2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO)",,10.23919/mipro48935.2020.9245153,https://doi.org/10.23919/mipro48935.2020.9245153,https://semanticscholar.org/paper/2e5ab5250e524801e2efd24249d75fcbb80f2b99,http://arxiv.org/pdf/2002.05671,"Last decade has seen major improvements in the performance of artificial intelligence which has driven wide-spread applications. Unforeseen effects of such massad-option has put the notion of AI safety into the public eye. AI safety is a relatively new field of research focused on techniques for building AI beneficial for humans. While there exist survey papers for the field of AI safety, there is a lack of a quantitative look at the research being conducted. The quantitative aspect gives a data-driven insight about the emerging trends, knowledge gaps and potential areas for future research. In this paper, bibliometric analysis of the literature finds significant increase in research activity since 2015. Also, the field is so new that most of the technical issues are open, including: explainability and its long-term utility, and value alignment which we have identified as the most important long-term research topic. Equally, there is a severe lack of research into concrete policies regarding AI. As we expect AI to be the one of the main driving forces of changes, AI safety is the field under which we need to decide the direction of humanity’s future.",,