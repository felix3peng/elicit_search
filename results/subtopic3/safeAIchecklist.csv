Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
AI Safety Gridworlds,A2C and Rainbow do not solve the AI safety problems on various environments satisfactorily.,Search,2017,141,"Jan  Leike, Miljan  Martic, Victoria  Krakovna, Pedro A. Ortega, Tom  Everitt, Andrew  Lefrancq, Laurent  Orseau, Shane  Legg",ArXiv,,,,https://semanticscholar.org/paper/d09bec5af4eef5038e48b26b6c14098f95997114,,"We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.",,
Safe AI for CPS (Invited Paper),Combining formal proofs with reinforcement learning can develop safe AI for cyber-physical systems.,Search,2018,5,"Nathan  Fulton, André  Platzer",2018 IEEE International Test Conference (ITC),,10.1109/TEST.2018.8624774,https://doi.org/10.1109/TEST.2018.8624774,https://semanticscholar.org/paper/03fce03060f6110b2772958f58539320bab519ed,,"Autonomous cyber-physical systems-such as self-driving cars and autonomous drones-often leverage artificial intelligence and machine learning algorithms to act well in open environments. Although testing plays an important role in ensuring safety and robustness, modern autonomous systems have grown so complex that achieving safety via testing alone is intractable. Formal verification reduces this testing burden by ruling out large classes of errant behavior at design time. This paper reviews recent work toward developing formal methods for cyber-physical systems that use AI for planning and control by combining the rigor of formal proofs with the flexibility of reinforcement learning.",,Review
Safe AI Systems,AI must meet safety values to minimize the destruction that may occur due to technology.,Search,2018,,Ananta  Bhatt,International Journal of Computer Applications,,10.5120/ijca2018918205,https://doi.org/10.5120/ijca2018918205,https://semanticscholar.org/paper/97aba8083bc5ad49af27a5c26c2bd40e6a17d6a3,,"As you know, it is inevitable to cease the advancement in AI because of the quest for developing systems that learns from experience and comply with the human capabilities. The leading ground-breaking impacts offers both positive and negative aspirations and are addressed as part of this research. To meet with the evil effects of the AI systems, is now the need of the hour to direct the focus on creating safe AI systems. The safe system must satisfy with 2 majorsapplying the security measures and analysing the system. Moreover, system must fulfil the risk and safety valuesintelligence, goals and safety in order to minimise the destruction that may occur due AI technology. However, further research is needed on the actual implementation of the safe AI systems in order to proceed with this research.",,
Safe Artificial Intelligence and Formal Methods - (Position Paper),Formal methods can assist in building safer and reliable AI.,Search,2016,9,Emil  Vassev,ISoLA,,10.1007/978-3-319-47166-2_49,https://doi.org/10.1007/978-3-319-47166-2_49,https://semanticscholar.org/paper/796dcb12b96e5d0c2f358b1b67fdfba36d023acc,https://ulir.ul.ie/bitstream/10344/5407/2/Vassev_2016_safe.pdf,"In one aspect of our life or another, today we all live with AI. For example, the mechanisms behind the search engines operating on the Internet do not just retrieve information, but also constantly learn how to respond more rapidly and usefully to our requests. Although framed by its human inventors, this AI is getting stronger and more powerful every day to go beyond the original human intentions in the future. One of the major questions emerging along with the propagation of AI in both technology and life is about safety in AI. This paper presents the author’s view about how formal methods can assist us in building safer and reliable AI.",,
Key Concepts in AI Safety: An Overview,“AI safety” is an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems.,Search,2021,,"Tim  Rudner, Helen  Toner",,,10.51593/20190040,https://doi.org/10.51593/20190040,https://semanticscholar.org/paper/f59ccf9ea5d7dd14ace4688503b8e15f3a2dd096,https://cset.georgetown.edu/wp-content/uploads/CSET-Key-Concepts-in-AI-Safety-An-Overview.pdf,"This paper is the first installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. In it, the authors introduce three categories of AI safety issues: problems of robustness, assurance, and specification. Other papers in this series elaborate on these and further key concepts.",,Review
Safe and sound - artificial intelligence in hazardous applications,Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making.,Search,2000,272,"John  Fox, Subrata Kumar Das",,,,,https://semanticscholar.org/paper/f62d215d4c40d299929d2e7fa105d54623a9e1bf,,"Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making, where small faults or errors can spell human catastrophe. This book describes, from both practical and theoretical perspectives, an AI technology for supporting sound clinical decision making and safe patient management. The technology combines techniques from conventional software engineering with a systematic method for building intelligent agents. Although the focus is on medicine, many of the ideas can be applied to AI systems in other hazardous settings. The book also covers a number of general AI problems, including knowledge representation and expertise modeling, reasoning and decision making under uncertainty, planning and scheduling, and the design and implementation of intelligent agents.The book, written in an informal style, begins with the medical background and motivations, technical challenges, and proposed solutions. It then turns to a wide-ranging discussion of intelligent and autonomous agents, with particular reference to safety and hazard management. The final section provides a detailed discussion of the knowledge representation and other aspects of the agent model developed in the book, along with a formal logical semantics for the language.",,
Safe AI—is this possible?☆,"The inherently self-learning, self-adaptive nature of artificial intelligence methods may require functionally deterministic controllers.",Search,1995,11,M. G. Rodd,,,10.1016/0952-1976(95)00010-X,https://doi.org/10.1016/0952-1976(95)00010-X,https://semanticscholar.org/paper/c53376e4e49ca954f4f4f2ca520b6042967f7dda,,"Abstract This deliberately provocative paper poses the question as to whether it is professionally acceptable to use AI-based control systems in real-time automation. The premise is that the increasing demands for improved control of industrial processes are resulting in a search for new control techniques which, it is suggested, will be based on various AI methodologies. However, in the face of the inherently self-learning, self-adaptive nature of such techniques, it is pointed out that to produce safe, predictable systems, the controllers should be functionally deterministic! In posing this dilemma, the paper suggests possible ways in which it may be resolved.",,
Decision Support for Safe AI Design,There is considerable interest in ethical designs for artificial intelligence that do not pose risks to humans.,Search,2012,4,Bill  Hibbard,AGI,,10.1007/978-3-642-35506-6_13,https://doi.org/10.1007/978-3-642-35506-6_13,https://semanticscholar.org/paper/c40cad8cacaa47aac6d4cab8f80749beaebd690c,http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_57.pdf,"There is considerable interest in ethical designs for artificial intelligence (AI) that do not pose risks to humans. This paper proposes using elements of Hutter's agent-environment framework to define a decision support system for simulating, visualizing and analyzing AI designs to understand their consequences. The simulations do not have to be accurate predictions of the future; rather they show the futures that an agent design predicts will fulfill its motivations and that can be explored by AI designers to find risks to humans. In order to safely create a simulation model this paper shows that the most probable finite stochastic program to explain a finite history is finitely computable, and that there is an agent that makes such a computation without any unintended instrumental actions. It also discusses the risks of running an AI in a simulated environment.",,
A Scenario-Based Method for Safety Certification of Artificial Intelligent Software,Increased researches and supererogatory efforts are providing to incorporate AI into the safety-critical systems.,Search,2010,2,"Guoqi  Li, Minyan  Lu, Bin  Liu",2010 International Conference on Artificial Intelligence and Computational Intelligence,,10.1109/AICI.2010.339,https://doi.org/10.1109/AICI.2010.339,https://semanticscholar.org/paper/6091d3429b8e1b88bc37a766129a1c0ae5089d4a,,"Artificial intelligence (AI) is attractive for safety critical fields. However, there have been few success cases, for the AI technique is usually lack of determinism and predictability, which is usually regarded as a disqualifier in a safety on text. Increased researches and supererogatory efforts are providing to incorporate AI into the safety-critical systems in recent years. In this paper, we present a scenario-based method for safety certification, with the method AI modules of system could be evaluated before invoked, if its trust ability is satisfied, then the program will be performed for safety critical systems, otherwise it will be terminated to ask human assistance or postpone the missions.",,
Safe AI - IS This Possible?,"The inherently self-learning, self-adaptive nature of AI may require deterministic controllers to produce safe, predictable systems.",Search,1994,1,M. G. Rodd,,,10.1016/S1474-6670(17)45702-8,https://doi.org/10.1016/S1474-6670(17)45702-8,https://semanticscholar.org/paper/a9ac4c8bd3611e530f5315b54abefe100ba79ae0,,"Abstract This deliberately provocative paper poses the question as to whether it is professionally-acceptable to use AI-based control systems in real-time automation. The premise is that the increasing demands for improved control of industrial processes are resulting in a search for new control techniques which, it is suggested, will be based on various AI methodologies. However, in the face of the inherently self-learning, self-adaptive nature of such techniques, it is pointed out that to produce safe, predictable systems, the controllers should be functionally deterministic! In posing this dilemma, the paper suggests possible ways in which it may be resolved.",,
Multilayered review of safety approaches for machine learning-based systems in the days of AI,"Safety requirements engineering, safety-driven design at both system and machine learning component level, validation and verification are required to ensure safe AI.",Search,2021,1,"Sangeeta  Dey, Seok-Won  Lee",J. Syst. Softw.,,10.1016/j.jss.2021.110941,https://doi.org/10.1016/j.jss.2021.110941,https://semanticscholar.org/paper/d3b68a36315eed6008f2b615671930e7f4d81310,,"Abstract: The unprecedented advancement of artificial intelligence (AI) in recent years has altered our perspectives on software engineering and systems engineering as a whole. Nowadays, software-intensive intelligent systems rely more on a learning model than thousands of lines of codes. Such alteration has led to new research challenges in the engineering process that can ensure the safe and beneficial behavior of AI systems. This paper presents a literature survey of the significant efforts made in the last fifteen years to foster safety in complex intelligent systems. This survey covers relevant aspects of AI safety research including safety requirements engineering, safety-driven design at both system and machine learning (ML) component level, validation and verification from the perspective of software and system engineers. We categorize these research efforts based on a three-layered conceptual framework for developing and maintaining AI systems. We also perform a gap analysis to emphasize the open research challenges in ensuring safe AI. Finally, we conclude the paper by providing future research directions and a road map for AI safety.",,Review
Governing AI safety through independent audits,Independent audit of AI systems is a pragmatic approach to a burdensome and unenforceable assurance challenge.,Search,2021,8,"Gregory  Falco, Ben  Shneiderman, Julia  Badger, Ryan  Carrier, A. T. Dahbura, David  Danks, Martin  Eling, Alwyn  Goodloe, Jerry  Gupta, Christopher  Hart, Marina  Jirotka, Henric  Johnson, Cara  LaPointe, Ashley J. Llorens, Alan K. Mackworth, Carsten  Maple, Sigurður Emil Pálsson, Frank A. Pasquale, Alan F. T. Winfield, Zee Kin Yeong",Nat. Mach. Intell.,,10.1038/S42256-021-00370-7,https://doi.org/10.1038/S42256-021-00370-7,https://semanticscholar.org/paper/93a04c8661ce96f9ab972a0ede4680232627467a,https://ora.ox.ac.uk/objects/uuid:900c78bf-4fe3-4afc-aaed-5a5fdffecbff/download_file?safe_filename=Falco_et_al_2021_Governing_AI_safety.pdf&file_format=pdf&type_of_work=Journal+article,"Highly automated systems are becoming omnipresent. They range in function from self-driving vehicles to advanced medical diagnostics and afford many benefits. However, there are assurance challenges that have become increasingly visible in high-profile crashes and incidents. Governance of such systems is critical to garner widespread public trust. Governance principles have been previously proposed offering aspirational guidance to automated system developers; however, their implementation is often impractical given the excessive costs and processes required to enact and then enforce the principles. This Perspective, authored by an international and multidisciplinary team across government organizations, industry and academia, proposes a mechanism to drive widespread assurance of highly automated systems: independent audit. As proposed, independent audit of AI systems would embody three ‘AAA’ governance principles of prospective risk Assessments, operation Audit trails and system Adherence to jurisdictional requirements. Independent audit of AI systems serves as a pragmatic approach to an otherwise burdensome and unenforceable assurance challenge. As highly automated systems become pervasive in society, enforceable governance principles are needed to ensure safe deployment. This Perspective proposes a pragmatic approach where independent audit of AI systems is central. The framework would embody three AAA governance principles: prospective risk Assessments, operation Audit trails and system Adherence to jurisdictional requirements.",,
"Visual book review 1 “ Safe and Sound , AI in hazardous applications",The critical component of the logic of argument (LA) is exposed in Axiom 8.,Search,2001,3,Boris  Kovalerchuk,,,,,https://semanticscholar.org/paper/41f895337f19cd042dd64b1942b63c48acc57128,,"A recent book “Safe and Sound, AI in hazardous applications” by John Fox and Subrata Das (AAAI Press/ The MIT Press, 2000, 293p., ISBN 0-262-06211-9) attracts attention of the research community and practitioners to the problem of safety of traditional and computer-aided medical diagnosis and treatment. The authors use medical applications as a focal point for the general safety problem through variety of hazardous applications. At first glance, the problem is already well known. However, they show that discovery and analysis of the sources of danger in hazardous applications are far from having rigorous solutions. The book uses an artificial intelligence (AI) approach, which allows one to express different types of statements in consistent logical fashion. Specifically the authors consider two main types of statements for making medical decisions: claims and their grounds. In addition, a confidence value is assigned to a claim using ground statements. For instance, the claim can be that Mr. P has a gastric ulcer and the grounds can be that Mr. P has pain after meals & ulcers are painful because of an increase in acidity. The word “support” can express the level of confidence. In the book, the safety issue for defining diagnosis and treatment is viewed as adding restrictions on inference. The goal of restrictions is to avoid dangerous actions for patients. This review should help a reader to see conditions for successful applications of logic of argument (LA), which is the central theme of the book. The book contains three parts (see Figure 1): Part 1: Method for building software agents, which produce Rigorously Engineered Decisions. Part 2: Technique for deploying agents in general and hazardous environment with medical examples. Part 3: Formal and logical aspect of the method. The spread of the central theme over the book is presented in Figure 1. Chapter 4 provides an informal description of LA, Chapters 13 and 15 contain formalisms and Chapter 15 implementation of LA in Prolog language. The critical component of LA is exposed in Axiom 8 (Chapter 13). This axiom is the central assumption of LA. It actually formalizes the requirement of independence of arguments used in LA in inferring diagnosis or treatment recommendation. The property is also known as truth-functionality in Artificial Intelligence literature [1] and was a subject of intensive discussions in AI community for years [1,2,3].",,Review
Guidelines for Artificial Intelligence Containment,Safety container software can study and analyze intelligent artificial agents while maintaining safety levels.,Search,2019,20,"James  Babcock, János  Kramár, Roman V. Yampolskiy",Next-Generation Ethics,,10.1017/9781108616188.008,https://doi.org/10.1017/9781108616188.008,https://semanticscholar.org/paper/b0c820f9d7fa4d8c56a7887548834b7ba65ddfa5,http://www.vetta.org/documents/Machine_Super_Intelligence.pdf,"With almost daily improvements in capabilities of artificial intelligence it is more important than ever to develop safety software for use by the AI research community. Building on our previous work on AI Containment Problem we propose a number of guidelines which should help AI safety researchers to develop reliable sandboxing software for intelligent programs of all levels. Such safety container software will make it possible to study and analyze intelligent artificial agent while maintaining certain level of safety against information leakage, social engineering attacks and cyberattacks from within the container.",,
AI safety: state of the field through quantitative lens,AI safety is the field under which we need to decide the direction of humanity’s future.,Search,2020,7,"Mislav  Juric, Agneza  Sandic, Mario  Brcic","2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO)",,10.23919/mipro48935.2020.9245153,https://doi.org/10.23919/mipro48935.2020.9245153,https://semanticscholar.org/paper/2e5ab5250e524801e2efd24249d75fcbb80f2b99,http://arxiv.org/pdf/2002.05671,"Last decade has seen major improvements in the performance of artificial intelligence which has driven wide-spread applications. Unforeseen effects of such massad-option has put the notion of AI safety into the public eye. AI safety is a relatively new field of research focused on techniques for building AI beneficial for humans. While there exist survey papers for the field of AI safety, there is a lack of a quantitative look at the research being conducted. The quantitative aspect gives a data-driven insight about the emerging trends, knowledge gaps and potential areas for future research. In this paper, bibliometric analysis of the literature finds significant increase in research activity since 2015. Also, the field is so new that most of the technical issues are open, including: explainability and its long-term utility, and value alignment which we have identified as the most important long-term research topic. Equally, there is a severe lack of research into concrete policies regarding AI. As we expect AI to be the one of the main driving forces of changes, AI safety is the field under which we need to decide the direction of humanity’s future.",,
Establishing the rules for building trustworthy AI,The European Commission’s report ‘Ethics guidelines for trustworthy AI’ facilitates international support for AI solutions that are good for the environment.,Search,2019,67,Luciano  Floridi,Nature Machine Intelligence,,10.1038/S42256-019-0055-Y,https://doi.org/10.1038/S42256-019-0055-Y,https://semanticscholar.org/paper/dc44e2be0f85b6225f05390c570885337a99ef83,https://philpapers.org/archive/FLOETR.pdf,"The European Commission’s report ‘Ethics guidelines for trustworthy AI’ provides a clear benchmark to evaluate the responsible development of AI systems, and facilitates international support for AI solutions that are good for humanity and the environment, says Luciano Floridi.",,