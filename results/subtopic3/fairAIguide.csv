Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Design Methods for Artificial Intelligence Fairness and Transparency,"Fairness and transparency in artificial intelligence continue to become more prevalent topics for research, design and development.",Search,2021,,"Simone  Stumpf, Lorenzo  Strappelli, Subeida  Ahmed, Yuri  Nakao, Aisha  Naseer, Giulia Del Gamba, Daniele  Regoli",IUI Workshops,,,,https://semanticscholar.org/paper/baf8cb94908738c3b2d040a6b44d812e5ca80396,,"Fairness and transparency in artificial intelligence (AI) continue to become more prevalent as topics for research, design and development. General principles and guidelines for designing ethical and responsible AI systems have been proposed, yet there is a lack of design methods for these kinds of systems. In this paper, we present CoFAIR, a novel method to design user interfaces for exploring fairness, consisting of series of co-design workshops, and wider evaluation. This method can be readily applied in practice by researchers, designers and developers to create responsible and ethical AI systems.",,
Towards the Right Kind of Fairness in AI,Choosing the right kind of fairness for a given AI system depends on ethical standards and legal requirements.,Search,2021,2,"Boris  Ruf, Marcin  Detyniecki",ArXiv,,,,https://semanticscholar.org/paper/58e8e3d258dc656d9ef90f7468eeff7d7a513e89,,"Fairness is a concept of justice. Various definitions exist, some of them conflicting with each other. In the absence of an uniformly accepted notion of fairness, choosing the right kind for a specific situation has always been a central issue in human history. When it comes to implementing sustainable fairness in artificial intelligence systems, this old question plays a key role once again: How to identify the most appropriate fairness metric for a particular application? The answer is often a matter of context, and the best choice depends on ethical standards and legal requirements. Since ethics guidelines on this topic are kept rather general for now, we aim to provide more hands-on guidance with this document. Therefore, we first structure the complex landscape of existing fairness metrics and explain the different options by example. Furthermore, we propose the “Fairness Compass”, a tool which formalises the selection process and makes identifying the most appropriate fairness definition for a given system a simple, straightforward procedure. Because this process also allows to document the reasoning behind the respective decisions, we argue that this approach can help to build trust from the user through explaining and justifying the implemented fairness. ∗{boris.ruf,marcin.detyniecki}@axa.com 1 ar X iv :2 10 2. 08 45 3v 6 [ cs .A I] 2 7 A ug 2 02 1",,
Fair and Responsible AI: A Focus on the Ability to Contest,AI ethics guidelines emphasize the ability to contest high-stakes decisions as an important safeguard for individuals.,Search,2021,1,"Henrietta  Lyons, Eduardo  Velloso, Tim  Miller",ArXiv,,,,https://semanticscholar.org/paper/8fe8f042a60c090c4c9bad60ef665c18c2ac9d87,,"Copyright held by the owner/author(s). Fair and Responsible AI Workshop (CHI’20), April 25–30, 2020, Honolulu, HI, USA ACM 978-1-4503-6819-3/20/04. https://doi.org/10.1145/3334480.XXXXXXX Abstract As the use of artificial intelligence (AI) in high-stakes decisionmaking increases, the ability to contest such decisions is being recognised in AI ethics guidelines as an important safeguard for individuals. Yet, there is little guidance on how AI systems can be designed to support contestation. In this paper we explain that the design of a contestation process is important due to its impact on perceptions of fairness and satisfaction. We also consider design challenges, including a lack of transparency as well as the numerous design options that decision-making entities will be faced with. We argue for a human-centred approach to designing for contestability to ensure that the needs of decision subjects, and the community, are met.",,
"AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias","A comprehensive set of metrics, algorithms to mitigate bias, and an interactive Web experience facilitate the transition of fairness research to an industrial setting.",Search,2018,316,"Rachel K. E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Jacquelyn  Martino, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",ArXiv,,,,https://semanticscholar.org/paper/c8541b1dc813f3a638d7acc79e5f972e77f3c5a7,,"Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license {this https URL). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms.

The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (this https URL) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.",,
Fairlearn: A toolkit for assessing and improving fairness in AI,Fairlearn is a toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems.,Search,2020,48,"Sarah  Bird, Miro  Dudík, Richard  Edgar, Brandon  Horn, Roman  Lutz, Vanessa  Milan, Mehrnoosh  Sameki, Hanna  Wallach, Kathleen  Walker",,,,,https://semanticscholar.org/paper/5894d57ea49bd5c136ebefb1e6c3986555908ea0,,"We introduce Fairlearn, an open source toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems. Fairlearn has two components: an interactive visualization dashboard and unfairness mitigation algorithms. These components are designed to help with navigating trade-offs between fairness and model performance. We emphasize that prioritizing fairness in AI systems is a sociotechnical challenge. Because there are many complex sources of unfairness—some societal and some technical—it is not possible to fully “debias” a system or to guarantee fairness; the goal is to mitigate fairness-related harms as much as possible. As Fairlearn grows to include additional fairness metrics, unfairness mitigation algorithms, and visualization capabilities, we hope that it will be shaped by a diverse community of stakeholders, ranging from data scientists, developers, and business decision makers to the people whose lives may be affected by the predictions of AI systems.",,
Introduction to AI Fairness,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern.",Search,2020,2,"Yunfeng  Zhang, Rachel K. E. Bellamy, Moninder  Singh, Q. Vera Liao",CHI Extended Abstracts,,10.1145/3334480.3375059,https://doi.org/10.1145/3334480.3375059,https://semanticscholar.org/paper/d49f8d57240cd04dd66d91bf53c639497139ab3a,,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. Recently, the AI research community has proposed many methods to measure and mitigate unwanted biases, and developed open-source toolkits for developers to make fair AI. This course will cover the recent development in algorithmic fairness, including the many different definitions of fairness, their corresponding quantitative measurements, and ways to mitigate biases. This course is open to beginners and is designed for anyone interested in the topic of AI fairness.",,
Introduction to AI Fairness,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern.",Search,2021,,"Yunfeng  Zhang, Rachel K. E. Bellamy, Q. Vera Liao, Moninder  Singh",CHI Extended Abstracts,,10.1145/3411763.3444998,https://doi.org/10.1145/3411763.3444998,https://semanticscholar.org/paper/e24d854f822bebd531926eee518d4b2e1455e5de,,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. Recently, the AI research community has proposed many methods to measure and mitigate unwanted biases, and developed open-source toolkits for developers to make fair AI. This course will cover the recent development in algorithmic fairness, including the many different definitions of fairness, their corresponding quantitative measurements, and ways to mitigate biases. This course is open to beginners and is designed for anyone interested in the topic of AI fairness.",,
HPCFAIR: Enabling FAIR AI for HPC Applications,"A modular, extensible framework to enable AI models to be Findable, Accessible, Interoperable and Reproducible (FAIR) is proposed.",Search,2021,,"Gaurav  Verma, Murali  Emani, Chunhua  Liao, Pei-Hung  Lin, Tristan  Vanderbruggen, Xipeng  Shen, Barbara M. Chapman",2021 IEEE/ACM Workshop on Machine Learning in High Performance Computing Environments (MLHPC),,10.1109/mlhpc54614.2021.00011,https://doi.org/10.1109/mlhpc54614.2021.00011,https://semanticscholar.org/paper/e3e6cb8547011b57644526f0c182da991fa6062d,,"Artificial Intelligence (AI) is being adopted in different domains at an unprecedented scale. A significant interest in the scientific community also involves leveraging machine learning (ML) to effectively run high performance computing applications at scale. Given multiple efforts in this arena, there are often duplicated efforts when existing rich data sets and ML models could be leveraged instead. The primary challenge is a lack of an ecosystem to reuse and reproduce the models and datasets. In this work, we propose HPCFAIR, a modular, extensible framework to enable AI models to be Findable, Accessible, Interoperable and Reproducible (FAIR). It enables users with a structured approach to search, load, save and reuse the models in their codes. We present the design, implementation of our framework and highlight how it can be seamlessly integrated to ML-driven applications for high performance computing applications and scientific machine learning workloads.",,
Understanding artificial intelligence ethics and safety,"The UK public sector can prevent harmful AI by putting in place governance processes that support the design and implementation of ethical, fair, and safe AI systems.",Search,2019,37,David  Leslie,ArXiv,,10.5281/zenodo.3240529,https://doi.org/10.5281/zenodo.3240529,https://semanticscholar.org/paper/adeb97576107e9f8e1140302cf614d1da709d523,,"A remarkable time of human promise has been ushered in by the convergence of the ever-expanding availability of big data, the soaring speed and stretch of cloud computing platforms, and the advancement of increasingly sophisticated machine learning algorithms. Innovations in AI are already leaving a mark on government by improving the provision of essential social goods and services from healthcare, education, and transportation to food supply, energy, and environmental management. These bounties are likely just the start. The prospect that progress in AI will help government to confront some of its most urgent challenges is exciting, but legitimate worries abound. As with any new and rapidly evolving technology, a steep learning curve means that mistakes and miscalculations will be made and that both unanticipated and harmful impacts will occur.

This guide, written for department and delivery leads in the UK public sector and adopted by the British Government in its publication, 'Using AI in the Public Sector,' identifies the potential harms caused by AI systems and proposes concrete, operationalisable measures to counteract them. It stresses that public sector organisations can anticipate and prevent these potential harms by stewarding a culture of responsible innovation and by putting in place governance processes that support the design and implementation of ethical, fair, and safe AI systems. It also highlights the need for algorithmically supported outcomes to be interpretable by their users and made understandable to decision subjects in clear, non-technical, and accessible ways. Finally, it builds out a vision of human-centred and context-sensitive implementation that gives a central role to communication, evidence-based reasoning, situational awareness, and moral justifiability.",,
FAIR AI: A Conceptual Framework for Democratisation of 21st Century AI,The lack of interpretability is a main inhibitor of broader use of 21st century AI.,Search,2021,,Saman  Halgamuge,"2021 International Conference on Instrumentation, Control, and Automation (ICA)",,10.1109/ICA52848.2021.9625672,https://doi.org/10.1109/ICA52848.2021.9625672,https://semanticscholar.org/paper/cae45a30452713cacad1e5ec04adfb4a065f6a8e,,"Popular models of AI have two significant deficiencies: 1) they are mostly manually designed using the experience of AI-experts 2) they lack human interpretability, i.e., users cannot make sense of the functionality of neural network architectures either semantically/linguistically or mathematically. This lack of interpretability is a main inhibitor of broader use of 21st century AI, e.g., Deep Neural Networks (DNN). The dependence on AI experts to create AI hinders the democratisation of AI and therefore the accessibility to AI. Addressing these deficiencies would provide answers to some of the valid questions about traceability, accountability and the ability to integrate existing knowledge (scientific or linguistically articulated human experience) into the model. This keynote abstract addresses these two significant deficiencies that inhibit the democratisation of AI by developing new methods that can automatically create interpretable neural network models without the help of AI-experts. The proposed cross-fertilised innovation will have a profound impact on the society through the increased accessibility and trustworthiness of AI beneficial to almost all areas of sciences, engineering, and humanities.",,
AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias,"A comprehensive set of metrics, explanations, and algorithms to mitigate bias in datasets and models is included in the AI Fairness 360 toolkit.",Search,2019,113,"Rachel K. E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Jacquelyn A. Martino, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",IBM J. Res. Dev.,,10.1147/jrd.2019.2942287,https://doi.org/10.1147/jrd.2019.2942287,https://semanticscholar.org/paper/333671a5fbbf726f8819138f3670524ec0405726,,"Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This article introduces a new open-source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license (

https://github.com/ibm/aif360

). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms for use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience that provides a gentle introduction to the concepts and capabilities for line-of-business users, researchers, and developers to extend the toolkit with their new algorithms and improvements and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.",,
How Fair AI Can Make Us Richer,"AI systems steer important parts of our lives if we get admitted to university, if we are hired, fired or promoted.",Search,2021,,Sandra  Wachter,European Data Protection Law Review,,10.21552/edpl/2021/3/5,https://doi.org/10.21552/edpl/2021/3/5,https://semanticscholar.org/paper/24aa2a3be2464b0f641207bd1f2886c180cb1dc1,https://edpl.lexxion.eu/data/article/17685/pdf/edpl_2021_03-006.pdf,"Sandra Wachter* We are all aware that artificial intelligence (AI) has now become an integral part of our lives. AI systems are behind mundane tasks such as displaying search results on Safari, preparing travel routs on Google and suggesting new music on Spotify. But algorithms also steer important parts of our lives: if we get admitted to university, if we are hired, fired or promoted, if we get insurance, social benefits or a loan, or even if we have to go to prison. Algorithms can touch almost every aspect of our lives.",,
Design for fairness in AI: Cooking a fair AI Dish,"AI development teams need support in creating more ethical AI systems, and design can help bridge the gap between ethical principles and current practice.",Search,2019,1,Dasha  Simons,,,,,https://semanticscholar.org/paper/1786ae209452ad3a187c640d2fa0905a16c7d6aa,,"Artificial intelligence (AI) is an emerging field which unleashes massive new (business) opportunities. The potential growth and broad application of the AI technology has great economic benefits however also severe societal implications. Simultaneously, ethical challenges arise with its development. Questions of values and ethics are becoming urgent, as systems can be negatively biased and the decision processes are often not traceable, while impacting our lives. Abstract concepts such as fairness and values need to find their way into the fast and agile AI development processes. The contemporary (research and practice) fields tackle these challenges by technological feats, ethical AI principles and strategies. However, it are the decisions made by humans today and tomorrow that will shape our future. It is, therefore, alarming the translation of ethics to that day to day work of the AI development team is missing. Hence, the central aim of this thesis is to explore and design support for AI teams with the creation of more ethical AI systems, bridging the gap between ethical AI principles and current practice. By that, design for organizational capacity for the development of fairer AI by using strategic design and critical design approaches. In this thesis, due to the diversity and magnitude of ethical challenges in AI, particular attention is paid to two challenges, fairness and value-alignment, to benefit from a design perspective. Three streams of expertise are brought together to tackle these challenges: AI, applied ethics and design. Ethics bears critique, and this thesis argues that it can benefit from a design perspective, using imagination in the solution space and synthesized thinking for implementable ideas instead of solely discussion. The thesis focuses on ways how design approaches can supplement the ethical ones and thereby stimulate the ethical uptake in the AI field. Instead of defining what fairness is, this thesis takes a novel approach in unraveling ten unfairness sources in the AI development. It is aspired to reduce these sources of unfairness in AI, in project specific fashion. In AI practice, the ways ethics is incorporated and how value tensions are resolved is under-researched. In depth interviews, generative tools and provotypes are conducted and designed to research and critique the contemporary AI field in relation to ethics, both with IBM and their clients. Simultaneously to inquire novel value tensions in its development. Five main value tensions are unraveled in its relation to fairness. All above is consolidated a framework to design for organizational capacity and team support leading to the creation of fairer and value-aligned AI systems. With this framework an organizational role is designed, the ethical coach, to aid the AI team with cocreating fairer and value-aligned AI systems with an accompanying modular toolkit. The modular toolkit is iterated upon multiple times and uses the AI dish metaphor. Finally, two evaluation sessions with IBM and their clients as well as the conversations concerning of the implementation of the toolkit led to recommendations for further development including education and implementation directions.",,
Explaining how your AI system is fair,Sharing reasons and principles for AI fairness decisions with the broader audience is key to maintaining confidence in AI systems.,Search,2021,,"Boris  Ruf, Marcin  Detyniecki",ArXiv,,,,https://semanticscholar.org/paper/1271e86aad602cfa09727dc98bae31c19ce77cc6,,"Copyright held by the owner/author(s). CHI’21,, May 8–13, 2021, Online Virtual Conference (originally Yokohama, Japan) ACM 978-1-4503-6819-3/20/04. https://doi.org/10.1145/3334480.XXXXXXX Abstract To implement fair machine learning in a sustainable way, choosing the right fairness objective is key. Since fairness is a concept of justice which comes in various, sometimes conflicting definitions, this is not a trivial task though. The most appropriate fairness definition for an artificial intelligence (AI) system is a matter of ethical standards and legal requirements, and the right choice depends on the particular use case and its context. In this position paper, we propose to use a decision tree as means to explain and justify the implemented kind of fairness to the end users. Such a structure would first of all support AI practitioners in mapping ethical principles to fairness definitions for a concrete application and therefore make the selection a straightforward and transparent process. However, this approach would also help document the reasoning behind the decision making. Due to the general complexity of the topic of fairness in AI, we argue that specifying ""fairness"" for a given use case is the best way forward to maintain confidence in AI systems. In this case, this could be achieved by sharing the reasons and principles expressed during the decision making process with the broader audience.",,
The Myth of Complete AI-Fairness,Nothing is ever completely fair in all situations.,Search,2021,1,Virginia  Dignum,AIME,,10.1007/978-3-030-77211-6_1,https://doi.org/10.1007/978-3-030-77211-6_1,https://semanticscholar.org/paper/62b5415c382c4b6584789a51262e1e902cb07998,http://arxiv.org/pdf/2104.12544,"Just recently, IBM invited me to participate in a panel titled “Will AI ever be completely fair?” My first reaction was that it surely would be a very short panel, as the only possible answer is ‘no’. In this short paper, I wish to further motivate my position in that debate: “I will never be completely fair. Nothing ever is. The point is not complete fairness, but the need to establish metrics and thresholds for fairness that ensure trust in AI systems”. The idea of fairness and justice has long and deep roots in Western civilization, and is strongly linked to ethics. It is therefore not strange that it is core to the current discussion about the ethics of development and use of AI systems. Given that we often associate fairness with consistency and accuracy, the idea that our decisions and decisions affecting us can become fairer by replacing human judgement by automated, numerical, systems, is therefore appealing. However, as Laurie Anderson recently said “If you think technology will solve your problems, you don’t understand technology — and you don’t understand your problems.” AI is not magic, and its results are fundamentally constrained by the convictions and expectations of those that build, manage, deploy and use it. Which makes it crucial that we understand the mechanisms behind the systems and their decision capabilities. The pursuit of fair AI is currently a lively one. One involving many researchers, meetings and conferences (of which FAccT is the most known) and refers to the notion that an algorithm is fair, if its results are independent of given variables, especially those considered sensitive, such as the traits of individuals which should not correlate with the outcome (i.e. gender, ethnicity, sexual orientation, disability, etc.). However, nothing is ever 100% fair in 100% of the situations, and due to complex networked connection, to ensure fairness for one (group) may lead to unfairness for others. Moreover, what we consider fair often does depend on the traits of individuals. An obvious example are social services. Most people believe in the need for some form of social services, whether it is for children, for the elderly, for the sick or the poor. And many of us will benefit from social services at least once in our lives. Decision making in the attribution of social benefits is dependent on individual characteristics such as",,
Fairness Score and Process Standardization: Framework for Fairness Certification in Artificial Intelligence Systems,A Fairness Certificate issued by a designated third-party auditing agency would boost the conviction of organizations in the AI systems that they intend to deploy.,Search,2022,,"Avinash  Agarwal, Harsh  Agarwal, Nihaarika  Agarwal",ArXiv,,,,https://semanticscholar.org/paper/64c2fa6b3c161fe2a5ae20f03ff77c3250027e10,,"Decisions made by various Artificial Intelligence (AI) systems greatly influence our day-to-day lives. With the increasing use of AI systems, it becomes crucial to know that they are fair, identify the underlying biases in their decision-making, and create a standardized framework to ascertain their fairness. In this paper, we propose a novel Fairness Score to measure the fairness of a data-driven AI system and a Standard Operating Procedure (SOP) for issuing Fairness Certification for such systems. Fairness Score and audit process standardization will ensure quality, reduce ambiguity, enable comparison and improve the trustworthiness of the AI systems. It will also provide a framework to operationalise the concept of fairness and facilitate the commercial deployment of such systems. Furthermore, a Fairness Certificate issued by a designated third-party auditing agency following the standardized process would boost the conviction of the organizations in the AI systems that they intend to deploy. The Bias Index proposed in this paper also reveals comparative bias amongst the various protected attributes within the dataset. To substantiate the proposed framework, we iteratively train a model on biased and unbiased data using multiple datasets and check that the Fairness Score and the proposed process correctly identify the biases and judge the fairness.",,