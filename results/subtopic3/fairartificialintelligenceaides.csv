Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Think Your Artificial Intelligence Software Is Fair? Think Again,"Machine-learning software is also biased, sometimes in similar ways to humans, often in different ways.",Search,2019,14,"Rachel K.E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John  Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",IEEE Software,,10.1109/MS.2019.2908514,https://doi.org/10.1109/MS.2019.2908514,https://semanticscholar.org/paper/4d60f78b44f34a67a5ce6316d1c45c90a912db44,,"Today, machine-learning software is used to help make decisions that affect people's lives. Some people believe that the application of such software results in fairer decisions because, unlike humans, machine-learning software generates models that are not biased. Think again. Machine-learning software is also biased, sometimes in similar ways to humans, often in different ways. While fair model- assisted decision making involves more than the application of unbiased models-consideration of application context, specifics of the decisions being made, resolution of conflicting stakeholder viewpoints, and so forth-mitigating bias from machine-learning software is important and possible but difficult and too often ignored.",,
Design Methods for Artificial Intelligence Fairness and Transparency,"Fairness and transparency in artificial intelligence continue to become more prevalent as topics for research, design and development.",Search,2021,,"Simone  Stumpf, Lorenzo  Strappelli, Subeida  Ahmed, Yuri  Nakao, Aisha  Naseer, Giulia Del Gamba, Daniele  Regoli",IUI Workshops,,,,https://semanticscholar.org/paper/baf8cb94908738c3b2d040a6b44d812e5ca80396,,"Fairness and transparency in artificial intelligence (AI) continue to become more prevalent as topics for research, design and development. General principles and guidelines for designing ethical and responsible AI systems have been proposed, yet there is a lack of design methods for these kinds of systems. In this paper, we present CoFAIR, a novel method to design user interfaces for exploring fairness, consisting of series of co-design workshops, and wider evaluation. This method can be readily applied in practice by researchers, designers and developers to create responsible and ethical AI systems.",,
Fairness Assessment for Artificial Intelligence in Financial Industry,Fairness evaluation is a key component of AI governance.,Search,2019,7,"Yukun  Zhang, Longsheng  Zhou",ArXiv,,,,https://semanticscholar.org/paper/9b0f1ec29596a1e5d41366235322f7d1d68be9e4,,"Artificial Intelligence (AI) is an important driving force for the development and transformation of the financial industry. However, with the fast-evolving AI technology and application, unintentional bias, insufficient model validation, immature contingency plan and other underestimated threats may expose the company to operational and reputational risks. In this paper, we focus on fairness evaluation, one of the key components of AI Governance, through a quantitative lens. Statistical methods are reviewed for imbalanced data treatment and bias mitigation. These methods and fairness evaluation metrics are then applied to a credit card default payment example.",,
AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias,Fairness is an increasingly important concern as machine learning models are used to support decision making.,Search,2019,113,"Rachel K. E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Jacquelyn A. Martino, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",IBM J. Res. Dev.,,10.1147/jrd.2019.2942287,https://doi.org/10.1147/jrd.2019.2942287,https://semanticscholar.org/paper/333671a5fbbf726f8819138f3670524ec0405726,,"Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This article introduces a new open-source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license (

https://github.com/ibm/aif360

). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms for use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience that provides a gentle introduction to the concepts and capabilities for line-of-business users, researchers, and developers to extend the toolkit with their new algorithms and improvements and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.",,
Bias in Multimodal AI: Testbed for Fair Automatic Recruitment,FairCVtest is a fictitious automated recruitment testbed able to show the capacity of the AI behind such recruitment tool to extract sensitive information from unstructured data.,Search,2020,16,"Alejandro  Pena, Ignacio  Serna, Aythami  Morales, Julian  Fierrez",2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),,10.1109/CVPRW50498.2020.00022,https://doi.org/10.1109/CVPRW50498.2020.00022,https://semanticscholar.org/paper/e924bfb95435153185f8d89e77f5a3534e2a29bd,http://arxiv.org/pdf/2004.07173,"The presence of decision-making algorithms in society is rapidly increasing nowadays, while concerns about their transparency and the possibility of these algorithms becoming new sources of discrimination are arising. In fact, many relevant automated systems have been shown to make decisions based on sensitive information or discriminate certain social groups (e.g. certain biometric systems for person recognition). With the aim of studying how current multimodal algorithms based on heterogeneous sources of information are affected by sensitive elements and inner biases in the data, we propose a fictitious automated recruitment testbed: FairCVtest. We train automatic recruitment algorithms using a set of multimodal synthetic profiles consciously scored with gender and racial biases. Fair-CVtest shows the capacity of the Artificial Intelligence (AI) behind such recruitment tool to extract sensitive information from unstructured data, and exploit it in combination to data biases in undesirable (unfair) ways. Finally, we present a list of recent works developing techniques capable of removing sensitive information from the decision-making process of deep learning architectures. We have used one of these algorithms (SensitiveNets) to experiment discrimination-aware learning for the elimination of sensitive information in our multimodal AI framework. Our methodology and results show how to generate fairer AI-based tools in general, and in particular fairer automated recruitment systems.",,
Explaining how your AI system is fair,Sharing reasons and principles for AI fairness decisions will help maintain confidence in AI systems.,Search,2021,,"Boris  Ruf, Marcin  Detyniecki",ArXiv,,,,https://semanticscholar.org/paper/1271e86aad602cfa09727dc98bae31c19ce77cc6,,"Copyright held by the owner/author(s). CHI’21,, May 8–13, 2021, Online Virtual Conference (originally Yokohama, Japan) ACM 978-1-4503-6819-3/20/04. https://doi.org/10.1145/3334480.XXXXXXX Abstract To implement fair machine learning in a sustainable way, choosing the right fairness objective is key. Since fairness is a concept of justice which comes in various, sometimes conflicting definitions, this is not a trivial task though. The most appropriate fairness definition for an artificial intelligence (AI) system is a matter of ethical standards and legal requirements, and the right choice depends on the particular use case and its context. In this position paper, we propose to use a decision tree as means to explain and justify the implemented kind of fairness to the end users. Such a structure would first of all support AI practitioners in mapping ethical principles to fairness definitions for a concrete application and therefore make the selection a straightforward and transparent process. However, this approach would also help document the reasoning behind the decision making. Due to the general complexity of the topic of fairness in AI, we argue that specifying ""fairness"" for a given use case is the best way forward to maintain confidence in AI systems. In this case, this could be achieved by sharing the reasons and principles expressed during the decision making process with the broader audience.",,
Towards Fairness Certification in Artificial Intelligence,Machine learning models incur the risk of amplifying prejudices and societal stereotypes by over associating protected attributes.,Search,2021,1,"Tatiana  Tommasi, Silvia  Bucci, Barbara  Caputo, Pietro  Asinari",ArXiv,,,,https://semanticscholar.org/paper/157246efaa0a667383cd78da0599231687368e0e,,"Thanks to the great progress of machine learning in the last years, several Artificial Intelligence (AI) techniques have been increasingly moving from the controlled research laboratory settings to our everyday life. The most simple examples are the spam filters that keep our email account in order, face detectors that help us when taking a portrait picture, online recommender systems that suggest which movie and clothing we might like, or interactive maps that navigate us towards our vacation home. Artificial intelligence is clearly supportive in many decision-making scenarios, but when it comes to sensitive areas such as health care, hiring policies, education, banking or justice, with major impact on individuals and society, it becomes crucial to establish guidelines on how to design, develop, deploy and monitor this technology. Indeed the decision rules elaborated by machine learning models are data-driven and there are multiple ways in which discriminatory biases can seep into data. Algorithms trained on those data incur the risk of amplifying prejudices and societal stereotypes by over associating protected attributes such as gender, ethnicity or disabilities with the prediction task.",,
"AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias",Fairness is an increasingly important concern as machine learning models are used to support decision making.,Search,2018,316,"Rachel K. E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Jacquelyn  Martino, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",ArXiv,,,,https://semanticscholar.org/paper/c8541b1dc813f3a638d7acc79e5f972e77f3c5a7,,"Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license {this https URL). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms.

The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (this https URL) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.",,
Fairlearn: A toolkit for assessing and improving fairness in AI,The goal of fairlearning is to mitigate the fairness-related harms of AI systems.,Search,2020,48,"Sarah  Bird, Miro  Dudík, Richard  Edgar, Brandon  Horn, Roman  Lutz, Vanessa  Milan, Mehrnoosh  Sameki, Hanna  Wallach, Kathleen  Walker",,,,,https://semanticscholar.org/paper/5894d57ea49bd5c136ebefb1e6c3986555908ea0,,"We introduce Fairlearn, an open source toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems. Fairlearn has two components: an interactive visualization dashboard and unfairness mitigation algorithms. These components are designed to help with navigating trade-offs between fairness and model performance. We emphasize that prioritizing fairness in AI systems is a sociotechnical challenge. Because there are many complex sources of unfairness—some societal and some technical—it is not possible to fully “debias” a system or to guarantee fairness; the goal is to mitigate fairness-related harms as much as possible. As Fairlearn grows to include additional fairness metrics, unfairness mitigation algorithms, and visualization capabilities, we hope that it will be shaped by a diverse community of stakeholders, ranging from data scientists, developers, and business decision makers to the people whose lives may be affected by the predictions of AI systems.",,
Introduction to AI Fairness,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern.",Search,2020,2,"Yunfeng  Zhang, Rachel K. E. Bellamy, Moninder  Singh, Q. Vera Liao",CHI Extended Abstracts,,10.1145/3334480.3375059,https://doi.org/10.1145/3334480.3375059,https://semanticscholar.org/paper/d49f8d57240cd04dd66d91bf53c639497139ab3a,,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. Recently, the AI research community has proposed many methods to measure and mitigate unwanted biases, and developed open-source toolkits for developers to make fair AI. This course will cover the recent development in algorithmic fairness, including the many different definitions of fairness, their corresponding quantitative measurements, and ways to mitigate biases. This course is open to beginners and is designed for anyone interested in the topic of AI fairness.",,
Introduction to AI Fairness,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern.",Search,2021,,"Yunfeng  Zhang, Rachel K. E. Bellamy, Q. Vera Liao, Moninder  Singh",CHI Extended Abstracts,,10.1145/3411763.3444998,https://doi.org/10.1145/3411763.3444998,https://semanticscholar.org/paper/e24d854f822bebd531926eee518d4b2e1455e5de,,"Today, AI is used in many high-stakes decision-making applications in which fairness is an important concern. Already, there are many examples of AI being biased and making questionable and unfair decisions. Recently, the AI research community has proposed many methods to measure and mitigate unwanted biases, and developed open-source toolkits for developers to make fair AI. This course will cover the recent development in algorithmic fairness, including the many different definitions of fairness, their corresponding quantitative measurements, and ways to mitigate biases. This course is open to beginners and is designed for anyone interested in the topic of AI fairness.",,
Human-Centered Approaches to Fair and Responsible AI,The HCI community can contribute to AI responsibility and account for real-world contexts.,Search,2020,5,"Min Kyung Lee, Nina  Grgić-Hlača, Michael Carl Tschantz, Reuben  Binns, Adrian  Weller, Michelle  Carney, Kori  Inkpen",CHI Extended Abstracts,,10.1145/3334480.3375158,https://doi.org/10.1145/3334480.3375158,https://semanticscholar.org/paper/7e02e25869b60f23c15bff7ffd8d406ff3321fca,,"As AI changes the way decisions are made in organizations and governments, it is ever more important to ensure that these systems work according to values that diverse users and groups find important. Researchers have proposed numerous algorithmic techniques to formalize statistical fairness notions, but emerging work suggests that AI systems must account for the real-world contexts in which they will be embedded in order to actually work fairly. These findings call for an expanded research focus beyond statistical fairness to that which includes fundamental understandings of human use and the social impact of AI systems, a theme central to the HCI community. The HCI community can contribute novel understandings, methods, and techniques for incorporating human values and cultural norms into AI systems; address human biases in developing and using AI; and empower individual users and society to audit and control AI systems. Our goal is to bring together academic and industry researchers in the fields of HCI, ML and AI, and the social sciences to devise a cross-disciplinary research agenda for fair and responsible AI systems. This workshop will build on previous algorithmic fairness workshops at AI and ML conferences, map research and design opportunities for future innovations, and disseminate them in each community.",,
Algorithmic Fairness,Algorithms now touch on many aspects of our lives and must be fair and objective.,Search,2020,146,"Dana  Pessach, Erez  Shmueli",ArXiv,,,,https://semanticscholar.org/paper/27cd5a3eb55d2df2a4c06e96247b79f215516a67,,"An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence (AI) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop AI algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision-making may be inherently prone to unfairness, even when there is no intention for it. This paper presents an overview of the main concepts of identifying, measuring and improving algorithmic fairness when using AI algorithms. The paper begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, towards a better understanding of which mechanisms should be used in different scenarios. The paper then describes the most commonly used fairness-related datasets in this field. Finally, the paper ends by reviewing several emerging research sub-fields of algorithmic fairness.",,Review
Evidence-based explanation to promote fairness in AI systems,"People need to understand how AI is part of that decision, which is sensitive to fairness.",Search,2020,1,"Juliana Jansen Ferreira, Mateus de Souza Monteiro",ArXiv,,,,https://semanticscholar.org/paper/4a638bbe20e8af4becb91db93dae9bf78ea849ed,,"As Artificial Intelligence (AI) technology gets more intertwined with every system, people are using AI to make decisions on their everyday activities. In simple contexts, such as Netflix recommendations, or in more complex context like in judicial scenarios, AI is part of people's decisions. People make decisions and usually, they need to explain their decision to others or in some matter. It is particularly critical in contexts where human expertise is central to decision-making. In order to explain their decisions with AI support, people need to understand how AI is part of that decision. When considering the aspect of fairness, the role that AI has on a decision-making process becomes even more sensitive since it affects the fairness and the responsibility of those people making the ultimate decision. We have been exploring an evidence-based explanation design approach to 'tell the story of a decision'. In this position paper, we discuss our approach for AI systems using fairness sensitive cases in the literature.",,
Transparency Tools for Fairness in AI (Luskin),"Tools are useful for understanding various dimensions of bias, and that in practice the algorithms are effective in starkly reducing a given observed bias when tested on new data.",Search,2020,1,"Mingliang  Chen, Aria  Shahverdi, Sarah  Anderson, Se Yong Park, Justin  Zhang, Dana  Dachman-Soled, Kristin  Lauter, Min  Wu",ArXiv,,10.1007/978-3-030-58748-2_4,https://doi.org/10.1007/978-3-030-58748-2_4,https://semanticscholar.org/paper/03e9164acb7b1cdf9b8870ede9288c816c5cf7d9,http://arxiv.org/pdf/2007.04484,"We propose new tools for policy-makers to use when assessing and correcting fairness and bias in AI algorithms. The three tools are:

- A new definition of fairness called ""controlled fairness"" with respect to choices of protected features and filters. The definition provides a simple test of fairness of an algorithm with respect to a dataset. This notion of fairness is suitable in cases where fairness is prioritized over accuracy, such as in cases where there is no ""ground truth"" data, only data labeled with past decisions (which may have been biased).

- Algorithms for retraining a given classifier to achieve ""controlled fairness"" with respect to a choice of features and filters. Two algorithms are presented, implemented and tested. These algorithms require training two different models in two stages. We experiment with combinations of various types of models for the first and second stage and report on which combinations perform best in terms of fairness and accuracy.

- Algorithms for adjusting model parameters to achieve a notion of fairness called ""classification parity"". This notion of fairness is suitable in cases where accuracy is prioritized. Two algorithms are presented, one which assumes that protected features are accessible to the model during testing, and one which assumes protected features are not accessible during testing.

We evaluate our tools on three different publicly available datasets. We find that the tools are useful for understanding various dimensions of bias, and that in practice the algorithms are effective in starkly reducing a given observed bias when tested on new data.",,
Fair Representation for Safe Artificial Intelligence via Adversarial Learning of Unbiased Information Bottleneck,Representation learning can reduce the algorithmic bias in safe artificial intelligence,Search,2020,3,"Jin-Young  Kim, Sung-Bae  Cho",SafeAI@AAAI,,,,https://semanticscholar.org/paper/4577090bcea3ab3777760937f47127d8f5756776,,"Algorithmic bias indicates the discrimination caused by algorithms, which occurs with protected features such as gender and race. Even if we exclude a protected feature inducing the unfairness from the input data, the bias can still appear due to proxy discrimination through the dependency of other attributes and protected features. Several methods have been devised to reduce the bias, but it is not yet fully explored to identify the cause of this problem. In this paper, non-discriminated representation is formulated as a dual objective optimization problem of encoding data while obfuscating the information about the protected features in the data representation by exploiting the unbiased information bottleneck. Encoder learns data representation and discriminator judges whether there is information about the protected features in the data representation or not. They are trained simultaneously in adversarial fashion to achieve fair representation. Moreover, the algorithmic bias is analyzed in terms of bias-variance dilemma to reveal the cause of bias, so as to prove that the proposed method is effective for reducing the algorithmic bias in theory and experiments. Experiments with the well-known benchmark datasets such as Adults, Census, and COMPAS demonstrate the efficacy of the proposed method compared to other conventional techniques. Our method not only reduces the bias but also can use the latent representation in other classifiers (i.e., once a fair representation is learned, it can be used in various classifiers). We illustrate it by applying to the conventional machine learning models and visualizing the data representation with t-SNE algorithm.",,