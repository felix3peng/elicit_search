Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Fairness of AI for people with disabilities,Recent developments in machine learning-based artificial intelligence pose challenges of fairness for people with disabilities.,Search,2020,1,Jason J. G. White,ACM SIGACCESS Access. Comput.,,10.1145/3386296.3386299,https://doi.org/10.1145/3386296.3386299,https://semanticscholar.org/paper/e202ff07c45cf3d8d0fb2524c6f2413c4db9ae8e,,"There are several respects in which recent developments in machine learning-based artificial intelligence pose challenges of fairness for people with disabilities. In this paper, some of the central problems are identified, and briefly reviewed from a philosophical perspective motivated by a broad concern for social justice, emphasizing the role of ethical considerations in informing the problem analysis.",,
Workshop on AI fairness for people with disabilities,Artificial intelligence is increasingly being used in decision-making that directly impacts people's lives.,Search,2020,5,"Shari  Trewin, Meredith Ringel Morris, Stacy  Branham, Walter S. Lasecki, Shiri  Azenkot, Nicole  Bleuel, Phill  Jenkins, Jeffrey P. Bigham",ACM SIGACCESS Access. Comput.,,10.1145/3386296.3386297,https://doi.org/10.1145/3386296.3386297,https://semanticscholar.org/paper/0c3a31ed1fc3213ab601653b11f91f0e83a20d95,,"This year the ASSETS conference is hosting a workshop on AI Fairness for People with Disabilities the day before the main conference program begins. This workshop will bring together forty participants to discuss the practical, ethical, and legal ramifications of emerging AI-powered technologies for people with disabilities. We organized this workshop because artificial intelligence is increasingly being used in decision-making that directly impacts people's lives.",,
Considerations for AI fairness for people with disabilities,AI systems should offer opportunities to redress errors and for users to raise fairness concerns.,Search,2019,19,"Shari  Trewin, Sara  Basson, Michael  Muller, Stacy  Branham, Jutta  Treviranus, Daniel  Gruen, Daniel  Hebert, Natalia  Lyckowski, Erich  Manser",SIGAI,,10.1145/3362077.3362086,https://doi.org/10.1145/3362077.3362086,https://semanticscholar.org/paper/68d3fa028db42157d988d2b8ad7d495c157e14e1,,"In society today, people experiencing disability can face discrimination. As artificial intelligence solutions take on increasingly important roles in decision-making and interaction, they have the potential to impact fair treatment of people with disabilities in society both positively and negatively. We describe some of the opportunities and risks across four emerging AI application areas: employment, education, public safety, and healthcare, identified in a workshop with participants experiencing a range of disabilities. In many existing situations, non-AI solutions are already discriminatory, and introducing AI runs the risk of simply perpetuating and replicating these flaws. We next discuss strategies for supporting fairness in the context of disability throughout the AI development lifecycle. AI systems should be reviewed for potential impact on the user in their broader context of use. They should offer opportunities to redress errors, and for users and those impacted to raise fairness concerns. People with disabilities should be included when sourcing data to build models, and in testing, to create a more inclusive and robust system. Finally, we offer pointers into an established body of literature on human-centered design processes and philosophies that may assist AI and ML engineers in innovating algorithms that reduce harm and ultimately enhance the lives of people with disabilities.",,
Toward fairness in AI for people with disabilities SBG@a research roadmap,AI technologies can improve the lives of people with disabilities.,Search,2020,39,"Anhong  Guo, Ece  Kamar, Jennifer Wortman Vaughan, Hanna  Wallach, Meredith Ringel Morris",ACM SIGACCESS Access. Comput.,,10.1145/3386296.3386298,https://doi.org/10.1145/3386296.3386298,https://semanticscholar.org/paper/e74eb6147977d94ac5db4ff779e6d4e53feeed75,,"AI technologies have the potential to dramatically impact the lives of people with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for many state-of-the-art AI systems, such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing, or language prediction algorithms that can augment communication for people with speech or cognitive disabilities. However, widely deployed AI systems may not work properly for PWD, or worse, may actively discriminate against them. These considerations regarding fairness in AI for PWD have thus far received little attention. In this position paper, we identify potential areas of concern regarding how several AI technology categories may impact particular disability constituencies if care is not taken in their design, development, and testing. We intend for this risk assessment of how various classes of AI might interact with various classes of disability to provide a roadmap for future research that is needed to gather data, test these hypotheses, and build more inclusive algorithms.",,
AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias,Fairness is an increasingly important concern as machine learning models are used to support decision making.,Search,2019,113,"Rachel K. E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Jacquelyn A. Martino, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",IBM J. Res. Dev.,,10.1147/jrd.2019.2942287,https://doi.org/10.1147/jrd.2019.2942287,https://semanticscholar.org/paper/333671a5fbbf726f8819138f3670524ec0405726,,"Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This article introduces a new open-source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license (

https://github.com/ibm/aif360

). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms for use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience that provides a gentle introduction to the concepts and capabilities for line-of-business users, researchers, and developers to extend the toolkit with their new algorithms and improvements and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.",,
FAIR AI: A Conceptual Framework for Democratisation of 21st Century AI,New methods can automatically create interpretable neural network models without the help of AI-experts.,Search,2021,,Saman  Halgamuge,"2021 International Conference on Instrumentation, Control, and Automation (ICA)",,10.1109/ICA52848.2021.9625672,https://doi.org/10.1109/ICA52848.2021.9625672,https://semanticscholar.org/paper/cae45a30452713cacad1e5ec04adfb4a065f6a8e,,"Popular models of AI have two significant deficiencies: 1) they are mostly manually designed using the experience of AI-experts 2) they lack human interpretability, i.e., users cannot make sense of the functionality of neural network architectures either semantically/linguistically or mathematically. This lack of interpretability is a main inhibitor of broader use of 21st century AI, e.g., Deep Neural Networks (DNN). The dependence on AI experts to create AI hinders the democratisation of AI and therefore the accessibility to AI. Addressing these deficiencies would provide answers to some of the valid questions about traceability, accountability and the ability to integrate existing knowledge (scientific or linguistically articulated human experience) into the model. This keynote abstract addresses these two significant deficiencies that inhibit the democratisation of AI by developing new methods that can automatically create interpretable neural network models without the help of AI-experts. The proposed cross-fertilised innovation will have a profound impact on the society through the increased accessibility and trustworthiness of AI beneficial to almost all areas of sciences, engineering, and humanities.",,
"AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias",Fairness is an increasingly important concern as machine learning models are used to support decision making.,Search,2018,316,"Rachel K. E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Jacquelyn  Martino, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John T. Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",ArXiv,,,,https://semanticscholar.org/paper/c8541b1dc813f3a638d7acc79e5f972e77f3c5a7,,"Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license {this https URL). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms.

The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (this https URL) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.",,
Fairness in AI applications,"AI applications that have broad, social impact for many people have recently increased greatly in number.",Search,2021,,Cameron  Shelley,2021 IEEE International Symposium on Technology and Society (ISTAS),,10.1109/istas52410.2021.9629140,https://doi.org/10.1109/istas52410.2021.9629140,https://semanticscholar.org/paper/048a7b203bfa22550ddd01af10f40a538a1bb27d,,"Applications of Artificial Intelligence (AI) that have broad, social impact for many people have recently increased greatly in number. They will continue to increase in ubiquity and impact for some time to come. In conjunction with this increase, many scholars have studied the nature of these impacts, including problems of fairness. Here, fairness refers to conflicts of interest between social groups that result from the configuration of these AI systems. One focus of research has been to define these fairness problems and to quantify them in a way that lends itself to calculation of fair outcomes. The purpose of this presentation is to show that this issue of fairness in AI is consistent with fairness problems posed by technological design in general and that addressing these problems goes beyond what can be readily quantified and calculated. For example, many such problems may be best resolved by forms of public consultation. This point is clarified by presenting an analytical tool, the Fairness Impact Assessment, and examples from AI and elsewhere.",,
AI Fairness for People with Disabilities: Point of View,Disability information is highly sensitive and not always shared because of the potential for discrimination.,Search,2018,30,Shari  Trewin,ArXiv,,,,https://semanticscholar.org/paper/8fc60a7489b76641ceee5da9180a3ca76b18560d,,"We consider how fair treatment in society for people with disabilities might be impacted by the rise in the use of artificial intelligence, and especially machine learning methods. We argue that fairness for people with disabilities is different to fairness for other protected attributes such as age, gender or race. One major difference is the extreme diversity of ways disabilities manifest, and people adapt. Secondly, disability information is highly sensitive and not always shared, precisely because of the potential for discrimination. Given these differences, we explore definitions of fairness and how well they work in the disability space. Finally, we suggest ways of approaching fairness for people with disabilities in AI applications.",,
Fairness via AI: Bias Reduction in Medical Information,AI can reduce bias in medical information to improve patient outcomes and wellbeing.,Search,2021,,"Shiri  Dori-Hacohen, Roberto  Montenegro, Fabricio  Murai, Scott A. Hale, Keen  Sung, Michela  Blain, Jennifer  Edwards-Johnson",ArXiv,,,,https://semanticscholar.org/paper/c2f5ef5e93cabaf82338731a3ad5d48f1e803054,,"Most Fairness in AI research focuses on exposing biases in AI systems. A broader lens on fairness reveals that AI can serve a greater aspiration: rooting out societal inequities from their source. Specifically, we focus on inequities in health information, and aim to reduce bias in that domain using AI. The AI algorithms under the hood of search engines and social media, many of which are based on recommender systems, have an outsized impact on the quality of medical and health information online. Therefore, embedding bias detection and reduction into these recommender systems serving up medical and health content online could have an outsized positive impact on patient outcomes and wellbeing. In this position paper, we offer the following contributions: (1) we propose a novel framework of Fairness via AI, inspired by insights from medical education, sociology and antiracism; (2) we define a new term, bisinformation, which is related to, but distinct from, misinformation, and encourage researchers to study it; (3) we propose using AI to study, detect and mitigate biased, harmful, and/or false health information that disproportionately hurts minority groups in society; and (4) we suggest several pillars and pose several open problems in order to seed inquiry in this new space.While part (3) of this work specifically focuses on the health domain, the fundamental computer science advances and contributions stemming from research efforts in bias reduction and Fairness via AI have broad implications in all areas of society.",,
Artificial intelligence fairness in the context of accessibility research on intelligent systems for people who are deaf or hard of hearing,Artificial intelligence fairness for people with disabilities requires including data from people with disabilities in training sets.,Search,2020,9,"Sushant  Kafle, Abraham  Glasser, Sedeeq  Al-khazraji, Larwan  Berke, Matthew  Seita, Matt  Huenerfauth",ACM SIGACCESS Access. Comput.,,10.1145/3386296.3386300,https://doi.org/10.1145/3386296.3386300,https://semanticscholar.org/paper/e3f537503b645b6ae51a1488d0188a2d9bfd5210,http://arxiv.org/pdf/1908.10414,"We discuss issues of Artificial Intelligence (AI) fairness for people with disabilities, with examples drawn from our research on HCI for AI-based systems for people who are Deaf or Hard of Hearing (DHH). In particular, we discuss the need for inclusion of data from people with disabilities in training sets, the lack of interpretability of AI systems, ethical responsibilities of access technology researchers and companies, the need for appropriate evaluation metrics for AI-based access technologies (to determine if they are ready to be deployed and if they can be trusted by users), and the ways in which AI systems influence human behavior and influence the set of abilities needed by users to successfully interact with computing systems.",,
Workshop on AI fairness for people with disabilities,AI fairness for people with disabilities is a workshop that will be hosted at the ASSETS conference.,Search,2020,5,"TrewinShari, MorrisMeredith  Ringel, BranhamStacy, S  LaseckiWalter, AzenkotShiri, BleuelNicole, JenkinsPhill, P  BighamJeffrey",,,10.1145/3386296.3386297,https://doi.org/10.1145/3386296.3386297,https://semanticscholar.org/paper/f977a50ffc82f00a6f999f92b5cccdf797194372,,This year the ASSETS conference is hosting a workshop on AI Fairness for People with Disabilities the day before the main conference program begins. This workshop will bring together forty particip...,,
Fairlearn: A toolkit for assessing and improving fairness in AI,Fairlearn is a toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems.,Search,2020,48,"Sarah  Bird, Miro  Dudík, Richard  Edgar, Brandon  Horn, Roman  Lutz, Vanessa  Milan, Mehrnoosh  Sameki, Hanna  Wallach, Kathleen  Walker",,,,,https://semanticscholar.org/paper/5894d57ea49bd5c136ebefb1e6c3986555908ea0,,"We introduce Fairlearn, an open source toolkit that empowers data scientists and developers to assess and improve the fairness of their AI systems. Fairlearn has two components: an interactive visualization dashboard and unfairness mitigation algorithms. These components are designed to help with navigating trade-offs between fairness and model performance. We emphasize that prioritizing fairness in AI systems is a sociotechnical challenge. Because there are many complex sources of unfairness—some societal and some technical—it is not possible to fully “debias” a system or to guarantee fairness; the goal is to mitigate fairness-related harms as much as possible. As Fairlearn grows to include additional fairness metrics, unfairness mitigation algorithms, and visualization capabilities, we hope that it will be shaped by a diverse community of stakeholders, ranging from data scientists, developers, and business decision makers to the people whose lives may be affected by the predictions of AI systems.",,
Design Methods for Artificial Intelligence Fairness and Transparency,"Fairness and transparency in artificial intelligence continue to become more prevalent as topics for research, design and development.",Search,2021,,"Simone  Stumpf, Lorenzo  Strappelli, Subeida  Ahmed, Yuri  Nakao, Aisha  Naseer, Giulia Del Gamba, Daniele  Regoli",IUI Workshops,,,,https://semanticscholar.org/paper/baf8cb94908738c3b2d040a6b44d812e5ca80396,,"Fairness and transparency in artificial intelligence (AI) continue to become more prevalent as topics for research, design and development. General principles and guidelines for designing ethical and responsible AI systems have been proposed, yet there is a lack of design methods for these kinds of systems. In this paper, we present CoFAIR, a novel method to design user interfaces for exploring fairness, consisting of series of co-design workshops, and wider evaluation. This method can be readily applied in practice by researchers, designers and developers to create responsible and ethical AI systems.",,
Algorithm Fairness in AI for Medicine and Healthcare,"Current issues in healthcare show enormous inequalities in how patients are diagnosed, given treatments, and billed for healthcare costs.",Search,2021,2,"Richard J. Chen, Tiffany Y. Chen, Jana  Lipkova, Judy J. Wang, Drew F.K. Williamson, Ming Y. Lu, Sharifa  Sahai, Faisal  Mahmood",ArXiv,,,,https://semanticscholar.org/paper/5555a00450b7aa56e8e2e62e0d0d69f86a885fa4,,"In the current development and deployment of many artificial intelligence (AI) systems in healthcare, algorithm fairness is a challenging problem in delivering equitable care. Recent evaluation of AI models stratified across race sub-populations have revealed enormous inequalities in how patients are diagnosed, given treatments, and billed for healthcare costs. In this perspective article, we summarize the intersectional field of fairness in machine learning through the context of current issues in healthcare, outline how algorithmic biases (e.g. image acquisition, genetic variation, intra-observer labeling variability) arise in current clinical workflows and their resulting healthcare disparities. Lastly, we also review emerging strategies for mitigating bias via decentralized learning, disentanglement, and model explainability.",,Review
How Could Equality and Data Protection Law Shape AI Fairness for People with Disabilities?,"There's a need for a distinctive approach to AI fairness that is fundamentally different to that used for other protected characteristics, due to the different ways in which discrimination and data protection law applies in respect of Disability.",Search,2021,,"Reuben  Binns, Reuben  Kirkham",ACM Trans. Access. Comput.,,10.1145/3473673,https://doi.org/10.1145/3473673,https://semanticscholar.org/paper/15ff6a8477420d80e9625c3e379baea83e169bce,http://arxiv.org/pdf/2107.05704,"This article examines the concept of ‘AI fairness’ for people with disabilities from the perspective of data protection and equality law. This examination demonstrates that there is a need for a distinctive approach to AI fairness that is fundamentally different to that used for other protected characteristics, due to the different ways in which discrimination and data protection law applies in respect of Disability. We articulate this new agenda for AI fairness for people with disabilities, explaining how combining data protection and equality law creates new opportunities for disabled people's organisations and assistive technology researchers alike to shape the use of AI, as well as to challenge potential harmful uses.",,