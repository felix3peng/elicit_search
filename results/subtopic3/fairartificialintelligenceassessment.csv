Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Fairness Assessment for Artificial Intelligence in Financial Industry,Fairness evaluation is a key component of AI governance.,Search,2019,7,"Yukun  Zhang, Longsheng  Zhou",ArXiv,,,,https://semanticscholar.org/paper/9b0f1ec29596a1e5d41366235322f7d1d68be9e4,,"Artificial Intelligence (AI) is an important driving force for the development and transformation of the financial industry. However, with the fast-evolving AI technology and application, unintentional bias, insufficient model validation, immature contingency plan and other underestimated threats may expose the company to operational and reputational risks. In this paper, we focus on fairness evaluation, one of the key components of AI Governance, through a quantitative lens. Statistical methods are reviewed for imbalanced data treatment and bias mitigation. These methods and fairness evaluation metrics are then applied to a credit card default payment example.",,
Adapting SQuaRE for Quality Assessment of Artificial Intelligence Systems,Many of existing principles and approaches to traditional software systems do not work effectively for the system behavior obtained by training not by logical design.,Search,2019,8,"Hiroshi  Kuwajima, Fuyuki  Ishikawa",2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),,10.1109/ISSREW.2019.00035,https://doi.org/10.1109/ISSREW.2019.00035,https://semanticscholar.org/paper/7f4cb5546bab43fa01988385fb89546e57485b69,http://arxiv.org/pdf/1908.02134,"More and more software practitioners are tackling towards industrial applications of artificial intelligence (AI) systems, especially those based on machine learning (ML). However, many of existing principles and approaches to traditional software systems do not work effectively for the system behavior obtained by training not by logical design. In addition, unique kinds of requirements are emerging such as fairness and explainability. To provide clear guidance to understand and tackle these difficulties, we present an analysis on what quality concepts we should evaluate for AI systems. We base our discussion on ISO/IEC 25000 series, known as SQuaRE, and identify how it should be adapted for the unique nature of ML and Ethics guidelines for trustworthy AI from European Commission. We thus provide holistic insights for quality of AI systems by incorporating the ML nature and AI ethics to the traditional software quality concepts.",,
"Fairness, Accountability and Transparency in Artificial Intelligence: A Case Study of Logical Predictive Models",Class-contrastive counterfactual statements can explain the interpretability of logical machine learning models.,Search,2019,4,Kacper  Sokol,AIES,,10.1145/3306618.3314316,https://doi.org/10.1145/3306618.3314316,https://semanticscholar.org/paper/f031fe645728d6fd36a9a36232cf19a846e9db73,,"Machine learning -- the part of artificial intelligence aimed at eliciting knowledge from data and automated decision making without explicit instructions -- is making great strides, with new algorithms being invented every day. These algorithms find myriads of applications, but their ubiquity often comes at the expense of limited interpretability, hidden biases and unexpected vulnerabilities. Whenever one of these factors is a priority, the learning algorithm of choice is often a method considered to be inherently interpretable, e.g. logical models such as decision trees. In my research I challenge this assumption and highlight (quite common) cases when the assumed interpretability fails to deliver. To restore interpretability of logical machine learning models (decision trees and their ensembles in particular) I propose to explain them with class-contrastive counterfactual statements, which are a very common type of explanation in human interactions, well-grounded in social science research. To evaluate transparency of such models I collate explainability desiderata that can be used to systematically assess and compare such methods as an addition to user studies. Given contrastive explanations, I investigate their influence on the model's security, in particular gaming and stealing the model. Finally, I evaluate model fairness, where I am interested in choosing the most fair model among all the models with equal performance.",,
Fairness Score and Process Standardization: Framework for Fairness Certification in Artificial Intelligence Systems,A Fairness Score and a Standard Operating Procedure (SOP) for issuing Fairness Certification would improve the trustworthiness of AI systems.,Search,2022,,"Avinash  Agarwal, Harsh  Agarwal, Nihaarika  Agarwal",ArXiv,,,,https://semanticscholar.org/paper/64c2fa6b3c161fe2a5ae20f03ff77c3250027e10,,"Decisions made by various Artificial Intelligence (AI) systems greatly influence our day-to-day lives. With the increasing use of AI systems, it becomes crucial to know that they are fair, identify the underlying biases in their decision-making, and create a standardized framework to ascertain their fairness. In this paper, we propose a novel Fairness Score to measure the fairness of a data-driven AI system and a Standard Operating Procedure (SOP) for issuing Fairness Certification for such systems. Fairness Score and audit process standardization will ensure quality, reduce ambiguity, enable comparison and improve the trustworthiness of the AI systems. It will also provide a framework to operationalise the concept of fairness and facilitate the commercial deployment of such systems. Furthermore, a Fairness Certificate issued by a designated third-party auditing agency following the standardized process would boost the conviction of the organizations in the AI systems that they intend to deploy. The Bias Index proposed in this paper also reveals comparative bias amongst the various protected attributes within the dataset. To substantiate the proposed framework, we iteratively train a model on biased and unbiased data using multiple datasets and check that the Fairness Score and the proposed process correctly identify the biases and judge the fairness.",,
Towards Fairness Certification in Artificial Intelligence,Artificial intelligence is supportive in many decision-making scenarios with major impact on society.,Search,2021,1,"Tatiana  Tommasi, Silvia  Bucci, Barbara  Caputo, Pietro  Asinari",ArXiv,,,,https://semanticscholar.org/paper/157246efaa0a667383cd78da0599231687368e0e,,"Thanks to the great progress of machine learning in the last years, several Artificial Intelligence (AI) techniques have been increasingly moving from the controlled research laboratory settings to our everyday life. The most simple examples are the spam filters that keep our email account in order, face detectors that help us when taking a portrait picture, online recommender systems that suggest which movie and clothing we might like, or interactive maps that navigate us towards our vacation home. Artificial intelligence is clearly supportive in many decision-making scenarios, but when it comes to sensitive areas such as health care, hiring policies, education, banking or justice, with major impact on individuals and society, it becomes crucial to establish guidelines on how to design, develop, deploy and monitor this technology. Indeed the decision rules elaborated by machine learning models are data-driven and there are multiple ways in which discriminatory biases can seep into data. Algorithms trained on those data incur the risk of amplifying prejudices and societal stereotypes by over associating protected attributes such as gender, ethnicity or disabilities with the prediction task.",,
Quality assessment standards in artificial intelligence diagnostic accuracy systematic reviews: a meta-research study,Artificial intelligence-based diagnostic systems are increasingly recognized as robust solutions in healthcare delivery pathways.,Search,2022,,"Shruti  Jayakumar, Viknesh  Sounderajah, Pasha  Normahani, Leanne  Harling, Sheraz R. Markar, Hutan  Ashrafian, Ara  Darzi",npj Digital Medicine,,10.1038/s41746-021-00544-y,https://doi.org/10.1038/s41746-021-00544-y,https://semanticscholar.org/paper/ffce1ad9419e9742477f36f7fb9d427bc78164da,https://www.nature.com/articles/s41746-021-00544-y.pdf,"Artificial intelligence (AI) centred diagnostic systems are increasingly recognised as robust solutions in healthcare delivery pathways. In turn, there has been a concurrent rise in secondary research studies regarding these technologies in order to influence key clinical and policymaking decisions. It is therefore essential that these studies accurately appraise methodological quality and risk of bias within shortlisted trials and reports. In order to assess whether this critical step is performed, we undertook a meta-research study evaluating adherence to the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS-2) tool within AI diagnostic accuracy systematic reviews. A literature search was conducted on all studies published from 2000 to December 2020. Of 50 included reviews, 36 performed the quality assessment, of which 27 utilised the QUADAS-2 tool. Bias was reported across all four domains of QUADAS-2. Two hundred forty-three of 423 studies (57.5%) across all systematic reviews utilising QUADAS-2 reported a high or unclear risk of bias in the patient selection domain, 110 (26%) reported a high or unclear risk of bias in the index test domain, 121 (28.6%) in the reference standard domain and 157 (37.1%) in the flow and timing domain. This study demonstrates the incomplete uptake of quality assessment tools in reviews of AI-based diagnostic accuracy studies and highlights inconsistent reporting across all domains of quality assessment. Poor standards of reporting act as barriers to clinical implementation. The creation of an AI-specific extension for quality assessment tools of diagnostic accuracy AI studies may facilitate the safe translation of AI tools into clinical practice.",,Review
Artificial intelligence: how it works and criteria for assessment,"Artificial intelligence technologies will have an impact on medical imaging, including ophthalmology, radiology, and dermatology.",Search,2021,,"Irena L. Shlivko, Oxana Ye. Garanina, Irina A. Klemenova, Kseniia A. Uskova, Anna M. Mironycheva, Veniamin I. Dardyk, Viktor N. Laskov",Consilium Medicum,,10.26442/20751753.2021.8.201148,https://doi.org/10.26442/20751753.2021.8.201148,https://semanticscholar.org/paper/622a4773fd895416a6cdce3b81cb087b27336a66,https://consilium.orscience.ru/2075-1753/article/download/97097/71487,"Artificial intelligence is a term used to describe computer technology in the modeling of intelligent behavior and critical thinking comparable to that of humans. To date, some of the first areas of medicine to be influenced by advances in artificial intelligence technologies will be those most dependent on imaging. These include ophthalmology, radiology, and dermatology. In connection with the emergence of numerous medical applications, scientists have formulated criteria for their assessment. This list included: clinical validation, regular application updates, functional focus, cost, availability of an information block for specialists and patients, compliance with the conditions of government regulation, and registration. One of the applications that meet all the requirements is the ProRodinki software package, developed for use by patients and specialists in the Russian Federation. Taking into account a widespread and rapidly developing competitive environment, it is necessary to soberly treat the resources of such applications, not exaggerating their capabilities and not considering them as a substitute for a specialist.",,
Assessing Artificial Intelligence,Artificial intelligence has an impact on the moral agency of humans.,Search,2020,,Susan K. Wood,,,10.3138/tjt-2020-0031,https://doi.org/10.3138/tjt-2020-0031,https://semanticscholar.org/paper/776df15b468c347237e4fc05fea46a98e2c6b99b,,"A contribution to a panel on artificial intelligence at Trinity College on January 21, 2020, this essay assesses artificial intelligence in terms of moral agency, particularly the impact of the dis...",,
Artificial intelligence in educational assessment: 'Breakthrough? Or buncombe and ballyhoo?',Artificial intelligence is at the heart of modern society with computers now capable of making process decisions in many spheres of human activity.,Search,2021,3,"John  Gardner, Michael  O'Leary, Li  Yuan",J. Comput. Assist. Learn.,,10.1111/jcal.12577,https://doi.org/10.1111/jcal.12577,https://semanticscholar.org/paper/26277649814e508d87202444a852fb97e25c3632,https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/jcal.12577,"Correspondence Michael O'Leary, Centre for Assessment Research, Policy and Practice in Education (CARPE), Dublin City University, Dublin, Ireland. Email: michael.oleary@dcu.ie Abstract Artificial Intelligence is at the heart of modern society with computers now capable of making process decisions in many spheres of human activity. In education, there has been intensive growth in systems that make formal and informal learning an anytime, anywhere activity for billions of people through online open educational resources and massive online open courses. Moreover, new developments in Artificial Intelligencerelated educational assessment are attracting increasing interest as means of improving assessment efficacy and validity, with much attention focusing on the analysis of the large volumes of process data being captured from digital assessment contexts. In evaluating the state of play of Artificial Intelligence in formative and summative educational assessment, this paper offers a critical perspective on the two core applications: automated essay scoring systems and computerized adaptive tests, along with the Big Data analysis approaches to machine learning that underpin them.",,
Explaining how your AI system is fair,Sharing reasons and principles for AI fairness decisions will help maintain confidence in AI systems.,Search,2021,,"Boris  Ruf, Marcin  Detyniecki",ArXiv,,,,https://semanticscholar.org/paper/1271e86aad602cfa09727dc98bae31c19ce77cc6,,"Copyright held by the owner/author(s). CHI’21,, May 8–13, 2021, Online Virtual Conference (originally Yokohama, Japan) ACM 978-1-4503-6819-3/20/04. https://doi.org/10.1145/3334480.XXXXXXX Abstract To implement fair machine learning in a sustainable way, choosing the right fairness objective is key. Since fairness is a concept of justice which comes in various, sometimes conflicting definitions, this is not a trivial task though. The most appropriate fairness definition for an artificial intelligence (AI) system is a matter of ethical standards and legal requirements, and the right choice depends on the particular use case and its context. In this position paper, we propose to use a decision tree as means to explain and justify the implemented kind of fairness to the end users. Such a structure would first of all support AI practitioners in mapping ethical principles to fairness definitions for a concrete application and therefore make the selection a straightforward and transparent process. However, this approach would also help document the reasoning behind the decision making. Due to the general complexity of the topic of fairness in AI, we argue that specifying ""fairness"" for a given use case is the best way forward to maintain confidence in AI systems. In this case, this could be achieved by sharing the reasons and principles expressed during the decision making process with the broader audience.",,
"Assessing the Attitude Towards Artificial Intelligence: Introduction of a Short Measure in German, Chinese, and English Language","Participants from Germany and China were additionally asked about their willingness to interact with/use self-driving cars, Siri, Alexa, the social robot Pepper, and the humanoid robot Erica.",Search,2021,4,"Cornelia  Sindermann, Peng  Sha, Min  Zhou, Jennifer  Wernicke, Helena S. Schmitt, Mei  Li, Rayna  Sariyska, Maria  Stavrou, Benjamin  Becker, Christian  Montag",Künstliche Intell.,,10.1007/S13218-020-00689-0,https://doi.org/10.1007/S13218-020-00689-0,https://semanticscholar.org/paper/721012c78aa41f26deb2ed880a4ce9f9c45f799a,https://link.springer.com/content/pdf/10.1007/s13218-020-00689-0.pdf,"In the context of (digital) human–machine interaction, people are increasingly dealing with artificial intelligence in everyday life. Through this, we observe humans who embrace technological advances with a positive attitude. Others, however, are particularly sceptical and claim to foresee substantial problems arising from such uses of technology. The aim of the present study was to introduce a short measure to assess the Attitude Towards Artificial Intelligence (ATAI scale) in the German, Chinese, and English languages. Participants from Germany (N = 461; 345 females), China (N = 413; 145 females), and the UK (N = 84; 65 females) completed the ATAI scale, for which the factorial structure was tested and compared between the samples. Participants from Germany and China were additionally asked about their willingness to interact with/use self-driving cars, Siri, Alexa, the social robot Pepper, and the humanoid robot Erica, which are representatives of popular artificial intelligence products. The results showed that the five-item ATAI scale comprises two negatively associated factors assessing (1) acceptance and (2) fear of artificial intelligence. The factor structure was found to be similar across the German, Chinese, and UK samples. Additionally, the ATAI scale was validated, as the items on the willingness to use specific artificial intelligence products were positively associated with the ATAI Acceptance scale and negatively with the ATAI Fear scale, in both the German and Chinese samples. In conclusion we introduce a short, reliable, and valid measure on the attitude towards artificial intelligence in German, Chinese, and English language.",,
A century-long commitment to assessing artificial intelligence and its impact on society,The media reports on the technological advances in AI and its societal implications.,Search,2018,15,"Barbara J. Grosz, Peter  Stone",Commun. ACM,,10.1145/3198470,https://doi.org/10.1145/3198470,https://semanticscholar.org/paper/b4bf49605f0c4c250c95a1da58b70b0e93a248b2,https://dash.harvard.edu/bitstream/1/37877417/1/GroszStone_CACM2018.pdf,A series of reports promises the general public a technologically accurate view of the state of AI and its societal implications.,,
Think Your Artificial Intelligence Software Is Fair? Think Again,"Machine-learning software is also biased, sometimes in similar ways to humans, often in different ways.",Search,2019,14,"Rachel K.E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John  Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",IEEE Software,,10.1109/MS.2019.2908514,https://doi.org/10.1109/MS.2019.2908514,https://semanticscholar.org/paper/4d60f78b44f34a67a5ce6316d1c45c90a912db44,,"Today, machine-learning software is used to help make decisions that affect people's lives. Some people believe that the application of such software results in fairer decisions because, unlike humans, machine-learning software generates models that are not biased. Think again. Machine-learning software is also biased, sometimes in similar ways to humans, often in different ways. While fair model- assisted decision making involves more than the application of unbiased models-consideration of application context, specifics of the decisions being made, resolution of conflicting stakeholder viewpoints, and so forth-mitigating bias from machine-learning software is important and possible but difficult and too often ignored.",,
Towards General Evaluation of Intelligent Systems: Lessons Learned from Reproducing AIQ Test Results,The Algorithmic Intelligence Quotient test is reliable but has several limits as a general artificial intelligence evaluation method.,Search,2018,1,Ondrej  Vadinský,J. Artif. Gen. Intell.,,10.2478/jagi-2018-0001,https://doi.org/10.2478/jagi-2018-0001,https://semanticscholar.org/paper/6b8290bed1184676833f125c15f4e8a7f08695e0,https://content.sciendo.com/downloadpdf/journals/jagi/9/1/article-p1.pdf,"Abstract This paper attempts to replicate the results of evaluating several artificial agents using the Algorithmic Intelligence Quotient test originally reported by Legg and Veness. Three experiments were conducted: One using default settings, one in which the action space was varied and one in which the observation space was varied. While the performance of freq, Q0, Qλ, and HLQλ corresponded well with the original results, the resulting values differed, when using MC-AIXI. Varying the observation space seems to have no qualitative impact on the results as reported, while (contrary to the original results) varying the action space seems to have some impact. An analysis of the impact of modifying parameters of MC-AIXI on its performance in the default settings was carried out with the help of data mining techniques used to identifying highly performing configurations. Overall, the Algorithmic Intelligence Quotient test seems to be reliable, however as a general artificial intelligence evaluation method it has several limits. The test is dependent on the chosen reference machine and also sensitive to changes to its settings. It brings out some differences among agents, however, since they are limited in size, the test setting may not yet be sufficiently complex. A demanding parameter sweep is needed to thoroughly evaluate configurable agents that, together with the test format, further highlights computational requirements of an agent. These and other issues are discussed in the paper along with proposals suggesting how to alleviate them. An implementation of some of the proposals is also demonstrated.",,
An instrument to evaluate the maturity of bias governance capability in artificial intelligence projects,"Bias is already prevalent in AI datasets and algorithms, which is considered to be unethical, unsustainable, and challenging to manage.",Search,2019,3,"Daphne  Coates, Andrew  Martin",IBM J. Res. Dev.,,10.1147/JRD.2019.2915062,https://doi.org/10.1147/JRD.2019.2915062,https://semanticscholar.org/paper/670e2331f78028f14ccfc94399eba72eccd53642,,"Artificial intelligence (AI) promises unprecedented contributions to both business and society, attracting a surge of interest from many organizations. However, there is evidence that bias is already prevalent in AI datasets and algorithms, which, albeit unintended, is considered to be unethical, suboptimal, unsustainable, and challenging to manage. It is believed that the governance of data and algorithmic bias must be deeply embedded in the values, mindsets, and procedures of AI software development teams, but currently there is a paucity of actionable mechanisms to help. In this paper, we describe a maturity framework based on ethical principles and best practices, which can be used to evaluate an organization's capability to govern bias. We also design, construct, validate, and test an original instrument for operationalizing the framework, which considers both technical and organizational aspects. The instrument has been developed and validated through a two-phase study involving field experts and academics. The framework and instrument are presented for ongoing evolution and utilization.",,
Initial validation of the general attitudes towards Artificial Intelligence Scale,People hold mixed views of Artificial Intelligence.,Search,2020,15,"Astrid  Schepman, Paul  Rodway",Computers in Human Behavior Reports,,10.1016/j.chbr.2020.100014,https://doi.org/10.1016/j.chbr.2020.100014,https://semanticscholar.org/paper/81b0ed73ad36c6f884566e32df7ca8dea8f23b42,https://chesterrep.openrepository.com/bitstream/10034/623459/4/SchepmanRodway_ArtificialIntelligence_AAM.pdf,"Abstract

A new General Attitudes towards Artificial Intelligence Scale (GAAIS) was developed. The scale underwent initial statistical validation via Exploratory Factor Analysis, which identified positive and negative subscales. Both subscales captured emotions in line with their valence. In addition, the positive subscale reflected societal and personal utility, whereas the negative subscale reflected concerns. The scale showed good psychometric indices and convergent and discriminant validity against existing measures. To cross-validate general attitudes with attitudes towards specific instances of AI applications, summaries of tasks accomplished by specific applications of Artificial Intelligence were sourced from newspaper articles. These were rated for comfortableness and perceived capability. Comfortableness with specific applications was a strong predictor of general attitudes as measured by the GAAIS, but perceived capability was a weaker predictor. Participants viewed AI applications involving big data (e.g. astronomy, law, pharmacology) positively, but viewed applications for tasks involving human judgement, (e.g. medical treatment, psychological counselling) negatively. Applications with a strong ethical dimension led to stronger discomfort than their rated capabilities would predict. The survey data suggested that people held mixed views of AI. The initially validated two-factor GAAIS to measure General Attitudes towards Artificial Intelligence is included in the Appendix.",,