Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
On Safety Assessment of Artificial Intelligence,Safety assessment is required for AI in safety-related applications.,Search,2020,5,"Jens  Braband, Hendrik  Schäbe",ArXiv,,10.21683/1729-2646-2020-20-4-25-34,https://doi.org/10.21683/1729-2646-2020-20-4-25-34,https://semanticscholar.org/paper/99cadaf09e78f298ebd920bdb8a6ae39025523e4,https://www.dependability.ru/jour/article/download/392/646,"In this paper we discuss how systems with Artificial Intelligence (AI) can undergo safety assessment. This is relevant, if AI is used in safety related applications. Taking a deeper look into AI models, we show, that many models of artificial intelligence, in particular machine learning, are statistical models. Safety assessment would then have t o concentrate on the model that is used in AI, besides the normal assessment procedure. Part of the budget of dangerous random failures for the relevant safety integrity level needs to be used for the probabilistic faulty behavior of the AI system. We demonstrate our thoughts with a simple example and propose a research challenge that may be decisive for the use of AI in safety related systems.",,
AI Safety Gridworlds,A2C and Rainbow do not solve the AI safety problems well.,Search,2017,141,"Jan  Leike, Miljan  Martic, Victoria  Krakovna, Pedro A. Ortega, Tom  Everitt, Andrew  Lefrancq, Laurent  Orseau, Shane  Legg",ArXiv,,,,https://semanticscholar.org/paper/d09bec5af4eef5038e48b26b6c14098f95997114,,"We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.",,
Evaluation Schemes for Safe AGIs,A safe AGI can be distilled down to 3 essential test cases.,Search,2014,,Deepak Justin Nath,AAAI Spring Symposia,,,,https://semanticscholar.org/paper/4d037fe1836bd5691ba4ffca14ef531ef8d0263f,,"Systems or agents with Artificial General Intelligence (AGI) are created to fulfill a particular or general goal. The agents are driven to achieve these mandated goals. This means that anything or person that stands in the way of the AGI reaching its goal is in danger as the AGI will learn that eliminating or removing the hindrance to its goal is an effective and necessary action to reach its goal. This paper proposes that evaluation scheme for safe AGI (Artificial General Intelligence) can be distilled down to 3 essential test cases. The paper explores various drives needed for an AGI system and how these drives generate actions and action sequences leading to AGI goals; the relationship between goals, drives, desires and motivation and a hypothetical abstraction levels in an AGI is mentioned here. The paper describes an AGI system with its goals and drives along with the various other aspect of this AGI system to provide arguments in favor of the 3 essential test cases.",,
AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation,AI2 can be used to certify the effectiveness of state-of-the-art defenses for neural networks.,Search,2018,465,"Timon  Gehr, Matthew  Mirman, Dana  Drachsler-Cohen, Petar  Tsankov, Swarat  Chaudhuri, Martin T. Vechev",2018 IEEE Symposium on Security and Privacy (SP),,10.1109/SP.2018.00058,https://doi.org/10.1109/SP.2018.00058,https://semanticscholar.org/paper/d21fde0f55ee0285c66334d37b8920c867959784,https://ieeexplore.ieee.org/ielx7/8418581/8418583/08418593.pdf,"We present AI2, the first sound and scalable analyzer for deep neural networks. Based on overapproximation, AI2 can automatically prove safety properties (e.g., robustness) of realistic neural networks (e.g., convolutional neural networks). The key insight behind AI2 is to phrase reasoning about safety and robustness of neural networks in terms of classic abstract interpretation, enabling us to leverage decades of advances in that area. Concretely, we introduce abstract transformers that capture the behavior of fully connected and convolutional neural network layers with rectified linear unit activations (ReLU), as well as max pooling layers. This allows us to handle real-world neural networks, which are often built out of those types of layers. We present a complete implementation of AI2 together with an extensive evaluation on 20 neural networks. Our results demonstrate that: (i) AI2 is precise enough to prove useful specifications (e.g., robustness), (ii) AI2 can be used to certify the effectiveness of state-of-the-art defenses for neural networks, (iii) AI2 is significantly faster than existing analyzers based on symbolic analysis, which often take hours to verify simple fully connected networks, and (iv) AI2 can handle deep convolutional networks, which are beyond the reach of existing methods.",,
Unifying Principles and Metrics for Safe and Assistive AI,"AI safety focuses on humans safely instructing and controlling AI systems, while AI impact on work focuses on the impact of AI on humans who may be unable to do so.",Search,2021,4,Siddharth  Srivastava,AAAI,,,,https://semanticscholar.org/paper/967cba5849f93869655af8d3d8fda9b0a3304a1d,,"The prevalence and success of AI applications have been tempered by concerns about the controllability of AI systems about AI’s impact on the future of work. These concerns reflect two aspects of a central question: how would humans work with AI systems? While research on AI safety focuses on designing AI systems that allow humans to safely instruct and control AI systems, research on AI and the future of work focuses on the impact of AI on humans who may be unable to do so. This Blue Sky Ideas paper proposes a unifying set of declarative principles that enable a more uniform evaluation of arbitrary AI systems along multiple dimensions of the extent to which they are suitable for use by specific classes of human operators. It leverages recent AI research and the unique strengths of the field to develop human-centric principles for AI systems that address the concerns noted above.",,
Safe AI for CPS (Invited Paper),Combining formal proofs with reinforcement learning can help develop safe AI for cyber-physical systems.,Search,2018,5,"Nathan  Fulton, André  Platzer",2018 IEEE International Test Conference (ITC),,10.1109/TEST.2018.8624774,https://doi.org/10.1109/TEST.2018.8624774,https://semanticscholar.org/paper/03fce03060f6110b2772958f58539320bab519ed,,"Autonomous cyber-physical systems-such as self-driving cars and autonomous drones-often leverage artificial intelligence and machine learning algorithms to act well in open environments. Although testing plays an important role in ensuring safety and robustness, modern autonomous systems have grown so complex that achieving safety via testing alone is intractable. Formal verification reduces this testing burden by ruling out large classes of errant behavior at design time. This paper reviews recent work toward developing formal methods for cyber-physical systems that use AI for planning and control by combining the rigor of formal proofs with the flexibility of reinforcement learning.",,Review
Decision Support for Safe AI Design,Simulations do not have to be accurate predictions of the future to be useful for AI evaluation.,Search,2012,4,Bill  Hibbard,AGI,,10.1007/978-3-642-35506-6_13,https://doi.org/10.1007/978-3-642-35506-6_13,https://semanticscholar.org/paper/c40cad8cacaa47aac6d4cab8f80749beaebd690c,http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_57.pdf,"There is considerable interest in ethical designs for artificial intelligence (AI) that do not pose risks to humans. This paper proposes using elements of Hutter's agent-environment framework to define a decision support system for simulating, visualizing and analyzing AI designs to understand their consequences. The simulations do not have to be accurate predictions of the future; rather they show the futures that an agent design predicts will fulfill its motivations and that can be explored by AI designers to find risks to humans. In order to safely create a simulation model this paper shows that the most probable finite stochastic program to explain a finite history is finitely computable, and that there is an agent that makes such a computation without any unintended instrumental actions. It also discusses the risks of running an AI in a simulated environment.",,
Safe AI Systems,The safe AI must satisfy with 2 majors applying the security measures and analyzing the system.,Search,2018,,Ananta  Bhatt,International Journal of Computer Applications,,10.5120/ijca2018918205,https://doi.org/10.5120/ijca2018918205,https://semanticscholar.org/paper/97aba8083bc5ad49af27a5c26c2bd40e6a17d6a3,,"As you know, it is inevitable to cease the advancement in AI because of the quest for developing systems that learns from experience and comply with the human capabilities. The leading ground-breaking impacts offers both positive and negative aspirations and are addressed as part of this research. To meet with the evil effects of the AI systems, is now the need of the hour to direct the focus on creating safe AI systems. The safe system must satisfy with 2 majorsapplying the security measures and analysing the system. Moreover, system must fulfil the risk and safety valuesintelligence, goals and safety in order to minimise the destruction that may occur due AI technology. However, further research is needed on the actual implementation of the safe AI systems in order to proceed with this research.",,
Human factors challenges for the safe use of artificial intelligence in patient care,AI should be developed with human factors research accompanying it from the outset.,Search,2019,21,"Mark  Sujan, Dominic  Furniss, Kath  Grundy, Howard  Grundy, David  Nelson, Matthew  Elliott, Sean  White, Ibrahim  Habli, Nick  Reynolds",BMJ Health & Care Informatics,,10.1136/bmjhci-2019-100081,https://doi.org/10.1136/bmjhci-2019-100081,https://semanticscholar.org/paper/9a99e50b44f176ea20ed40fe90b00f4f57762666,https://informatics.bmj.com/content/bmjhci/26/1/e100081.full.pdf,"The use of artificial intelligence (AI) in patient care can offer significant benefits. However, there is a lack of independent evaluation considering AI in use. The paper argues that consideration should be given to how AI will be incorporated into clinical processes and services. Human factors challenges that are likely to arise at this level include cognitive aspects (automation bias and human performance), handover and communication between clinicians and AI systems, situation awareness and the impact on the interaction with patients. Human factors research should accompany the development of AI from the outset.",,
Establishing the rules for building trustworthy AI,The European Commission’s report ‘Ethics guidelines for trustworthy AI’ does not provide a clear benchmark to evaluate the responsible development of AI systems.,Search,2019,67,Luciano  Floridi,Nature Machine Intelligence,,10.1038/S42256-019-0055-Y,https://doi.org/10.1038/S42256-019-0055-Y,https://semanticscholar.org/paper/dc44e2be0f85b6225f05390c570885337a99ef83,https://philpapers.org/archive/FLOETR.pdf,"The European Commission’s report ‘Ethics guidelines for trustworthy AI’ provides a clear benchmark to evaluate the responsible development of AI systems, and facilitates international support for AI solutions that are good for humanity and the environment, says Luciano Floridi.",,
Software Verification and Validation of Safe Autonomous Cars: A Systematic Literature Review,Safety assessment needs to address trustworthy AI using appropriate verification and validation processes.,Search,2021,5,"Nijat  Rajabli, Francesco  Flammini, Roberto  Nardone, Valeria  Vittorini",IEEE Access,,10.1109/ACCESS.2020.3048047,https://doi.org/10.1109/ACCESS.2020.3048047,https://semanticscholar.org/paper/957c844595584a5cad10e581518262877aa102df,https://ieeexplore.ieee.org/ielx7/6287639/9312710/09310181.pdf,"Autonomous, or self-driving, cars are emerging as the solution to several problems primarily caused by humans on roads, such as accidents and traffic congestion. However, those benefits come with great challenges in the verification and validation (V&V) for safety assessment. In fact, due to the possibly unpredictable nature of Artificial Intelligence (AI), its use in autonomous cars creates concerns that need to be addressed using appropriate V&V processes that can address trustworthy AI and safe autonomy. In this study, the relevant research literature in recent years has been systematically reviewed and classified in order to investigate the state-of-the-art in the software V&V of autonomous cars. By appropriate criteria, a subset of primary studies has been selected for more in-depth analysis. The first part of the review addresses certification issues against reference standards, challenges in assessing machine learning, as well as general V&V methodologies. The second part investigates more specific approaches, including simulation environments and mutation testing, corner cases and adversarial examples, fault injection, software safety cages, techniques for cyber-physical systems, and formal methods. Relevant approaches and related tools have been discussed and compared in order to highlight open issues and opportunities.",,Review
Safe Artificial Intelligence and Formal Methods - (Position Paper),Formal methods can assist in building safer and reliable AI.,Search,2016,9,Emil  Vassev,ISoLA,,10.1007/978-3-319-47166-2_49,https://doi.org/10.1007/978-3-319-47166-2_49,https://semanticscholar.org/paper/796dcb12b96e5d0c2f358b1b67fdfba36d023acc,https://ulir.ul.ie/bitstream/10344/5407/2/Vassev_2016_safe.pdf,"In one aspect of our life or another, today we all live with AI. For example, the mechanisms behind the search engines operating on the Internet do not just retrieve information, but also constantly learn how to respond more rapidly and usefully to our requests. Although framed by its human inventors, this AI is getting stronger and more powerful every day to go beyond the original human intentions in the future. One of the major questions emerging along with the propagation of AI in both technology and life is about safety in AI. This paper presents the author’s view about how formal methods can assist us in building safer and reliable AI.",,
Safe and sound - artificial intelligence in hazardous applications,Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making.,Search,2000,272,"John  Fox, Subrata Kumar Das",,,,,https://semanticscholar.org/paper/f62d215d4c40d299929d2e7fa105d54623a9e1bf,,"Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making, where small faults or errors can spell human catastrophe. This book describes, from both practical and theoretical perspectives, an AI technology for supporting sound clinical decision making and safe patient management. The technology combines techniques from conventional software engineering with a systematic method for building intelligent agents. Although the focus is on medicine, many of the ideas can be applied to AI systems in other hazardous settings. The book also covers a number of general AI problems, including knowledge representation and expertise modeling, reasoning and decision making under uncertainty, planning and scheduling, and the design and implementation of intelligent agents.The book, written in an informal style, begins with the medical background and motivations, technical challenges, and proposed solutions. It then turns to a wide-ranging discussion of intelligent and autonomous agents, with particular reference to safety and hazard management. The final section provides a detailed discussion of the knowledge representation and other aspects of the agent model developed in the book, along with a formal logical semantics for the language.",,
Safe and Ethical Artificial Intelligence in Radiotherapy - Lessons Learned From the Aviation Industry.,The Aletheia Framework from Rolls-Royce can guide organizations in developing safe and ethical AI systems.,Search,2021,1,"R  Hallows, L  Glazier, M S Katz, M  Aznar, M  Williams",Clinical oncology (Royal College of Radiologists (Great Britain)),,10.1016/j.clon.2021.11.019,https://doi.org/10.1016/j.clon.2021.11.019,https://semanticscholar.org/paper/518441aba1c5f08526564b51f7b516b9837ff964,,"Ethical artificial intelligence (AI) frameworks can be the catalyst in improving the safety and wellbeing of people when developing AI systems. In 2020 Rolls-Royce released its ethical and trustworthiness toolkit, The Aletheia FrameworkTM, which helps guide organisations as they consider the ethics around the use of AI. It covers three facets: social impact, accuracy and trust, and governance - which apply across all uses of AI. By adopting AI ethics and trust frameworks, oncologists can ensure the ratio between the benefit and harms of AI can be maximised. With AI transforming every sector, collaboration across industries to share ideas and learn from each other - even unlikely partnerships between engineering and oncology - could help optimise that transition.",,
"AI Evaluation: past, present and future",AI systems are becoming more complex and unpredictable.,Search,2014,24,José  Hernández-Orallo,ArXiv,,,,https://semanticscholar.org/paper/8f2d78fb9293eac038a6a431d23ca5a18df7bfb4,,"Artificial intelligence develops techniques and systems whose performance must be evaluated on a regular basis in order to certify and foster progress in the discipline. We will describe and critically assess the different ways AI systems are evaluated. We first focus on the traditional task-oriented evaluation approach. We see that black-box (behavioural evaluation) is becoming more and more common, as AI systems are becoming more complex and unpredictable. We identify three kinds of evaluation: Human discrimination, problem benchmarks and peer confrontation. We describe the limitations of the many evaluation settings and competitions in these three categories and propose several ideas for a more systematic and robust evaluation. We then focus on a less customary (and challenging) ability-oriented evaluation approach, where a system is characterised by its (cognitive) abilities, rather than by the tasks it is designed to solve. We discuss several possibilities: the adaptation of cognitive tests used for humans and animals, the development of tests derived from algorithmic information theory or more general approaches under the perspective of universal psychometrics.",,
Governing AI safety through independent audits,Independent audit of AI systems is a pragmatic approach to a burdensome and unenforceable assurance challenge.,Search,2021,8,"Gregory  Falco, Ben  Shneiderman, Julia  Badger, Ryan  Carrier, A. T. Dahbura, David  Danks, Martin  Eling, Alwyn  Goodloe, Jerry  Gupta, Christopher  Hart, Marina  Jirotka, Henric  Johnson, Cara  LaPointe, Ashley J. Llorens, Alan K. Mackworth, Carsten  Maple, Sigurður Emil Pálsson, Frank A. Pasquale, Alan F. T. Winfield, Zee Kin Yeong",Nat. Mach. Intell.,,10.1038/S42256-021-00370-7,https://doi.org/10.1038/S42256-021-00370-7,https://semanticscholar.org/paper/93a04c8661ce96f9ab972a0ede4680232627467a,https://ora.ox.ac.uk/objects/uuid:900c78bf-4fe3-4afc-aaed-5a5fdffecbff/download_file?safe_filename=Falco_et_al_2021_Governing_AI_safety.pdf&file_format=pdf&type_of_work=Journal+article,"Highly automated systems are becoming omnipresent. They range in function from self-driving vehicles to advanced medical diagnostics and afford many benefits. However, there are assurance challenges that have become increasingly visible in high-profile crashes and incidents. Governance of such systems is critical to garner widespread public trust. Governance principles have been previously proposed offering aspirational guidance to automated system developers; however, their implementation is often impractical given the excessive costs and processes required to enact and then enforce the principles. This Perspective, authored by an international and multidisciplinary team across government organizations, industry and academia, proposes a mechanism to drive widespread assurance of highly automated systems: independent audit. As proposed, independent audit of AI systems would embody three ‘AAA’ governance principles of prospective risk Assessments, operation Audit trails and system Adherence to jurisdictional requirements. Independent audit of AI systems serves as a pragmatic approach to an otherwise burdensome and unenforceable assurance challenge. As highly automated systems become pervasive in society, enforceable governance principles are needed to ensure safe deployment. This Perspective proposes a pragmatic approach where independent audit of AI systems is central. The framework would embody three AAA governance principles: prospective risk Assessments, operation Audit trails and system Adherence to jurisdictional requirements.",,