Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Understanding artificial intelligence ethics and safety,The British Government published a guide on AI safety for the UK public sector.,Search,2019,37,David  Leslie,ArXiv,,10.5281/zenodo.3240529,https://doi.org/10.5281/zenodo.3240529,https://semanticscholar.org/paper/adeb97576107e9f8e1140302cf614d1da709d523,,"A remarkable time of human promise has been ushered in by the convergence of the ever-expanding availability of big data, the soaring speed and stretch of cloud computing platforms, and the advancement of increasingly sophisticated machine learning algorithms. Innovations in AI are already leaving a mark on government by improving the provision of essential social goods and services from healthcare, education, and transportation to food supply, energy, and environmental management. These bounties are likely just the start. The prospect that progress in AI will help government to confront some of its most urgent challenges is exciting, but legitimate worries abound. As with any new and rapidly evolving technology, a steep learning curve means that mistakes and miscalculations will be made and that both unanticipated and harmful impacts will occur.

This guide, written for department and delivery leads in the UK public sector and adopted by the British Government in its publication, 'Using AI in the Public Sector,' identifies the potential harms caused by AI systems and proposes concrete, operationalisable measures to counteract them. It stresses that public sector organisations can anticipate and prevent these potential harms by stewarding a culture of responsible innovation and by putting in place governance processes that support the design and implementation of ethical, fair, and safe AI systems. It also highlights the need for algorithmically supported outcomes to be interpretable by their users and made understandable to decision subjects in clear, non-technical, and accessible ways. Finally, it builds out a vision of human-centred and context-sensitive implementation that gives a central role to communication, evidence-based reasoning, situational awareness, and moral justifiability.",,
Safe and sound - artificial intelligence in hazardous applications,Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making.,Search,2000,272,"John  Fox, Subrata Kumar Das",,,,,https://semanticscholar.org/paper/f62d215d4c40d299929d2e7fa105d54623a9e1bf,,"Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making, where small faults or errors can spell human catastrophe. This book describes, from both practical and theoretical perspectives, an AI technology for supporting sound clinical decision making and safe patient management. The technology combines techniques from conventional software engineering with a systematic method for building intelligent agents. Although the focus is on medicine, many of the ideas can be applied to AI systems in other hazardous settings. The book also covers a number of general AI problems, including knowledge representation and expertise modeling, reasoning and decision making under uncertainty, planning and scheduling, and the design and implementation of intelligent agents.The book, written in an informal style, begins with the medical background and motivations, technical challenges, and proposed solutions. It then turns to a wide-ranging discussion of intelligent and autonomous agents, with particular reference to safety and hazard management. The final section provides a detailed discussion of the knowledge representation and other aspects of the agent model developed in the book, along with a formal logical semantics for the language.",,
Safety of Artificial Intelligence: A Collaborative Model,"A healthcare use case demonstrates a three-layer model for safety, ""AI/ML safety"", and safety-critical software engineering.",Search,2020,1,"John  McDermid, Yan  Jia",AISafety@IJCAI,,,,https://semanticscholar.org/paper/355bcdbd0e611afd38c1dc7ccebb29fb1c873cda,,"Achieving and assuring the safety of systems that use artificial intelligence (AI), especially machine learning (ML), pose some specific challenges that require unique solutions. However, that does not mean that good safety and software engineering practices are no longer relevant. This paper shows how the issues associated with AI and ML can be tackled by integrating with established safety and software engineering practices. It sets out a three-layer model, going from top to bottom: system safety/functional safety; “AI/ML safety”; and safety-critical software engineering. This model gives both a basis for achieving and assuring safety and a structure for collaboration between safety engineers and AI/ML specialists. The model is illustrated with a healthcare use case which uses deep reinforcement learning for treating sepsis patients. It is argued that this model is general and that it should underpin future standards and guidelines for safety of this class of system which employ ML, particularly because the model can facilitate collaboration between the different communities.",,
Safe AI Systems,The safe AI systems must fulfill the risk and safety values in order to minimize the destruction that may occur.,Search,2018,,Ananta  Bhatt,International Journal of Computer Applications,,10.5120/ijca2018918205,https://doi.org/10.5120/ijca2018918205,https://semanticscholar.org/paper/97aba8083bc5ad49af27a5c26c2bd40e6a17d6a3,,"As you know, it is inevitable to cease the advancement in AI because of the quest for developing systems that learns from experience and comply with the human capabilities. The leading ground-breaking impacts offers both positive and negative aspirations and are addressed as part of this research. To meet with the evil effects of the AI systems, is now the need of the hour to direct the focus on creating safe AI systems. The safe system must satisfy with 2 majorsapplying the security measures and analysing the system. Moreover, system must fulfil the risk and safety valuesintelligence, goals and safety in order to minimise the destruction that may occur due AI technology. However, further research is needed on the actual implementation of the safe AI systems in order to proceed with this research.",,
Safe Artificial Intelligence and Formal Methods - (Position Paper),Formal methods can assist in building safer and reliable AI.,Search,2016,9,Emil  Vassev,ISoLA,,10.1007/978-3-319-47166-2_49,https://doi.org/10.1007/978-3-319-47166-2_49,https://semanticscholar.org/paper/796dcb12b96e5d0c2f358b1b67fdfba36d023acc,https://ulir.ul.ie/bitstream/10344/5407/2/Vassev_2016_safe.pdf,"In one aspect of our life or another, today we all live with AI. For example, the mechanisms behind the search engines operating on the Internet do not just retrieve information, but also constantly learn how to respond more rapidly and usefully to our requests. Although framed by its human inventors, this AI is getting stronger and more powerful every day to go beyond the original human intentions in the future. One of the major questions emerging along with the propagation of AI in both technology and life is about safety in AI. This paper presents the author’s view about how formal methods can assist us in building safer and reliable AI.",,
Safe AI for CPS (Invited Paper),Combining formal proofs with reinforcement learning can help develop safe artificial intelligence for cyber-physical systems.,Search,2018,5,"Nathan  Fulton, André  Platzer",2018 IEEE International Test Conference (ITC),,10.1109/TEST.2018.8624774,https://doi.org/10.1109/TEST.2018.8624774,https://semanticscholar.org/paper/03fce03060f6110b2772958f58539320bab519ed,,"Autonomous cyber-physical systems-such as self-driving cars and autonomous drones-often leverage artificial intelligence and machine learning algorithms to act well in open environments. Although testing plays an important role in ensuring safety and robustness, modern autonomous systems have grown so complex that achieving safety via testing alone is intractable. Formal verification reduces this testing burden by ruling out large classes of errant behavior at design time. This paper reviews recent work toward developing formal methods for cyber-physical systems that use AI for planning and control by combining the rigor of formal proofs with the flexibility of reinforcement learning.",,Review
How to achieve trustworthy artificial intelligence for health,"The EU's guidance leaves room for local, contextualized discretion in the global health sector.",Search,2020,15,"Kristine  Bærøe, Ainar  Miyata-Sturm, Edmund  Henden",Bulletin of the World Health Organization,,10.2471/BLT.19.237289,https://doi.org/10.2471/BLT.19.237289,https://semanticscholar.org/paper/0a3c92d6c4fa4670743fb13758916067e772a143,https://europepmc.org/articles/pmc7133476?pdf=render,"Abstract Artificial intelligence holds great promise in terms of beneficial, accurate and effective preventive and curative interventions. At the same time, there is also awareness of potential risks and harm that may be caused by unregulated developments of artificial intelligence. Guiding principles are being developed around the world to foster trustworthy development and application of artificial intelligence systems. These guidelines can support developers and governing authorities when making decisions about the use of artificial intelligence. The High-Level Expert Group on Artificial Intelligence set up by the European Commission launched the report Ethical guidelines for trustworthy artificial intelligence in2019. The report aims to contribute to reflections and the discussion on the ethics of artificial intelligence technologies also beyond the countries of the European Union (EU). In this paper, we use the global health sector as a case and argue that the EU’s guidance leaves too much room for local, contextualized discretion for it to foster trustworthy artificial intelligence globally. We point to the urgency of shared globalized efforts to safeguard against the potential harms of artificial intelligence technologies in health care.",,
How Safe Is Artificial Intelligence?,Machine learning dramatically changes our civilization.,Search,2019,1,Klaus  Mainzer,Artificial intelligence - When do machines take over?,,10.1007/978-3-662-59717-0_11,https://doi.org/10.1007/978-3-662-59717-0_11,https://semanticscholar.org/paper/5493c1017f9c42565e7f01a170e12a99d128ea82,,"Machine learning dramatically changes our civilization. We rely more and more on efficient algorithms, because otherwise the complexity of our civilizing infrastructure would not be manageable: Our brains are too slow and hopelessly overwhelmed by the amount of data we have to deal with. But how secure are AI algorithms? In practical applications, learning algorithms refer to models of neural networks, which themselves are extremely complex. They are fed and trained with huge amounts of data. The number of necessary parameters explodes exponentially. Nobody knows exactly what happens in these “black boxes” in detail. A statistical trial-and-error procedure often remains. But how should questions of responsibility be decided in, e.g., autonomous driving or in medicine, if the methodological basics remain dark?",,
Guidelines for Artificial Intelligence Containment,Safety container software can study and analyze intelligent artificial agents while maintaining safety levels.,Search,2019,20,"James  Babcock, János  Kramár, Roman V. Yampolskiy",Next-Generation Ethics,,10.1017/9781108616188.008,https://doi.org/10.1017/9781108616188.008,https://semanticscholar.org/paper/b0c820f9d7fa4d8c56a7887548834b7ba65ddfa5,http://www.vetta.org/documents/Machine_Super_Intelligence.pdf,"With almost daily improvements in capabilities of artificial intelligence it is more important than ever to develop safety software for use by the AI research community. Building on our previous work on AI Containment Problem we propose a number of guidelines which should help AI safety researchers to develop reliable sandboxing software for intelligent programs of all levels. Such safety container software will make it possible to study and analyze intelligent artificial agent while maintaining certain level of safety against information leakage, social engineering attacks and cyberattacks from within the container.",,
Controlling Safety of Artificial Intelligence-Based Systems in Healthcare,A safety controlling system could guide and control the implementation of artificial intelligence in healthcare.,Search,2021,6,"Mohammad Reza Davahli, Waldemar  Karwowski, Krzysztof  Fiok, Thomas T. H. Wan, Hamid R. Parsaei",Symmetry,,10.3390/sym13010102,https://doi.org/10.3390/sym13010102,https://semanticscholar.org/paper/f725733c4a497ef80f089d391c7b9f5304206d28,https://www.mdpi.com/2073-8994/13/1/102/pdf,"In response to the need to address the safety challenges in the use of artificial intelligence (AI), this research aimed to develop a framework for a safety controlling system (SCS) to address the AI black-box mystery in the healthcare industry. The main objective was to propose safety guidelines for implementing AI black-box models to reduce the risk of potential healthcare-related incidents and accidents. The system was developed by adopting the multi-attribute value model approach (MAVT), which comprises four symmetrical parts: extracting attributes, generating weights for the attributes, developing a rating scale, and finalizing the system. On the basis of the MAVT approach, three layers of attributes were created. The first level contained 6 key dimensions, the second level included 14 attributes, and the third level comprised 78 attributes. The key first level dimensions of the SCS included safety policies, incentives for clinicians, clinician and patient training, communication and interaction, planning of actions, and control of such actions. The proposed system may provide a basis for detecting AI utilization risks, preventing incidents from occurring, and developing emergency plans for AI-related risks. This approach could also guide and control the implementation of AI systems in the healthcare industry.",,
Building Safer AGI by introducing Artificial Stupidity,Artificial Stupidity can be used to make Artificial General Intelligence safer by limiting its computing power and memory.,Search,2018,18,"Michaël  Trazzi, Roman V. Yampolskiy",ArXiv,,,,https://semanticscholar.org/paper/feef1ddc618a406e4b125b51ad7de0505516c8e6,,"Artificial Intelligence (AI) achieved super-human performance in a broad variety of domains. We say that an AI is made Artificially Stupid on a task when some limitations are deliberately introduced to match a human's ability to do the task. An Artificial General Intelligence (AGI) can be made safer by limiting its computing power and memory, or by introducing Artificial Stupidity on certain tasks. We survey human intellectual limits and give recommendations for which limits to implement in order to build a safe AGI.",,
Decision Support for Safe AI Design,Simulations do not have to be accurate predictions of the future to be useful for AI safety.,Search,2012,4,Bill  Hibbard,AGI,,10.1007/978-3-642-35506-6_13,https://doi.org/10.1007/978-3-642-35506-6_13,https://semanticscholar.org/paper/c40cad8cacaa47aac6d4cab8f80749beaebd690c,http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_57.pdf,"There is considerable interest in ethical designs for artificial intelligence (AI) that do not pose risks to humans. This paper proposes using elements of Hutter's agent-environment framework to define a decision support system for simulating, visualizing and analyzing AI designs to understand their consequences. The simulations do not have to be accurate predictions of the future; rather they show the futures that an agent design predicts will fulfill its motivations and that can be explored by AI designers to find risks to humans. In order to safely create a simulation model this paper shows that the most probable finite stochastic program to explain a finite history is finitely computable, and that there is an agent that makes such a computation without any unintended instrumental actions. It also discusses the risks of running an AI in a simulated environment.",,
Entropic boundary conditions towards safe artificial superintelligence,Artificial superintelligent (ASI) agents that will not cause harm to humans or other organisms are central to mitigating a growing contemporary global safety concern.,Search,2021,,"Santiago  Núñez-Corrales, Eric  Jakobsson",Journal of Experimental & Theoretical Artificial Intelligence,,10.1080/0952813X.2021.1952653,https://doi.org/10.1080/0952813X.2021.1952653,https://semanticscholar.org/paper/026c8c82cf2e2b80e6c0fdf5986b5a6146baa384,,Artificial superintelligent (ASI) agents that will not cause harm to humans or other organisms are central to mitigating a growing contemporary global safety concern as artificial intelligent agent...,,
Towards Safe Artificial General Intelligence,"If humanity would find itself in conflict with a system of much greater intelligence than itself, then human society would likely lose.",Search,2018,21,Tom  Everitt,,,10.25911/5D134A2F8A7D3,https://doi.org/10.25911/5D134A2F8A7D3,https://semanticscholar.org/paper/8b1e149ae23ea7839c9e3a2bd063c354ff7075d0,,"The field of artificial intelligence has recently experienced a number of breakthroughs thanks to progress in deep learning and reinforcement learning. Computer algorithms now outperform humans at Go, Jeopardy, image classification, and lip reading, and are becoming very competent at driving cars and interpreting natural language. The rapid development has led many to conjecture that artificial intelligence with greater-thanhuman ability on a wide range of tasks may not be far. This in turn raises concerns whether we know how to control such systems, in case we were to successfully build them. Indeed, if humanity would find itself in conflict with a system of much greater intelligence than itself, then human society would likely lose. One way to make sure we avoid such a conflict is to ensure that any future AI system with potentially greater-thanhuman-intelligence has goals that are aligned with the goals of the rest of humanity. For example, it should not wish to kill humans or steal their resources. The main focus of this thesis will therefore be goal alignment, i.e. how to design artificially intelligent agents with goals coinciding with the goals of their designers. Focus will mainly be directed towards variants of reinforcement learning, as reinforcement learning currently seems to be the most promising path towards powerful artificial intelligence. We identify and categorize goal misalignment problems in reinforcement learning agents as designed today, and give examples of how these agents may cause catastrophes in the future. We also suggest a number of reasonably modest modifications that can be used to avoid or mitigate each identified misalignment problem. Finally, we also study various choices of decision algorithms, and conditions for when a powerful reinforcement learning system will permit us to shut it down. The central conclusion is that while reinforcement learning systems as designed today are inherently unsafe to scale to human levels of intelligence, there are ways to potentially address many of these issues without straying too far from the currently so successful reinforcement learning paradigm. Much work remains in turning the high-level proposals suggested in this thesis into practical algorithms, however. Central claim: There are a number of theoretically valid, partial solutions to the problem of keeping artificial general intelligence both safe and useful.",,
Safe AI—is this possible?☆,"The inherently self-learning, self-adaptive nature of artificial intelligence methods may require safe, predictable controllers.",Search,1995,11,M. G. Rodd,,,10.1016/0952-1976(95)00010-X,https://doi.org/10.1016/0952-1976(95)00010-X,https://semanticscholar.org/paper/c53376e4e49ca954f4f4f2ca520b6042967f7dda,,"Abstract This deliberately provocative paper poses the question as to whether it is professionally acceptable to use AI-based control systems in real-time automation. The premise is that the increasing demands for improved control of industrial processes are resulting in a search for new control techniques which, it is suggested, will be based on various AI methodologies. However, in the face of the inherently self-learning, self-adaptive nature of such techniques, it is pointed out that to produce safe, predictable systems, the controllers should be functionally deterministic! In posing this dilemma, the paper suggests possible ways in which it may be resolved.",,
Fading intelligence theory: A theory on keeping artificial intelligence safety for the future,A theory on achieving safe intelligent systems by considering the life-time of an Artificial Intelligence system.,Search,2017,3,"Utku  Kose, Pandian  Vasant",2017 International Artificial Intelligence and Data Processing Symposium (IDAP),,10.1109/IDAP.2017.8090235,https://doi.org/10.1109/IDAP.2017.8090235,https://semanticscholar.org/paper/0df5135a1044d9df389bc4d40b71674d9de5c27d,,"As a result of unstoppable rise of Artificial Intelligence, there has been a remarkable focus on the question of “Will intelligent systems be safe for humankind of the future?” Because of that, many researchers have started to direct their works on dealing with problems that may cause problems on enabling Artificial Intelligence systems to behave out of control or take positions dangerous for humans. Such research works are currently included under the literature of Artificial Intelligence Safety and / or Future of Artificial Intelligence. In the context of the explanations, this research paper proposes a theory on achieving safe intelligent systems by considering life-time of an Artificial Intelligence based system according to some operational variables and eliminate – terminate an intelligent system, which is ‘old enough' to operate for giving chance to new generations of systems, which seem safer. The paper makes a brief introduction to the theory and opens doors widely for further research on it.",,