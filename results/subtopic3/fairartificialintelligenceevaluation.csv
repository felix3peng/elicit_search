Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Fairness Assessment for Artificial Intelligence in Financial Industry,Fairness evaluation metrics are applied to credit card default payment examples.,Search,2019,7,"Yukun  Zhang, Longsheng  Zhou",ArXiv,,,,https://semanticscholar.org/paper/9b0f1ec29596a1e5d41366235322f7d1d68be9e4,,"Artificial Intelligence (AI) is an important driving force for the development and transformation of the financial industry. However, with the fast-evolving AI technology and application, unintentional bias, insufficient model validation, immature contingency plan and other underestimated threats may expose the company to operational and reputational risks. In this paper, we focus on fairness evaluation, one of the key components of AI Governance, through a quantitative lens. Statistical methods are reviewed for imbalanced data treatment and bias mitigation. These methods and fairness evaluation metrics are then applied to a credit card default payment example.",,
"Fairness, Accountability and Transparency in Artificial Intelligence: A Case Study of Logical Predictive Models","Fairness, accountability, and transparency are important for the evaluation of artificial intelligence.",Search,2019,4,Kacper  Sokol,AIES,,10.1145/3306618.3314316,https://doi.org/10.1145/3306618.3314316,https://semanticscholar.org/paper/f031fe645728d6fd36a9a36232cf19a846e9db73,,"Machine learning -- the part of artificial intelligence aimed at eliciting knowledge from data and automated decision making without explicit instructions -- is making great strides, with new algorithms being invented every day. These algorithms find myriads of applications, but their ubiquity often comes at the expense of limited interpretability, hidden biases and unexpected vulnerabilities. Whenever one of these factors is a priority, the learning algorithm of choice is often a method considered to be inherently interpretable, e.g. logical models such as decision trees. In my research I challenge this assumption and highlight (quite common) cases when the assumed interpretability fails to deliver. To restore interpretability of logical machine learning models (decision trees and their ensembles in particular) I propose to explain them with class-contrastive counterfactual statements, which are a very common type of explanation in human interactions, well-grounded in social science research. To evaluate transparency of such models I collate explainability desiderata that can be used to systematically assess and compare such methods as an addition to user studies. Given contrastive explanations, I investigate their influence on the model's security, in particular gaming and stealing the model. Finally, I evaluate model fairness, where I am interested in choosing the most fair model among all the models with equal performance.",,
Adapting SQuaRE for Quality Assessment of Artificial Intelligence Systems,"ISO/IEC 25000 series, known as SQuaRE, can be adapted for the unique nature of ML and Ethics guidelines for trustworthy AI from European Commission.",Search,2019,8,"Hiroshi  Kuwajima, Fuyuki  Ishikawa",2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),,10.1109/ISSREW.2019.00035,https://doi.org/10.1109/ISSREW.2019.00035,https://semanticscholar.org/paper/7f4cb5546bab43fa01988385fb89546e57485b69,http://arxiv.org/pdf/1908.02134,"More and more software practitioners are tackling towards industrial applications of artificial intelligence (AI) systems, especially those based on machine learning (ML). However, many of existing principles and approaches to traditional software systems do not work effectively for the system behavior obtained by training not by logical design. In addition, unique kinds of requirements are emerging such as fairness and explainability. To provide clear guidance to understand and tackle these difficulties, we present an analysis on what quality concepts we should evaluate for AI systems. We base our discussion on ISO/IEC 25000 series, known as SQuaRE, and identify how it should be adapted for the unique nature of ML and Ethics guidelines for trustworthy AI from European Commission. We thus provide holistic insights for quality of AI systems by incorporating the ML nature and AI ethics to the traditional software quality concepts.",,
Evaluation in artificial intelligence: from task-oriented to ability-oriented measurement,The evaluation of artificial intelligence systems and components is crucial for the progress of the discipline.,Search,2016,80,José  Hernández-Orallo,Artificial Intelligence Review,,10.1007/s10462-016-9505-7,https://doi.org/10.1007/s10462-016-9505-7,https://semanticscholar.org/paper/1a48604fbc09f88a89e551ed70dda07bdb6f01e5,https://riunet.upv.es/bitstream/10251/83598/3/AIRE-Evaluation-of-AI.pdf,"The evaluation of artificial intelligence systems and components is crucial for the progress of the discipline. In this paper we describe and critically assess the different ways AI systems are evaluated, and the role of components and techniques in these systems. We first focus on the traditional task-oriented evaluation approach. We identify three kinds of evaluation: human discrimination, problem benchmarks and peer confrontation. We describe some of the limitations of the many evaluation schemes and competitions in these three categories, and follow the progression of some of these tests. We then focus on a less customary (and challenging) ability-oriented evaluation approach, where a system is characterised by its (cognitive) abilities, rather than by the tasks it is designed to solve. We discuss several possibilities: the adaptation of cognitive tests used for humans and animals, the development of tests derived from algorithmic information theory or more integrated approaches under the perspective of universal psychometrics. We analyse some evaluation tests from AI that are better positioned for an ability-oriented evaluation and discuss how their problems and limitations can possibly be addressed with some of the tools and ideas that appear within the paper. Finally, we enumerate a series of lessons learnt and generic guidelines to be used when an AI evaluation scheme is under consideration.",,
Fairness Score and Process Standardization: Framework for Fairness Certification in Artificial Intelligence Systems,A Fairness Score and a Standard Operating Procedure (SOP) for issuing Fairness Certification for fair AI systems would improve the trustworthiness of AI systems.,Search,2022,,"Avinash  Agarwal, Harsh  Agarwal, Nihaarika  Agarwal",ArXiv,,,,https://semanticscholar.org/paper/64c2fa6b3c161fe2a5ae20f03ff77c3250027e10,,"Decisions made by various Artificial Intelligence (AI) systems greatly influence our day-to-day lives. With the increasing use of AI systems, it becomes crucial to know that they are fair, identify the underlying biases in their decision-making, and create a standardized framework to ascertain their fairness. In this paper, we propose a novel Fairness Score to measure the fairness of a data-driven AI system and a Standard Operating Procedure (SOP) for issuing Fairness Certification for such systems. Fairness Score and audit process standardization will ensure quality, reduce ambiguity, enable comparison and improve the trustworthiness of the AI systems. It will also provide a framework to operationalise the concept of fairness and facilitate the commercial deployment of such systems. Furthermore, a Fairness Certificate issued by a designated third-party auditing agency following the standardized process would boost the conviction of the organizations in the AI systems that they intend to deploy. The Bias Index proposed in this paper also reveals comparative bias amongst the various protected attributes within the dataset. To substantiate the proposed framework, we iteratively train a model on biased and unbiased data using multiple datasets and check that the Fairness Score and the proposed process correctly identify the biases and judge the fairness.",,
Validity Evaluation for the Data Used for Artificial Intelligence System,The data used to develop an AI system shall reflect the phenomena of the real-world sufficiently.,Search,2019,1,Han Seong Son,IntelliSys,,10.1007/978-3-030-29516-5_28,https://doi.org/10.1007/978-3-030-29516-5_28,https://semanticscholar.org/paper/56ccc87a0f5a01991c7204a620099ac53dfa9c9a,,"The data used to develop an AI system shall reflect the phenomena of the real-world sufficiently. This draws out a need to evaluate the validity of the data. This work has proposed a data validity evaluation method. To evaluate the data validity, the three steps of data preparation, data clustering, and validation are performed. To apply a data clustering algorithm is the main idea of the proposed data validity evaluation method. Analyzing the results from the data clustering in various perspectives leads us to evaluate the data validity of certain training data. Through a comparative study, the applicability of the proposed method is demonstrated.",,
Evaluation Platform for Artificial Intelligence Algorithms,Choosing the most appropriate algorithm to solve a particular problem is not a trivial task.,Search,2018,2,"Zoltan  Czako, Gheorghe  Sebestyen, Anca  Hangan",IJCCI,,10.5220/0006888900390046,https://doi.org/10.5220/0006888900390046,https://semanticscholar.org/paper/84e157edf26507db899356e67d6c24b4f901dc7f,,"Currently, artificial intelligence (AI) algorithms have been receiving a lot of attention from researchers as well as from commercial product developers. Hundreds of different AI algorithms aiming different kind of real-life problems have qualitatively different results, based on the nature of data, the nature of the problems and based on the context in which they are used. Choosing the most appropriate algorithm to solve a particular problem is not a trivial task. The goal of our research is to create a platform, which can be used in the early stage of problem solving. With this platform, the user could be able to quickly train, test and evaluate several artificial intelligence algorithms and also they will be able to find out which is the algorithm that performs best for a specific problem. Moreover, this platform will help developers to tune the parameters of the chosen algorithm in order to get better results on their problem. We will demonstrate our approach by running different types of algorithms initially in the case of breast cancer sample dataset and after that we will use the platform for solving an anomaly detection problem.",,
Towards Fairness Certification in Artificial Intelligence,Artificial intelligence is supportive in many decision-making scenarios with major impact on society.,Search,2021,1,"Tatiana  Tommasi, Silvia  Bucci, Barbara  Caputo, Pietro  Asinari",ArXiv,,,,https://semanticscholar.org/paper/157246efaa0a667383cd78da0599231687368e0e,,"Thanks to the great progress of machine learning in the last years, several Artificial Intelligence (AI) techniques have been increasingly moving from the controlled research laboratory settings to our everyday life. The most simple examples are the spam filters that keep our email account in order, face detectors that help us when taking a portrait picture, online recommender systems that suggest which movie and clothing we might like, or interactive maps that navigate us towards our vacation home. Artificial intelligence is clearly supportive in many decision-making scenarios, but when it comes to sensitive areas such as health care, hiring policies, education, banking or justice, with major impact on individuals and society, it becomes crucial to establish guidelines on how to design, develop, deploy and monitor this technology. Indeed the decision rules elaborated by machine learning models are data-driven and there are multiple ways in which discriminatory biases can seep into data. Algorithms trained on those data incur the risk of amplifying prejudices and societal stereotypes by over associating protected attributes such as gender, ethnicity or disabilities with the prediction task.",,
Effect of Information Presentation on Fairness Perceptions of Machine Learning Predictors,The chosen visualisation technique significantly alters people’s fairness perception and the presented scenario influences perceived fairness.,Search,2021,5,"Niels van Berkel, Jorge  Gonçalves, Daniel  Russo, Simo  Hosio, Mikael B. Skov",CHI,,10.1145/3411764.3445365,https://doi.org/10.1145/3411764.3445365,https://semanticscholar.org/paper/5246de247ad77da539b5b01fac310f7d324d424c,http://jultika.oulu.fi/files/nbnfi-fe2021052130978.pdf,"The uptake of artificial intelligence-based applications raises concerns about the fairness and transparency of AI behaviour. Consequently, the Computer Science community calls for the involvement of the general public in the design and evaluation of AI systems. Assessing the fairness of individual predictors is an essential step in the development of equitable algorithms. In this study, we evaluate the effect of two common visualisation techniques (text-based and scatterplot) and the display of the outcome information (i.e., ground-truth) on the perceived fairness of predictors. Our results from an online crowdsourcing study (N = 80) show that the chosen visualisation technique significantly alters people’s fairness perception and that the presented scenario, as well as the participant’s gender and past education, influence perceived fairness. Based on these results we draw recommendations for future work that seeks to involve non-experts in AI fairness evaluations.",,
An instrument to evaluate the maturity of bias governance capability in artificial intelligence projects,"Bias is already prevalent in AI datasets and algorithms, which is considered to be unethical, unsustainable, and challenging to manage.",Search,2019,3,"Daphne  Coates, Andrew  Martin",IBM J. Res. Dev.,,10.1147/JRD.2019.2915062,https://doi.org/10.1147/JRD.2019.2915062,https://semanticscholar.org/paper/670e2331f78028f14ccfc94399eba72eccd53642,,"Artificial intelligence (AI) promises unprecedented contributions to both business and society, attracting a surge of interest from many organizations. However, there is evidence that bias is already prevalent in AI datasets and algorithms, which, albeit unintended, is considered to be unethical, suboptimal, unsustainable, and challenging to manage. It is believed that the governance of data and algorithmic bias must be deeply embedded in the values, mindsets, and procedures of AI software development teams, but currently there is a paucity of actionable mechanisms to help. In this paper, we describe a maturity framework based on ethical principles and best practices, which can be used to evaluate an organization's capability to govern bias. We also design, construct, validate, and test an original instrument for operationalizing the framework, which considers both technical and organizational aspects. The instrument has been developed and validated through a two-phase study involving field experts and academics. The framework and instrument are presented for ongoing evolution and utilization.",,
How to Trust the Middle Artificial Intelligence: Uncertainty Oriented Evaluation,There is considerable interest in the human-level machine intelligence.,Search,2019,,"Marwa  Brichni, Said El Gattoufi",ISDA,,10.1007/978-3-030-49342-4_42,https://doi.org/10.1007/978-3-030-49342-4_42,https://semanticscholar.org/paper/6fe718161f1d31b1f533015b3f7360e849742da8,,"These last years have seen a renewed importance in measuring machine intelligence and considerable interest in the human-level machine intelligence. Despite this interest, few to the best of our knowledge who proposed a classification for these evaluations.",,
Explaining how your AI system is fair,Sharing reasons and principles for fair AI system selection would help maintain confidence in the systems.,Search,2021,,"Boris  Ruf, Marcin  Detyniecki",ArXiv,,,,https://semanticscholar.org/paper/1271e86aad602cfa09727dc98bae31c19ce77cc6,,"Copyright held by the owner/author(s). CHI’21,, May 8–13, 2021, Online Virtual Conference (originally Yokohama, Japan) ACM 978-1-4503-6819-3/20/04. https://doi.org/10.1145/3334480.XXXXXXX Abstract To implement fair machine learning in a sustainable way, choosing the right fairness objective is key. Since fairness is a concept of justice which comes in various, sometimes conflicting definitions, this is not a trivial task though. The most appropriate fairness definition for an artificial intelligence (AI) system is a matter of ethical standards and legal requirements, and the right choice depends on the particular use case and its context. In this position paper, we propose to use a decision tree as means to explain and justify the implemented kind of fairness to the end users. Such a structure would first of all support AI practitioners in mapping ethical principles to fairness definitions for a concrete application and therefore make the selection a straightforward and transparent process. However, this approach would also help document the reasoning behind the decision making. Due to the general complexity of the topic of fairness in AI, we argue that specifying ""fairness"" for a given use case is the best way forward to maintain confidence in AI systems. In this case, this could be achieved by sharing the reasons and principles expressed during the decision making process with the broader audience.",,
Artificial intelligence: how it works and criteria for assessment,Artificial intelligence technologies will have a significant impact on medical imaging.,Search,2021,,"Irena L. Shlivko, Oxana Ye. Garanina, Irina A. Klemenova, Kseniia A. Uskova, Anna M. Mironycheva, Veniamin I. Dardyk, Viktor N. Laskov",Consilium Medicum,,10.26442/20751753.2021.8.201148,https://doi.org/10.26442/20751753.2021.8.201148,https://semanticscholar.org/paper/622a4773fd895416a6cdce3b81cb087b27336a66,https://consilium.orscience.ru/2075-1753/article/download/97097/71487,"Artificial intelligence is a term used to describe computer technology in the modeling of intelligent behavior and critical thinking comparable to that of humans. To date, some of the first areas of medicine to be influenced by advances in artificial intelligence technologies will be those most dependent on imaging. These include ophthalmology, radiology, and dermatology. In connection with the emergence of numerous medical applications, scientists have formulated criteria for their assessment. This list included: clinical validation, regular application updates, functional focus, cost, availability of an information block for specialists and patients, compliance with the conditions of government regulation, and registration. One of the applications that meet all the requirements is the ProRodinki software package, developed for use by patients and specialists in the Russian Federation. Taking into account a widespread and rapidly developing competitive environment, it is necessary to soberly treat the resources of such applications, not exaggerating their capabilities and not considering them as a substitute for a specialist.",,
Evaluating representational systems in artificial intelligence,Artificial reasoners need the ability to reason over representational systems.,Search,2017,6,"John  Licato, Zhitian  Zhang",Artificial Intelligence Review,,10.1007/s10462-017-9598-7,https://doi.org/10.1007/s10462-017-9598-7,https://semanticscholar.org/paper/cace74396cc6f9b5c4c0a0fc3cc1931c429eec11,,"All artificial reasoners work within representational systems. These systems, which may have varying levels of formality or detail, determine the space of possible representations over which the artificial reasoner can operate, by defining the syntactic and semantic properties of the symbols, structures, and inferences that they manipulate. But we are now seeing an increasing need for the ability to reason over representational systems, rather than just working within them. A prerequisite of performing such reasoning is the ability to evaluate and compare representational objects (and to know the difference between them). We survey the criteria that are used for such evaluations in AI, machine learning, and other AI-related fields. To aid our survey, we introduce a formalism of representations, representational systems, and representational spaces that lends itself nicely to an analysis of the criteria typically used for evaluating them.",,
Think Your Artificial Intelligence Software Is Fair? Think Again,"Machine-learning software is also biased, sometimes in similar ways to humans, often in different ways.",Search,2019,14,"Rachel K.E. Bellamy, Kuntal  Dey, Michael  Hind, Samuel C. Hoffman, Stephanie  Houde, Kalapriya  Kannan, Pranay  Lohia, Sameep  Mehta, Aleksandra  Mojsilovic, Seema  Nagar, Karthikeyan Natesan Ramamurthy, John  Richards, Diptikalyan  Saha, Prasanna  Sattigeri, Moninder  Singh, Kush R. Varshney, Yunfeng  Zhang",IEEE Software,,10.1109/MS.2019.2908514,https://doi.org/10.1109/MS.2019.2908514,https://semanticscholar.org/paper/4d60f78b44f34a67a5ce6316d1c45c90a912db44,,"Today, machine-learning software is used to help make decisions that affect people's lives. Some people believe that the application of such software results in fairer decisions because, unlike humans, machine-learning software generates models that are not biased. Think again. Machine-learning software is also biased, sometimes in similar ways to humans, often in different ways. While fair model- assisted decision making involves more than the application of unbiased models-consideration of application context, specifics of the decisions being made, resolution of conflicting stakeholder viewpoints, and so forth-mitigating bias from machine-learning software is important and possible but difficult and too often ignored.",,
A fuzzy evaluation method about facing distant intelligent test,The reasonable evaluation of intelligence test is the key technology for the embodiment of excellence in intelligent test platform.,Search,2009,1,"Zhang  Meiyu, Jiang  Rongrong",2009 4th International Conference on Computer Science & Education,,10.1109/ICCSE.2009.5228503,https://doi.org/10.1109/ICCSE.2009.5228503,https://semanticscholar.org/paper/7e0138934b539a076f0b4dee97a6c28701e823f2,,"The platform of intelligent test is one of the core content for modern distant education system. The reasonable evaluation of intelligence test is the key technology for the embodiment of excellence in intelligent test platform. In this paper, in order to more accurately analyze and evaluate the students' distance learning behavior, process and achievement, we design a remote intelligent test platform, and on this basis propose a score-assessment method based on fuzzy mathematics. Utilizing fuzzy inference in uncertainty reasoning, we assign the score membership function, then the original score had been standardized and weighted disposal for a more comprehensive reasoning of intelligent evaluation.",,