Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
On Safety Assessment of Artificial Intelligence,Safety assessment for AI would require analysis of the model that is used in AI.,Search,2020,5,"Jens  Braband, Hendrik  Schäbe",ArXiv,,10.21683/1729-2646-2020-20-4-25-34,https://doi.org/10.21683/1729-2646-2020-20-4-25-34,https://semanticscholar.org/paper/99cadaf09e78f298ebd920bdb8a6ae39025523e4,https://www.dependability.ru/jour/article/download/392/646,"In this paper we discuss how systems with Artificial Intelligence (AI) can undergo safety assessment. This is relevant, if AI is used in safety related applications. Taking a deeper look into AI models, we show, that many models of artificial intelligence, in particular machine learning, are statistical models. Safety assessment would then have t o concentrate on the model that is used in AI, besides the normal assessment procedure. Part of the budget of dangerous random failures for the relevant safety integrity level needs to be used for the probabilistic faulty behavior of the AI system. We demonstrate our thoughts with a simple example and propose a research challenge that may be decisive for the use of AI in safety related systems.",,
Opening the software engineering toolbox for the assessment of trustworthy AI,Software engineering and testing practices can be used for the assessment of trustworthy AI.,Search,2020,2,"Mohit Kumar Ahuja, Mohamed-Bachir  Belaid, Pierre  Bernab'e, Mathieu  Collet, Arnaud  Gotlieb, Chhagan  Lal, Dusica  Marijan, Sagar  Sen, Aizaz  Sharif, Helge  Spieker",NeHuAI@ECAI,,,,https://semanticscholar.org/paper/3b0ff6bd000e9c615d024614343f2c1cf12bf124,,"Trustworthiness is a central requirement for the acceptance and success of human-centered artificial intelligence (AI). To deem an AI system as trustworthy, it is crucial to assess its behaviour and characteristics against a gold standard of Trustworthy AI, consisting of guidelines, requirements, or only expectations. While AI systems are highly complex, their implementations are still based on software. The software engineering community has a long-established toolbox for the assessment of software systems, especially in the context of software testing. In this paper, we argue for the application of software engineering and testing practices for the assessment of trustworthy AI. We make the connection between the seven key requirements as defined by the European Commission's AI high-level expert group and established procedures from software engineering and raise questions for future work.",,
Governing AI safety through independent audits,Independent audit of AI systems is a pragmatic approach to a burdensome and unenforceable assurance challenge.,Search,2021,8,"Gregory  Falco, Ben  Shneiderman, Julia  Badger, Ryan  Carrier, A. T. Dahbura, David  Danks, Martin  Eling, Alwyn  Goodloe, Jerry  Gupta, Christopher  Hart, Marina  Jirotka, Henric  Johnson, Cara  LaPointe, Ashley J. Llorens, Alan K. Mackworth, Carsten  Maple, Sigurður Emil Pálsson, Frank A. Pasquale, Alan F. T. Winfield, Zee Kin Yeong",Nat. Mach. Intell.,,10.1038/S42256-021-00370-7,https://doi.org/10.1038/S42256-021-00370-7,https://semanticscholar.org/paper/93a04c8661ce96f9ab972a0ede4680232627467a,https://ora.ox.ac.uk/objects/uuid:900c78bf-4fe3-4afc-aaed-5a5fdffecbff/download_file?safe_filename=Falco_et_al_2021_Governing_AI_safety.pdf&file_format=pdf&type_of_work=Journal+article,"Highly automated systems are becoming omnipresent. They range in function from self-driving vehicles to advanced medical diagnostics and afford many benefits. However, there are assurance challenges that have become increasingly visible in high-profile crashes and incidents. Governance of such systems is critical to garner widespread public trust. Governance principles have been previously proposed offering aspirational guidance to automated system developers; however, their implementation is often impractical given the excessive costs and processes required to enact and then enforce the principles. This Perspective, authored by an international and multidisciplinary team across government organizations, industry and academia, proposes a mechanism to drive widespread assurance of highly automated systems: independent audit. As proposed, independent audit of AI systems would embody three ‘AAA’ governance principles of prospective risk Assessments, operation Audit trails and system Adherence to jurisdictional requirements. Independent audit of AI systems serves as a pragmatic approach to an otherwise burdensome and unenforceable assurance challenge. As highly automated systems become pervasive in society, enforceable governance principles are needed to ensure safe deployment. This Perspective proposes a pragmatic approach where independent audit of AI systems is central. The framework would embody three AAA governance principles: prospective risk Assessments, operation Audit trails and system Adherence to jurisdictional requirements.",,
Safe AI for CPS (Invited Paper),Combining formal proofs with reinforcement learning can help develop safe AI for cyber-physical systems.,Search,2018,5,"Nathan  Fulton, André  Platzer",2018 IEEE International Test Conference (ITC),,10.1109/TEST.2018.8624774,https://doi.org/10.1109/TEST.2018.8624774,https://semanticscholar.org/paper/03fce03060f6110b2772958f58539320bab519ed,,"Autonomous cyber-physical systems-such as self-driving cars and autonomous drones-often leverage artificial intelligence and machine learning algorithms to act well in open environments. Although testing plays an important role in ensuring safety and robustness, modern autonomous systems have grown so complex that achieving safety via testing alone is intractable. Formal verification reduces this testing burden by ruling out large classes of errant behavior at design time. This paper reviews recent work toward developing formal methods for cyber-physical systems that use AI for planning and control by combining the rigor of formal proofs with the flexibility of reinforcement learning.",,Review
Decision Support for Safe AI Design,Simulations do not have to be accurate predictions of the future to be useful for AI assessment.,Search,2012,4,Bill  Hibbard,AGI,,10.1007/978-3-642-35506-6_13,https://doi.org/10.1007/978-3-642-35506-6_13,https://semanticscholar.org/paper/c40cad8cacaa47aac6d4cab8f80749beaebd690c,http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_57.pdf,"There is considerable interest in ethical designs for artificial intelligence (AI) that do not pose risks to humans. This paper proposes using elements of Hutter's agent-environment framework to define a decision support system for simulating, visualizing and analyzing AI designs to understand their consequences. The simulations do not have to be accurate predictions of the future; rather they show the futures that an agent design predicts will fulfill its motivations and that can be explored by AI designers to find risks to humans. In order to safely create a simulation model this paper shows that the most probable finite stochastic program to explain a finite history is finitely computable, and that there is an agent that makes such a computation without any unintended instrumental actions. It also discusses the risks of running an AI in a simulated environment.",,
Safe Artificial Intelligence and Formal Methods - (Position Paper),Formal methods can assist in building safer and reliable AI.,Search,2016,9,Emil  Vassev,ISoLA,,10.1007/978-3-319-47166-2_49,https://doi.org/10.1007/978-3-319-47166-2_49,https://semanticscholar.org/paper/796dcb12b96e5d0c2f358b1b67fdfba36d023acc,https://ulir.ul.ie/bitstream/10344/5407/2/Vassev_2016_safe.pdf,"In one aspect of our life or another, today we all live with AI. For example, the mechanisms behind the search engines operating on the Internet do not just retrieve information, but also constantly learn how to respond more rapidly and usefully to our requests. Although framed by its human inventors, this AI is getting stronger and more powerful every day to go beyond the original human intentions in the future. One of the major questions emerging along with the propagation of AI in both technology and life is about safety in AI. This paper presents the author’s view about how formal methods can assist us in building safer and reliable AI.",,
Safe AI Systems,The safe AI must satisfy with 2 majors applying the security measures and analyzing the system.,Search,2018,,Ananta  Bhatt,International Journal of Computer Applications,,10.5120/ijca2018918205,https://doi.org/10.5120/ijca2018918205,https://semanticscholar.org/paper/97aba8083bc5ad49af27a5c26c2bd40e6a17d6a3,,"As you know, it is inevitable to cease the advancement in AI because of the quest for developing systems that learns from experience and comply with the human capabilities. The leading ground-breaking impacts offers both positive and negative aspirations and are addressed as part of this research. To meet with the evil effects of the AI systems, is now the need of the hour to direct the focus on creating safe AI systems. The safe system must satisfy with 2 majorsapplying the security measures and analysing the system. Moreover, system must fulfil the risk and safety valuesintelligence, goals and safety in order to minimise the destruction that may occur due AI technology. However, further research is needed on the actual implementation of the safe AI systems in order to proceed with this research.",,
Safe and sound - artificial intelligence in hazardous applications,Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making.,Search,2000,272,"John  Fox, Subrata Kumar Das",,,,,https://semanticscholar.org/paper/f62d215d4c40d299929d2e7fa105d54623a9e1bf,,"Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making, where small faults or errors can spell human catastrophe. This book describes, from both practical and theoretical perspectives, an AI technology for supporting sound clinical decision making and safe patient management. The technology combines techniques from conventional software engineering with a systematic method for building intelligent agents. Although the focus is on medicine, many of the ideas can be applied to AI systems in other hazardous settings. The book also covers a number of general AI problems, including knowledge representation and expertise modeling, reasoning and decision making under uncertainty, planning and scheduling, and the design and implementation of intelligent agents.The book, written in an informal style, begins with the medical background and motivations, technical challenges, and proposed solutions. It then turns to a wide-ranging discussion of intelligent and autonomous agents, with particular reference to safety and hazard management. The final section provides a detailed discussion of the knowledge representation and other aspects of the agent model developed in the book, along with a formal logical semantics for the language.",,
Evaluation Schemes for Safe AGIs,A safe AI assessment requires testing for the AGI's reaction to 3 essential test cases.,Search,2014,,Deepak Justin Nath,AAAI Spring Symposia,,,,https://semanticscholar.org/paper/4d037fe1836bd5691ba4ffca14ef531ef8d0263f,,"Systems or agents with Artificial General Intelligence (AGI) are created to fulfill a particular or general goal. The agents are driven to achieve these mandated goals. This means that anything or person that stands in the way of the AGI reaching its goal is in danger as the AGI will learn that eliminating or removing the hindrance to its goal is an effective and necessary action to reach its goal. This paper proposes that evaluation scheme for safe AGI (Artificial General Intelligence) can be distilled down to 3 essential test cases. The paper explores various drives needed for an AGI system and how these drives generate actions and action sequences leading to AGI goals; the relationship between goals, drives, desires and motivation and a hypothetical abstraction levels in an AGI is mentioned here. The paper describes an AGI system with its goals and drives along with the various other aspect of this AGI system to provide arguments in favor of the 3 essential test cases.",,
Human factors challenges for the safe use of artificial intelligence in patient care,AI will require human factors research to ensure safe use in patient care.,Search,2019,21,"Mark  Sujan, Dominic  Furniss, Kath  Grundy, Howard  Grundy, David  Nelson, Matthew  Elliott, Sean  White, Ibrahim  Habli, Nick  Reynolds",BMJ Health & Care Informatics,,10.1136/bmjhci-2019-100081,https://doi.org/10.1136/bmjhci-2019-100081,https://semanticscholar.org/paper/9a99e50b44f176ea20ed40fe90b00f4f57762666,https://informatics.bmj.com/content/bmjhci/26/1/e100081.full.pdf,"The use of artificial intelligence (AI) in patient care can offer significant benefits. However, there is a lack of independent evaluation considering AI in use. The paper argues that consideration should be given to how AI will be incorporated into clinical processes and services. Human factors challenges that are likely to arise at this level include cognitive aspects (automation bias and human performance), handover and communication between clinicians and AI systems, situation awareness and the impact on the interaction with patients. Human factors research should accompany the development of AI from the outset.",,
Safety Challenges of AI in Autonomous Systems Design - Solutions from Human Factors Perspective Emphasizing AI Awareness,Artificial intelligence introduces safety challenges in autonomous systems.,Search,2020,2,"Hannu  Karvonen, Eetu  Heikkilä, Mikael  Wahlström",HCI,,10.1007/978-3-030-49183-3_12,https://doi.org/10.1007/978-3-030-49183-3_12,https://semanticscholar.org/paper/f556f921ece38e286420ea1cdf9d880285b0d6f9,,"Artificial intelligence (AI) is a key technology that is utilized in autonomous systems. However, using AI in such systems introduces also several safety challenges, many of which are related to complex human-AI interactions. In general, these challenges relate to the changing roles of the people who interact with the increasingly autonomous systems in various ways. In this paper, we consider a set of practical safety challenges related to applying advanced AI to autonomous machine systems and present solutions from the perspective of human factors. We apply the novel concept of AI awareness (AIA) to discuss the challenges in detail, and based on this, provide suggestions and guidelines for mitigating the AI safety risks with autonomous systems. In addition, we briefly consider the system design process to identify the actions that should be taken to ensure AIA throughout the different systems engineering design phases. The theoretical research presented in this paper aims to provide the first steps towards considering AIA in autonomous systems and understanding the human factors perspective viewpoint on AI safety for autonomous systems.",,
Multilayered review of safety approaches for machine learning-based systems in the days of AI,"Safety requirements engineering, safety-driven design at both system and machine learning component level, validation and verification from the perspective of software and system engineers.",Search,2021,1,"Sangeeta  Dey, Seok-Won  Lee",J. Syst. Softw.,,10.1016/j.jss.2021.110941,https://doi.org/10.1016/j.jss.2021.110941,https://semanticscholar.org/paper/d3b68a36315eed6008f2b615671930e7f4d81310,,"Abstract: The unprecedented advancement of artificial intelligence (AI) in recent years has altered our perspectives on software engineering and systems engineering as a whole. Nowadays, software-intensive intelligent systems rely more on a learning model than thousands of lines of codes. Such alteration has led to new research challenges in the engineering process that can ensure the safe and beneficial behavior of AI systems. This paper presents a literature survey of the significant efforts made in the last fifteen years to foster safety in complex intelligent systems. This survey covers relevant aspects of AI safety research including safety requirements engineering, safety-driven design at both system and machine learning (ML) component level, validation and verification from the perspective of software and system engineers. We categorize these research efforts based on a three-layered conceptual framework for developing and maintaining AI systems. We also perform a gap analysis to emphasize the open research challenges in ensuring safe AI. Finally, we conclude the paper by providing future research directions and a road map for AI safety.",,Review
Safe AI—is this possible?☆,"The inherently self-learning, self-adaptive nature of AI control systems makes them inherently unsafe.",Search,1995,11,M. G. Rodd,,,10.1016/0952-1976(95)00010-X,https://doi.org/10.1016/0952-1976(95)00010-X,https://semanticscholar.org/paper/c53376e4e49ca954f4f4f2ca520b6042967f7dda,,"Abstract This deliberately provocative paper poses the question as to whether it is professionally acceptable to use AI-based control systems in real-time automation. The premise is that the increasing demands for improved control of industrial processes are resulting in a search for new control techniques which, it is suggested, will be based on various AI methodologies. However, in the face of the inherently self-learning, self-adaptive nature of such techniques, it is pointed out that to produce safe, predictable systems, the controllers should be functionally deterministic! In posing this dilemma, the paper suggests possible ways in which it may be resolved.",,
Unifying Principles and Metrics for Safe and Assistive AI,"AI safety focuses on humans safely instructing and controlling AI systems, while AI impact on work focuses on the impact of AI on humans who may be unable to do so.",Search,2021,4,Siddharth  Srivastava,AAAI,,,,https://semanticscholar.org/paper/967cba5849f93869655af8d3d8fda9b0a3304a1d,,"The prevalence and success of AI applications have been tempered by concerns about the controllability of AI systems about AI’s impact on the future of work. These concerns reflect two aspects of a central question: how would humans work with AI systems? While research on AI safety focuses on designing AI systems that allow humans to safely instruct and control AI systems, research on AI and the future of work focuses on the impact of AI on humans who may be unable to do so. This Blue Sky Ideas paper proposes a unifying set of declarative principles that enable a more uniform evaluation of arbitrary AI systems along multiple dimensions of the extent to which they are suitable for use by specific classes of human operators. It leverages recent AI research and the unique strengths of the field to develop human-centric principles for AI systems that address the concerns noted above.",,
AI safety: state of the field through quantitative lens,There is a severe lack of research into concrete policies regarding AI.,Search,2020,7,"Mislav  Juric, Agneza  Sandic, Mario  Brcic","2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO)",,10.23919/mipro48935.2020.9245153,https://doi.org/10.23919/mipro48935.2020.9245153,https://semanticscholar.org/paper/2e5ab5250e524801e2efd24249d75fcbb80f2b99,http://arxiv.org/pdf/2002.05671,"Last decade has seen major improvements in the performance of artificial intelligence which has driven wide-spread applications. Unforeseen effects of such massad-option has put the notion of AI safety into the public eye. AI safety is a relatively new field of research focused on techniques for building AI beneficial for humans. While there exist survey papers for the field of AI safety, there is a lack of a quantitative look at the research being conducted. The quantitative aspect gives a data-driven insight about the emerging trends, knowledge gaps and potential areas for future research. In this paper, bibliometric analysis of the literature finds significant increase in research activity since 2015. Also, the field is so new that most of the technical issues are open, including: explainability and its long-term utility, and value alignment which we have identified as the most important long-term research topic. Equally, there is a severe lack of research into concrete policies regarding AI. As we expect AI to be the one of the main driving forces of changes, AI safety is the field under which we need to decide the direction of humanity’s future.",,
Safe and Ethical Artificial Intelligence in Radiotherapy - Lessons Learned From the Aviation Industry.,The Aletheia Framework from Rolls-Royce can guide organizations in developing safe and ethical AI systems.,Search,2021,1,"R  Hallows, L  Glazier, M S Katz, M  Aznar, M  Williams",Clinical oncology (Royal College of Radiologists (Great Britain)),,10.1016/j.clon.2021.11.019,https://doi.org/10.1016/j.clon.2021.11.019,https://semanticscholar.org/paper/518441aba1c5f08526564b51f7b516b9837ff964,,"Ethical artificial intelligence (AI) frameworks can be the catalyst in improving the safety and wellbeing of people when developing AI systems. In 2020 Rolls-Royce released its ethical and trustworthiness toolkit, The Aletheia FrameworkTM, which helps guide organisations as they consider the ethics around the use of AI. It covers three facets: social impact, accuracy and trust, and governance - which apply across all uses of AI. By adopting AI ethics and trust frameworks, oncologists can ensure the ratio between the benefit and harms of AI can be maximised. With AI transforming every sector, collaboration across industries to share ideas and learn from each other - even unlikely partnerships between engineering and oncology - could help optimise that transition.",,