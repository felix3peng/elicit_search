Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Revisiting Trust in Machines: Examining Human–Machine Trust Using a Reprogrammed Pasteurizer Task,"Faith governs overall trust early in the interaction with the automated system, then dependability, and finally predictability.",Search,2018,1,"Jieun  Lee, Yusuke  Yamani, Makoto  Itoh",Proceedings of the Human Factors and Ergonomics Society Annual Meeting,,10.1177/1541931218621400,https://doi.org/10.1177/1541931218621400,https://semanticscholar.org/paper/726a8ef851e554f9eb8427bb0164d70cd5f07528,,"Automated technologies have brought a number of benefits to professional domains, expanding the area in which humans can perform optimally in complex work environments. Human–automation trust has become an important aspect when designing acceptable automated systems considering general users who have no comprehensive knowledge of the systems. Muir and Moray (1996) proposed a model of human–machine trust incorporating predictability, dependability, and faith as predictors of overall trust in machines. Though Muir and Moray (1996) predicted that trust in machines grows from predictability, then dependability, and finally faith, their results suggested the opposite. This study will reexamine their theoretical framework and test which of the three dimensions governs initial trust in automation. Participants will be trained to operate a simulated pasteurization plant, as in Muir and Moray (1996), and they will be asked to maximize system performance in the pasteurizing task. We hypothesized that faith governs overall trust early in the interaction with the automated system, then dependability, and finally predictability as lay automation users become more familiar with the system. We attempt to replicate the results of Muir and Moray (1996) and argue that their model should be revised for trust development for general automation users.",,
Can We Trust Machines? The Role of Trust in Technological Environments,Trust is inextricably linked with a sort of fear.,Search,2020,2,Adriano  Fabris,"Studies in Applied Philosophy, Epistemology and Rational Ethics",,10.1007/978-3-030-44018-3_9,https://doi.org/10.1007/978-3-030-44018-3_9,https://semanticscholar.org/paper/b7415a498fcd03b654d1312d37a8421966d32406,,"In this paper, I will deal with trust in technological devices. Nowadays, we are led to trust AI machines, extending the notion of “trust” to our relationship with such devices. After reviewing different forms of trust and showing that trust is inextricably linked with a sort of fear, this paper will try to relate all this to the ethical judgement we can form about what machines “do” as well as about the machines themselves. Prompted by this, the paper will discuss the case of a recent attempt made by the EU at working out ethical guidelines for a trustworthy AI. To conclude, it will try to answer the above question—can we trust machines? –and accurately define on what terms they can be trusted.",,Review
Trust in social machines: the challenges,"The notions of trust often used in social machines research are imported from agent-based computing and are too formal, objective, and selective.",Search,2012,14,Kieron  O'Hara,,,,,https://semanticscholar.org/paper/68fb0224d56458915a291dde069fb16a606a8da2,,"The World Wide Web has ushered in a new generation of applications constructively linking people and computers to create what have been called ‘social machines.’ The ‘components’ of these machines are people and technologies. It has long been recognised that for people to participate in social machines, they have to trust the processes. However, the notions of trust often used tend to be imported from agent-based computing, and may be too formal, objective and selective to describe human trust accurately. This paper applies a theory of human trust to social machines research, and sets out some of the challenges to system designers.",,
The method used to configure the trust machines and systems,A trusted virtual machine is installed for each terminal to solve the issue of inadequate machine trust resulting in a waste of computer resources.,Search,2012,,"邓振波, 张家柱, 温铭, 李宇",,,,,https://semanticscholar.org/paper/6b242da42c7b6feedb6521b212803273d215bcaa,,"Embodiments of the present invention provides a method and a system configured to trust machines used to solve the set of trust machines and cause the computer to function without adequate, resulting in a waste of computer resources issues. The system comprises a server and a plurality of terminals; the terminal comprising: a generating module; wherein the trusted virtual machine is installed for the terminal, further comprising adding module; said server comprising: a receiving module; detecting module adapted to obtain random a terminal information, the existence of the virtual machine information terminal detection information from the control set; if so, trigger configuration module, and if not, return detection module, retrieve information from a terminal of the control set to continue perform detection operation; configuration module; wherein the virtual machine identification information includes a virtual machine, the virtual machine is identified as a trusted virtual machine virtual hardware unique identification.",,
Trusting Intelligent Machines: Deepening Trust Within Socio-Technical Systems,Trust between humans and intelligent machines is required for continued economic growth.,Search,2018,49,"Peter  Andras, Lukas  Esterle, Michael  Guckert, Han The Anh, Peter R. Lewis, Kristina  Milanovic, Terry R. Payne, Cedric  Perret, Jeremy V. Pitt, Simon T. Powers, Neil  Urquhart, Simon  Wells",IEEE Technology and Society Magazine,,10.1109/MTS.2018.2876107,https://doi.org/10.1109/MTS.2018.2876107,https://semanticscholar.org/paper/2e8aabe1363b994218f8578344fea1d98bedc54e,https://publications.aston.ac.uk/id/eprint/37825/1/IEEE_Trust_Article_R1B.pdf,"Intelligent machines have reached capabilities that go beyond a level that a human being can fully comprehend without sufficiently detailed understanding of the underlying mechanisms. The choice of moves in the game Go (generated by Deep Mind?s Alpha Go Zero [1]) are an impressive example of an artificial intelligence system calculating results that even a human expert for the game can hardly retrace [2]. But this is, quite literally, a toy example. In reality, intelligent algorithms are encroaching more and more into our everyday lives, be it through algorithms that recommend products for us to buy, or whole systems such as driverless vehicles. We are delegating ever more aspects of our daily routines to machines, and this trend looks set to continue in the future. Indeed, continued economic growth is set to depend on it. The nature of human-computer interaction in the world that the digital transformation is creating will require (mutual) trust between humans and intelligent, or seemingly intelligent, machines. But what does it mean to trust an intelligent machine? How can trust be established between human societies and intelligent machines?",,
Trust in automation. Part II. Experimental studies of trust and human intervention in a process control simulation.,Trust in an automation is based mainly upon the automation's competence.,Search,1996,780,"B M Muir, N  Moray",Ergonomics,,10.1080/00140139608964474,https://doi.org/10.1080/00140139608964474,https://semanticscholar.org/paper/0fea4371d73960c8fcb17e6d94123aba9905a7ab,,"Two experiments are reported which examined operators' trust in and use of the automation in a simulated supervisory process control task. Tests of the integrated model of human trust in machines proposed by Muir (1994) showed that models of interpersonal trust capture some important aspects of the nature and dynamics of human-machine trust. Results showed that operators' subjective ratings of trust in the automation were based mainly upon their perception of its competence. Trust was significantly reduced by any sign of incompetence in the automation, even one which had no effect on overall system performance. Operators' trust changed very little with experience, with a few notable exceptions. Distrust in one function of an automatic component spread to reduce trust in another function of the same component, but did not generalize to another independent automatic component in the same system, or to other systems. There was high positive correlation between operators' trust in and use of the automation; operators used automation they trusted and rejected automation they distrusted, preferring to do the control task manually. There was an inverse relationship between trust and monitoring of the automation. These results suggest that operators' subjective ratings of trust and the properties of the automation which determine their trust, can be used to predict and optimize the dynamic allocation of functions in automated systems.",,
Laboratory studies of trust between humans and machines in automated systems,Laboratory experiments show that the dynamics of trust between humans and machines and the calibration of trust are important.,Search,1999,72,"Neville  Moray, Toshiyuki  Inagaki",,,10.1177/014233129902100408,https://doi.org/10.1177/014233129902100408,https://semanticscholar.org/paper/15eed532be929410fd86e5088295e1a4937ae934,,"In supervisory control, operators are expected to monitor automation and to intervene if there is an opportunity to improve system productivity or if faults develop which cannot be managed by the automation. Central to how humans interact with automation is the degree to which they trust the system to perform well and handle unforeseen events. This paper summarises recent laboratory experiments and theoretical models, both quantitative and qualitative, of the dynamics of trust between humans and machines and discusses the calibration of trust and the problem of allocating responsibility for control between human and machine.",,
Trusting your computer to be trusted,The spread of ‘trust’ is unstoppable.,Search,2005,4,Stephen  Mason,,,10.1016/S1361-3723(05)00146-6,https://doi.org/10.1016/S1361-3723(05)00146-6,https://semanticscholar.org/paper/6305a869d0c79b38d4ee7214d7cf9b50a72e8c95,,"The spread of ‘trust’ is unstoppable… The Trusted Computing Group is a movement from the most formidable software and hardware manufacturers in the world to develop an architecture that would improve the underlying security of computers. The Trusted Computing Group aims to make sure a computer is working to a specified expectation of behaviour. This would apply to the problem of malicious code, for example. Malicious code makes a computer behave differently to the expected normality so cannot be hidden. The measurement of such changes in behaviour is done by a Trusted Platform Module, which is implicity trusted and is the core of the trusted computing architecture. Sound too good to be true? Some commentators believe these security benefits could come at a price, which may include digital rights management or privacy compromise. Stephen Mason believes that the implications of Trusted Computing are profound and there are serious concerns, which must be addressed before the technology become ubiquitous, which it is well on the way to becoming. It is axiomatic that computers are not to be trusted. As a tool, they are capable of improving our lives. However, beneficial as computers may be, they are also unsafe. The dangers are obvious, although what you consider is a threat will depend on your perspective. Application software vendors do not always receive licence fees for all of their software running on every computer. Users have to buy anti-virus solutions to protect their computers from attack by malicious code. Owners of secrets have to take precautions to prevent the unauthorized use or theft of sensitive data. Musicians, actresses and authors want to be properly remunerated for their creativity. The range of problems associated with the misuse of computers is manifest, and once a computer is linked into a network, the risks increase considerably.",,
Do I Trust a Machine? Differences in User Trust Based on System Performance,The accuracy of a simulated automated quality monitoring system affects user trust.,Search,2018,11,"Kun  Yu, Shlomo  Berkovsky, Dan  Conway, Ronnie  Taib, Jianlong  Zhou, Fang  Chen",Human and Machine Learning,,10.1007/978-3-319-90403-0_12,https://doi.org/10.1007/978-3-319-90403-0_12,https://semanticscholar.org/paper/82965e3f0d836fc937c1d16e222b59ab296ab702,,"Trust plays an important role in various user-facing systems and applications. It is particularly important in the context of decision support systems, where the system’s output serves as one of the inputs for the users’ decision making processes. In this chapter, we study the dynamics of explicit and implicit user trust in a simulated automated quality monitoring system, as a function of the system accuracy. We establish that users correctly perceive the accuracy of the system and adjust their trust accordingly. The results also show notable differences between two groups of users and indicate a possible threshold in the acceptance of the system. This important learning can be leveraged by designers of practical systems for sustaining the desired level of user trust.",,
Efficient trust chain model based on turing machine,A trusted mobile terminal system can be protected from malicious attacks with little impact to system performance.,Search,2015,1,"Li  Tao, Hu  Aiqun",Secur. Commun. Networks,,10.1002/sec.808,https://doi.org/10.1002/sec.808,https://semanticscholar.org/paper/66b6c3f77fc0d7b7f02758247d90b058d6264b2d,,"Trust chain, which focuses on the security in trusted computing platform, is the key technology to ensure system security. Aiming to establish the trust chain for mobile terminals, this paper proposes a trusted turing machine to formally describe the trust transitive process and construct an efficient trust chain model during the system boot time and the run time. The model consists of the following two characteristics. First, the boot code and operating system image are stored in Root of Trusted Storage. This structure provides more safety, reliability, and efficiency than that proposed by Trusted Computing Group. Second, a resource-oriented protecting scheme is designed during the system run time. A process can access specific resources on the condition that it has been granted trust property by the related verifying program. In addition, we also develop a prototype of trusted mobile terminal systems. Results show that the system boot time is shortened by 5.2s. In the meantime, the dynamic trusted mechanism executed during system run time can efficiently protect platform from malicious attack while it has little impact to system performance. The proposed model has the trust transitive property of the trust chain and can be applied to build a high efficiency trusted mobile terminal. Copyright © 2013 John Wiley & Sons, Ltd.",,
The Use of Trust in Social Machines,Trust plays an important role in the effective working of Social Machines by allowing for cooperative behavior amongst human and digital components of the system.,Search,2016,4,"Arpit  Merchant, Tushant  Jha, Navjyoti  Singh",WWW,,10.1145/2872518.2890597,https://doi.org/10.1145/2872518.2890597,https://semanticscholar.org/paper/e0501a89cc26b0df64b82f352cf40a987e6419c7,,"Trust plays an important role in the effective working of Social Machines by allowing for cooperative behaviour amongst human and digital components of the system. A detailed study of trust helps in gaining insights into the working of social machines, and allows designers to create better systems which are able to engage more people and allow for efficient operations. In this paper, we undertake a discussion on the variety of ways in which trust can be observed in Social Machines by outlining a three class taxonomy (personal, social and functional). We build upon earlier observations in past literature while seeking a broader definition. We discuss the problem of trust, that of promoting trust amongst the trustworthy in social machines, and present the various insights, challenges and frontiers that arise in response. This includes the role of institutions, communication processes and value aligned technologies in social, personal and functional trust respectively.",,
When to (or not to) trust intelligent machines: Insights from an evolutionary game theory analysis of trust in repeated games,People use trust-based strategies more frequently in interactions with intelligent agents.,Search,2021,7,"Han The Anh, Cedric  Perret, Simon T. Powers",Cognitive Systems Research,,10.1016/j.cogsys.2021.02.003,https://doi.org/10.1016/j.cogsys.2021.02.003,https://semanticscholar.org/paper/f2730aabf3b4e7daac81c4593ee7f6b950f2ac8c,https://research.tees.ac.uk/ws/files/24791738/2007.11338.pdf,"The actions of intelligent agents, such as chatbots, recommender systems, and virtual assistants are typically not fully transparent to the user. Consequently, using such an agent involves the user exposing themselves to the risk that the agent may act in a way opposed to the user's goals. It is often argued that people use trust as a cognitive shortcut to reduce the complexity of such interactions. Here we formalise this by using the methods of evolutionary game theory to study the viability of trust-based strategies in repeated games. These are reciprocal strategies that cooperate as long as the other player is observed to be cooperating. Unlike classic reciprocal strategies, once mutual cooperation has been observed for a threshold number of rounds they stop checking their co-player's behaviour every round, and instead only check with some probability. By doing so, they reduce the opportunity cost of verifying whether the action of their co-player was actually cooperative. We demonstrate that these trust-based strategies can outcompete strategies that are always conditional, such as Tit-for-Tat, when the opportunity cost is non-negligible. We argue that this cost is likely to be greater when the interaction is between people and intelligent agents, because of the reduced transparency of the agent. Consequently, we expect people to use trust-based strategies more frequently in interactions with intelligent agents. Our results provide new, important insights into the design of mechanisms for facilitating interactions between humans and intelligent agents, where trust is an essential factor.",,
The relationship between trust in AI and trustworthy machine learning technologies,Trust can be impacted throughout the life cycle of AI-based systems.,Search,2020,50,"Ehsan  Toreini, Mhairi  Aitken, Kovila  Coopamootoo, Karen  Elliott, Carlos Gonzalez Zelaya, Aad van Moorsel",FAT*,,10.1145/3351095.3372834,https://doi.org/10.1145/3351095.3372834,https://semanticscholar.org/paper/bd4cf5a6987ef0f39b7e4e88d59b3a3365abcc70,http://arxiv.org/pdf/1912.00782,"To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.",,
A theoretical model for trust in automated systems,Researchers have used various automated systems in unique experimental paradigms to identify factors that regulate the trust formation process.,Search,2013,15,"Kevin  Hoff, Masooda  Bashir",CHI Extended Abstracts,,10.1145/2468356.2468378,https://doi.org/10.1145/2468356.2468378,https://semanticscholar.org/paper/71396a48ad4b7a82f0c73e6bb51f561e718ad8aa,,"The concept of trust in automation has received a great deal of attention in recent years in response to modern society's ever-increasing usage of automated systems. Researchers have used a variety of different automated systems in unique experimental paradigms to identify factors that regulate the trust formation process. In this work-in-progress report, we propose a preliminary, theoretical model of factors that influence trust in automation. Our model utilizes three layers of analysis (dispositional trust, situational trust, and learned trust) to explain the variability of trust in a wide range of circumstances. We are in the process of verifying certain aspects of the model empirically, but our current framework provides a useful perspective for future investigations into the intricacies of trust in automated systems.",,
Trust in Automation,"dispositional trust, situational trust, and learned trust.",Search,2015,751,"Kevin  Hoff, Masooda  Bashir",Human factors,,10.1177/0018720814547570,https://doi.org/10.1177/0018720814547570,https://semanticscholar.org/paper/95d66e12bb9116c3e98c7d422decd6b5c74769dc,,"Objective: We systematically review recent empirical research on factors that influence trust in automation to present a three-layered trust model that synthesizes existing knowledge. Background: Much of the existing research on factors that guide human-automation interaction is centered around trust, a variable that often determines the willingness of human operators to rely on automation. Studies have utilized a variety of different automated systems in diverse experimental paradigms to identify factors that impact operators’ trust. Method: We performed a systematic review of empirical research on trust in automation from January 2002 to June 2013. Papers were deemed eligible only if they reported the results of a human-subjects experiment in which humans interacted with an automated system in order to achieve a goal. Additionally, a relationship between trust (or a trust-related behavior) and another variable had to be measured. All together, 101 total papers, containing 127 eligible studies, were included in the review. Results: Our analysis revealed three layers of variability in human–automation trust (dispositional trust, situational trust, and learned trust), which we organize into a model. We propose design recommendations for creating trustworthy automation and identify environmental conditions that can affect the strength of the relationship between trust and reliance. Future research directions are also discussed for each layer of trust. Conclusion: Our three-layered trust model provides a new lens for conceptualizing the variability of trust in automation. Its structure can be applied to help guide future research and develop training interventions and design procedures that encourage appropriate trust.",,Systematic Review
Trust in Automation A Literature Review,Increasingly complex autonomous systems require the human operator to appropriately calibrate their trust in the automation.,Search,2018,7,"Bronwyn  French, Andreas  Duenser, Andrew  Heathcote",,,,,https://semanticscholar.org/paper/92f07d3d1356307decb6e97382ad884d0f62668d,,"Increasingly complex autonomous systems require the human operator to appropriately calibrate their trust in the automation in order to achieve performance and safety goals. Although humanautomation trust has been shown to be similar in many respects to human-human trust, precisely defining and measuring trust in automation has proved to be one of the greatest challenges facing research in this area. This report reviews literature pertaining to trust in automated systems to provide an integrated summary of the major theoretical and empirical work in the field to date.",,Review