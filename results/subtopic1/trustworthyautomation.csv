Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Trustworthy Laboratory Automation,"Sustainability, integrity, and authenticity of data is essential for scientific research.",Search,2014,2,"Jan  Potthoff, Dominic  Lütjohann, Nicole  Jung",DBKDA 2014,,,,https://semanticscholar.org/paper/905488d862f4dcf4020d728c84b33a0b2f182cc5,,"To ensure the quality of scientific data, the integrity and authenticity of the data has to be guaranteed. Due to the significance of the primary data, the integrity and authenticity of research data have to be ensured with their generation. Special measurement devices offer this possibility by an automated digital signing process. Unfortunately, these devices are rare. Therefore, a generic software application which is shown in this paper has been implemented. Furthermore, the solution has been adjusted to an existing laboratory automation system which is depicted as well. The combination of a new, web-based Laboratory Information and Management System (LIMS) with a new protocol for an authentication of electronic data shows that the establishment of an automated collection of secure data can be realized even without disturbing the researcher. Keywords-integrity and authenticity; sustainability; trustworthy data generation; data management",,
Trust in Automation A Literature Review,Increasingly complex autonomous systems require the human operator to appropriately calibrate their trust in the automation.,Search,2018,7,"Bronwyn  French, Andreas  Duenser, Andrew  Heathcote",,,,,https://semanticscholar.org/paper/92f07d3d1356307decb6e97382ad884d0f62668d,,"Increasingly complex autonomous systems require the human operator to appropriately calibrate their trust in the automation in order to achieve performance and safety goals. Although humanautomation trust has been shown to be similar in many respects to human-human trust, precisely defining and measuring trust in automation has proved to be one of the greatest challenges facing research in this area. This report reviews literature pertaining to trust in automated systems to provide an integrated summary of the major theoretical and empirical work in the field to date.",,Review
Designing Trustworthy Automation,Safety-critical domains require appropriate calibration of operator trust in automation.,Search,2015,1,Arathi  Sethumadhavan,,,10.1177/1064804615609923,https://doi.org/10.1177/1064804615609923,https://semanticscholar.org/paper/a704b808e1d8db9ea36400f73f576bb1c73ed7a4,,"From self-learning thermostats to agricultural vehicles to baggagescreening systems, automation is becoming an integral part of modern life. In safety-critical domains, automation offers incredible benefits by improving the accuracy of complex tasks. Because automation failures are inevitable, appropriate calibration of operator trust in automation is integral for the optimal performance of human– automation teams. Trust in automation is affected by several factors, such as automation reliability, operator workload and selfconfidence, task difficulty and novelty, consequences of automation misses, and automation display format and content. Trust is therefore a multidimensional concept. So what can be done to appropriately calibrate operator trust level on automation?",,
Trust in Automation,"dispositional trust, situational trust, and learned trust.",Search,2015,751,"Kevin  Hoff, Masooda  Bashir",Human factors,,10.1177/0018720814547570,https://doi.org/10.1177/0018720814547570,https://semanticscholar.org/paper/95d66e12bb9116c3e98c7d422decd6b5c74769dc,,"Objective: We systematically review recent empirical research on factors that influence trust in automation to present a three-layered trust model that synthesizes existing knowledge. Background: Much of the existing research on factors that guide human-automation interaction is centered around trust, a variable that often determines the willingness of human operators to rely on automation. Studies have utilized a variety of different automated systems in diverse experimental paradigms to identify factors that impact operators’ trust. Method: We performed a systematic review of empirical research on trust in automation from January 2002 to June 2013. Papers were deemed eligible only if they reported the results of a human-subjects experiment in which humans interacted with an automated system in order to achieve a goal. Additionally, a relationship between trust (or a trust-related behavior) and another variable had to be measured. All together, 101 total papers, containing 127 eligible studies, were included in the review. Results: Our analysis revealed three layers of variability in human–automation trust (dispositional trust, situational trust, and learned trust), which we organize into a model. We propose design recommendations for creating trustworthy automation and identify environmental conditions that can affect the strength of the relationship between trust and reliance. Future research directions are also discussed for each layer of trust. Conclusion: Our three-layered trust model provides a new lens for conceptualizing the variability of trust in automation. Its structure can be applied to help guide future research and develop training interventions and design procedures that encourage appropriate trust.",,Systematic Review
Trustworthiness of Autonomous Systems,Transdisciplinary research is needed to design trustworthy autonomous systems.,Search,2018,17,S. Kate Devitt,CDC 2018,,10.1007/978-3-319-64816-3_9,https://doi.org/10.1007/978-3-319-64816-3_9,https://semanticscholar.org/paper/d84e786286b6afa167ca016a01cf0d8e408df76e,https://link.springer.com/content/pdf/10.1007%2F978-3-319-64816-3_9.pdf,"Effective robots and autonomous systems must be trustworthy. This chapter examines models of trustworthiness from a philosophical and empirical perspective to inform the design and adoption of autonomous systems. Trustworthiness is a property of trusted agents or organisations that engenders trust in other agent or organisations. Trust is a complex phenomena defined differently depending on the discipline. This chapter aims to bring different approaches under a single framework for investigation with three sorts of questions: Who or what is trustworthy? –metaphysics. How do we know who or what is trustworthy? –epistemology. What factors influence what or who should we trust? –normativity. A two-component model of trust is used that incorporates competence (skills, reliability and experience) and integrity (motives, honesty and character). It is supposed that human levels of competence yield the highest trust whereas trust is reduced at sub-human and super-human levels. The threshold for trustworthiness of an agent or organisation in a particular context is a function of their relationship with the truster and potential impacts of decisions. Building trustworthy autonomous systems requires obeying the norms of logic, rationality and ethics under pragmatic constraints–even though there is disagreement on these principles by experts. Autonomous systems may need sophisticated social identities including empathy and reputational concerns to build human-like trust relationships. Ultimately transdisciplinary research drawing on metaphysical, epistemological and normative human and machine theories of trust are needed to design trustworthy autonomous systems for adoption.",,
Development of Metrics for Trust in Automation,A method to accurately measure its goodness or level during active interaction between a human and automation would be beneficial.,Search,2010,11,"Janet E. Miller, LeeAnn  Perkins",,,,,https://semanticscholar.org/paper/9cb395e1d6e5024e58d00db62e166eb8b4fcaf64,,"Abstract : Research in ""trust"" in automation has gained momentum and ""trust"" has been identified as playing an essential role for implementing effective work-centered computer systems. In a socio-technical work system, the automation handles the majority of an algorithmically-intense workload but the human is generally a final decision-maker. Therefore, the human's acceptance of the automation's output is required for a successful result. Some researchers believe that system failures are connected to the human nature of trust, which is based on experiences with other humans, acting as the foundation for reliance on automated systems. However, using a common word as ""trust"" allows for misunderstandings when used in multiple contexts. While all have some overtures of similarity, there are important unstated differences. Additionally, if trust is critical, then a method to accurately measure its goodness or level during active interaction between a human and automation would be beneficial. This paper will discuss three qualifiers for a trust evaluation such that measures can be developed to gauge a user's trust perception over time; will lay out five components to specifically evaluate trust in automation, and propose a technique for measuring and monitoring trust in automation.",,
Trustworthy Human-Centered Automation Through Explainable AI and High-Fidelity Simulation,Shared expectations between autonomous systems and the humans who interact with them can increase the trust in the systems.,Search,2020,2,"Bradley  Hayes, Michael  Moniz",AHFE,,10.1007/978-3-030-51064-0_1,https://doi.org/10.1007/978-3-030-51064-0_1,https://semanticscholar.org/paper/d59e02b072628ce304e154d12804ffcc9f18d4de,,"As we become more competent developers of artificially intelligent systems, the level of deployment and associated implicit trust in these systems will increase in kind. While this is an attractive concept, with an already-demonstrated capability to positively disrupt industries around the world, it remains a dangerous premise that demands attention and intentional resource allocation to ensure that these systems’ behaviors match our expectations. Until we can develop explainable AI techniques or high-fidelity simulators to enable us to examine the models’ underlying logic for the situations we intend to utilize them in, it will be irresponsible to place our trust in their ability to act on our behalf. In this work we describe and provide guidelines for ongoing efforts in using novel explainable AI techniques and high-fidelity simulation to help establish shared expectations between autonomous systems and the humans who interact with them, discussing collaborative robotics and cybersecurity domains.",,
A Brief Review of Frequently Used Self-Report Measures of Trust in Automation,There is a strong need for a review of which methods are used to measure trust in automation.,Search,2020,1,"Spencer C. Kohn, Molly  Kluck, Tyler  Shaw",,,10.1177/1071181320641342,https://doi.org/10.1177/1071181320641342,https://semanticscholar.org/paper/25e40a315c02ca93fc8182d8b7c92db8c140b973,,"With the growing popularity of trust in automation research, there is a strong need for a review of which methods are used to measure trust in automation. This review catalogues the self-report methods commonly used to measure trust in automation, via publications found in the PsychINFO and ACM databases. The frequency of commonly deployed self-report scales are reported and the state of self-report measurement of trust in automation is discussed in brief.",,Review
Verifying Automation Trust in Automated Driving System from Potential Users' Perspective,"Factors related to the vehicle itself, specifically its reliability, had the greatest association with trust.",Search,2020,,"Jue  Li, Chun  Meng, Long  Liu",AHFE,,10.1007/978-3-030-51758-8_5,https://doi.org/10.1007/978-3-030-51758-8_5,https://semanticscholar.org/paper/76a62c21c85032d948af5be3b711e079c5de6db3,,"Trust was recognized as a key element in automation as it relates to the system safety and performance. However, automation trust established in research of automation industry may not be used directly into automated driving system. In order to identify automation trust in automated driving, a semi-structed interview was conducted based on a systematic review of automation trust with potential users. Results show that factors related to the vehicle itself, specifically its reliability, had the greatest association with trust. And it is difficult to apply automation trust theory directly to human-automated vehicle interaction as the driving task itself is dangerous and user of automated vehicle (AV) are usually not skilled operators. This study provides a new lens for conceptualizing automation trust, which can be applied to help guide future research and design procedures that enhance driver–automation cooperation.",,Systematic Review
Trust in Automation: A Critical Review,"Automation has opened new doors for solution of long standing problems, but at same time has posed new challenges in human-machine interaction.",Search,2015,,"Janmejay  Pradhan, Inder  Singh, Anjali  Samal",,,,,https://semanticscholar.org/paper/7fd0679173579c8bd2bda8fe2efc0a02a5e4bafd,,"Current era of time can be characterized by the use of sophisticated instruments in almost all walks of life. These gadgets are handling those functions which were previously done by humans. Transfers of functions have thrown light on new avenues of knowledge pursuit. New horizons are being probed in general and reliability of the automated machines in particular. Human operators develop a state of trust while working on these machines. This paper has deliberated on the different aspects on trust in automated systems with the issues pertaining to use, misuse and disuse of the automation. Automation has opened new doors for solution of long standing problems, but at same time has posed new challenges in human-machine interaction.",,Review
Adaptive Cognitive Mechanisms to Maintain Calibrated Trust and Reliance in Automation,Trust calibration for a human-machine team is the process by which a human adjusts their expectations of the automation’s reliability and trustworthiness.,Search,2021,,"Christian  Lebiere, Leslie M. Blaha, Corey K. Fallon, Brett  Jefferson",Frontiers in Robotics and AI,,10.3389/frobt.2021.652776,https://doi.org/10.3389/frobt.2021.652776,https://semanticscholar.org/paper/487686514f14e7504c4336902ce292d158c2a812,https://www.frontiersin.org/articles/10.3389/frobt.2021.652776/pdf,"Trust calibration for a human–machine team is the process by which a human adjusts their expectations of the automation’s reliability and trustworthiness; adaptive support for trust calibration is needed to engender appropriate reliance on automation. Herein, we leverage an instance-based learning ACT-R cognitive model of decisions to obtain and rely on an automated assistant for visual search in a UAV interface. This cognitive model matches well with the human predictive power statistics measuring reliance decisions; we obtain from the model an internal estimate of automation reliability that mirrors human subjective ratings. The model is able to predict the effect of various potential disruptions, such as environmental changes or particular classes of adversarial intrusions on human trust in automation. Finally, we consider the use of model predictions to improve automation transparency that account for human cognitive biases in order to optimize the bidirectional interaction between human and machine through supporting trust calibration. The implications of our findings for the design of reliable and trustworthy automation are discussed.",,
Trust in Automated Vehicles,The issue of trust in automated technology becomes more salient the more the driver delegates control to the vehicle.,Search,2018,5,"Alexander G. Mirnig, Sandra  Trösterer, Alexander  Meschtscherjakov, Magdalena  Gärtner, Manfred  Tscheligi",i-com,,10.1515/icom-2017-0031,https://doi.org/10.1515/icom-2017-0031,https://semanticscholar.org/paper/7f87e52729ca0d95b6bec9ccfb05fd04864cb6eb,,"Abstract Increasing degrees of automation in on-road vehicles bear great potential for heightened driver safety and traffic efficiency in both the near and far future. The more the driver delegates control to the vehicle, the more salient the issue of trust in automated technology becomes. Misaligned trust can lead to mishandling of automation controls in individual instances and decreases the general acceptance of on-road automation on a broader scale. In this paper, we apply insights from trust research for dynamic web service interaction to the novel automated driving domain, in order to scope the problem space regarding trust in automated vehicles. We conclude that the appropriate communication of trustworthiness, the necessity to calibrate trust, the importance of intervention capabilities by the driver, and the unambiguous transparency of locus of control are all important aspects when in comes to understanding trust in automated vehicles.",,
The role of trust in automation reliance,Viewing slides of Fort Sill terrain and indicating the presence or absence of a camouflaged soldier involves trust in automation.,Search,2003,783,"Mary T. Dzindolet, Scott A. Peterson, Regina A. Pomranky, Linda G. Pierce, Hall P. Beck",Int. J. Hum. Comput. Stud.,,10.1016/S1071-5819(03)00038-7,https://doi.org/10.1016/S1071-5819(03)00038-7,https://semanticscholar.org/paper/b0e89da3eb67767b63ca5c23c233238f70263ae7,,"A recent and dramatic increase in the use of automation has not yielded comparable improvements in performance. Researchers have found human operators often underutilize (disuse) and overly rely on (misuse) automated aids (Parasuraman and Riley, 1997). Three studies were performed with Cameron University students to explore the relationship among automation reliability, trust, and reliance. With the assistance of an automated decision aid, participants viewed slides of Fort Sill terrain and indicated the presence or absence of a camouflaged soldier. Results from the three studies indicate that trust is an important factor in understanding automation reliance decisions. Participants initially considered the automated decision aid trustworthy and reliable. After observing the automated aid make errors, participants distrusted even reliable aids, unless an explanation was provided regarding why the aid might err. Knowing why the aid might err increased trust in the decision aid and increased automation reliance, even when the trust was unwarranted. Our studies suggest a need for future research focused on understanding automation use, examining individual differences in automation reliance, and developing valid and reliable self-report measures of trust in automation.",,
Towards Dependable Automation,Information security for automation systems should be based on the dependability required from automation.,Search,2015,3,"Jari  Seppälä, Mikko  Salmenperä",,,10.1007/978-3-319-18302-2_15,https://doi.org/10.1007/978-3-319-18302-2_15,https://semanticscholar.org/paper/3156203bce173598b1e6cb4ced972cf47f9a02ac,,"Automation runs the modern society and it’s critical systems. It is a networked software product depending on the co-operation of old and new technologies. Information security for automation systems should be regarded in light of the most important quality required from automation—dependability. This chapter focuses on process of developing dependable solutions for the entire lifecycle of automation systems. The approach includes a guideline for securing automation and a dependability model that is a data flow model extended with security and automation requirements. Results of this analysis should be used in final requirement specification for implementation. Dependability model is the key tool in secure development lifecycle. It can be used in new product development, improving an old automation system and also during the active lifecycle of automation to manage inevitable changes occurring during the entire lifespan of automation system.",,
The challenges and opportunities of artificial intelligence in implementing trustworthy robotics and autonomous systems,"Robots should provide high-quality services, as “No trust, no use”.",Search,2020,3,"Hongmei  He, J.  Gray, Angelo  Cangelosi, Q.  Meng, T. M. McGinnity, J.  Mehnen",,,,,https://semanticscholar.org/paper/3e130750bc9d95e12b4194b67137d89da0f87a07,,"Effective Robots and Autonomous Systems (RAS) must be trustworthy. Trust is essential in designing autonomous and semi-autonomous technologies, because “No trust, no use”. RAS should provide high quality of services, with the four key properties that make it trust, i.e. they must be (i) robust for any health issues, (ii) safe for any matters in their surrounding environments, (iii) secure for any threats from cyber spaces, and (iv) trusted for human-machine interaction. We have thoroughly analysed the challenges in implementing the trustworthy RAS in respects of the four properties, and addressed the power of AI in improving the trustworthiness of RAS. While we put our eyes on the beneﬁts that AI brings to human, we should realise the potential risks that could be caused by AI. The new concept of human-centred AI will be the core in implementing the trustworthy RAS. This review could provide a brief reference for the research on AI for trustworthy RAS.",,Review
Trust and Automation in Verification Tools,Verification tools can be both powerful and trusted through a verified reference kernel that checks the results of untrusted tools.,Search,2008,23,Natarajan  Shankar,ATVA,,10.1007/978-3-540-88387-6_3,https://doi.org/10.1007/978-3-540-88387-6_3,https://semanticscholar.org/paper/a11f03fc879aac7dd7a127a622b76e555431300c,,"On the one hand, we would like verification tools to feature powerful automation, but on the other hand, we also want to be able to trust the results with a high degree of confidence. The question of trust in verification tools has been debated for a long time. One popular way of achieving trust in verification tools is through proof generation. However, proof generation could hamstring both the functionality and the efficiency of the automation that can be built into these tools. We argue that trust need not be achieved at the expense of automation, and outline a lightweight approach where the results of untrusted verifiers are checked by a trusted offline checker. The trusted checker is a verified reference kernel that contains a satisfiability solver to support the robust and efficient checking of untrusted tools.",,
Automated Trust Negotiation in Autonomic Environments,Autonomic computing environments rely on devices that are able to make intelligent decisions without human supervision.,Search,2007,6,"Andreas  Klenk, Frank  Petri, Benoit  Radier, Mikaël  Salaün, Georg  Carle",IWSOS,,10.1007/978-3-540-74917-2_22,https://doi.org/10.1007/978-3-540-74917-2_22,https://semanticscholar.org/paper/b5d5c6ce893fdf296cfdb83a0c9bcf0194e9a1b8,http://www.net.in.tum.de/fileadmin/RI/members/klenk/iwsos2007.pdf,Autonomic computing environments rely on devices that are able to make intelligent decisions without human supervision. Automated Trust Negotiation supports the cooperation of devices with no prior trust relationship. They can reach an agreement by iteratively exchanging credentials during a negotiation process. These credentials can serve as authorization tokens or may carry information that becomes a parameter of the further service usage. A careful negotiation strategy helps in protecting sensitive credentials that must only be available to authorized entities. We introduce the VersaTrust framework that supports a stateless negotiation protocol to reach comprehensive agreements. We argue how this approach applies to autonomic environments and demonstrate its scalability.,,
Quantifying Compliance and Reliance Trust Behaviors to Influence Trust in Human-Automation Teams,The United States Department of Defense is investigating trust in automation in order to influence the rate of adoption of automation technology.,Search,2017,6,"Jayson G. Boubin, Christina F. Rusnock, Jason M. Bindewald",,,10.1177/1541931213601672,https://doi.org/10.1177/1541931213601672,https://semanticscholar.org/paper/491a4f0e17e05fdf3fcd91a588ff50b2fbc56fde,,"Automation is utilized heavily in many domains to increase productivity. With new, more complex automation, like the self-driving car, humans will be required to forego direct task performance in favor of maintaining a supervisory role over automation systems. While the use of these systems generally results in greater performance than humans performing alone, humans are reluctant to adopt these superior systems due to a lack of trust. The United States Department of Defense is investigating trust in automation in order to influence the rate of adoption of automation technology. Studying trust in automation systems requires a mechanism for quantifying and measuring trust. This paper proposes a method for measuring human trust behaviors with regard to human-automation systems through response rates of compliance and reliance. Using behavioral data from a human-subjects experiment involving automated agents, we create a system dynamics model which relates trust to other system level variables. Using this trust model, engineers will be able to study trust in human-automation team scenarios in order to design automation systems with higher rates of adoption.",,
Enforcing trust in control automotive platforms,Trusted computing is a main concern for ensuring that the platform is secure and dependable.,Search,2010,2,"Christophe  Jouvray, Grégoire  Chartier, Nicolas  François, Ismael  Ripoll, Miguel  Masmano, Alfons  Crespo",EDCC-CARS,,10.1145/1772643.1772656,https://doi.org/10.1145/1772643.1772656,https://semanticscholar.org/paper/08ec5f3f032035b0ae56da2aaea0b4a6f9ba8381,,Trusted computing is a main concern for ensuring that the platform is secure and dependable. This paper will study AUTOSAR regarding to this area and will propose some suggestions to enforce trust in control automotive platforms.,,
Progressive automation to gain appropriate trust in management automation systems,Progressive automation can gain appropriate trust in management automation systems.,Search,2009,2,"Ralf  König, Elaine  Wong, Grégoire  Danoy",Self-Healing and Self-Adaptive Systems,,,,https://semanticscholar.org/paper/8e6181653611927ca1d0bc11f91b19015d409d11,,"This document summarizes the results of the Working Group 5

- ``Progressive Automation / Trust'' - at the Dagstuhl Seminar 09201

``Self-Healing and Self-Adaptive Systems'' (organized by A. Andrzejak, K. Geihs, O. Shehory and J. Wilkes). The seminar was held from May 10th 2009 to May 15th 2009 in Schloss Dagstuhl~--~Leibniz Center for Informatics.",,
Trustworthy AI,The pursuit of responsible AI raises the ante on both the trustworthy computing and formal methods communities.,Search,2021,7,Jeannette M. Wing,Commun. ACM,,10.1145/3448248,https://doi.org/10.1145/3448248,https://semanticscholar.org/paper/33cf9b4d6c76f988380b1adff2c06c30010f93d3,https://dl.acm.org/doi/pdf/10.1145/3448248,The pursuit of responsible AI raises the ante on both the trustworthy computing and formal methods communities.,,
Creating Trustworthy Robots: Lessons and Inspirations from Automated Systems,One of the most significant challenges of human-robot interaction research is designing systems which foster an appropriate level of trust in their users.,Search,2009,70,"Munjal  Desai, Kristen  Stubbs, Aaron  Steinfeld, Holly A. Yanco",,,10.1184/R1/6552464.V1,https://doi.org/10.1184/R1/6552464.V1,https://semanticscholar.org/paper/78250cdfdde497999e40a1e2c1bb51738b7b06b1,,"One of the most significant challenges of human-robot interaction research is designing systems which foster an appropriate level of trust in their users: in order to use a robot effectively and safely, a user must place neither too little nor too much trust in the system. In order to better understand the factors which influence trust in a robot, we present a survey of prior work on trust in automated systems. We also discuss issues specific to robotics which pose challenges not addressed in the automation literature, particularly related to reliability, capability, and adjustable autonomy. We conclude with the results of a preliminary web-based questionnaire which illustrate some of the biases which autonomous robots may need to overcome in order to promote trust in users.",,
Enforcing trust in home automation platforms,Industrialised societies are increasingly dependent on embedded systems that are getting more and more complex.,Search,2010,2,"Sarah  Majoul, Marc  Richard-Foy, Aitor  Agirre, Antonio  Perez, Antonio  Kung",2010 IEEE 15th Conference on Emerging Technologies & Factory Automation (ETFA 2010),,10.1109/ETFA.2010.5641362,https://doi.org/10.1109/ETFA.2010.5641362,https://semanticscholar.org/paper/9d7c1e0cc02d586a05020d40adbf0486a8fc9b37,,"Industrialised societies are increasingly dependent on embedded systems that are getting more and more complex, dynamic, open, while interacting with a progressively more demanding and heterogeneous environment.",,
Business process automation with representing and reasoning on trust,RAP/AOR modeling supports the natural representation of business rules and processes.,Search,2005,2,K.  Taveter,"INDIN '05. 2005 3rd IEEE International Conference on Industrial Informatics, 2005.",,10.1109/INDIN.2005.1560395,https://doi.org/10.1109/INDIN.2005.1560395,https://semanticscholar.org/paper/2b89d8ed56e973fd2e5aa6604c035dbfacb5eb66,,"This article proposes a way how one could design and create systems to support decision-making within business processes by software agents. It argues that the RAP/AOR modelling framework lends itself to the natural representation of both business rules and processes, as well as of an agent's trust in another agent and its evolution. The framework also includes an efficient way of implementing business process automation systems based on XML and software agents. The author hopes this work to be a step towards effective creation of business process automation systems.",,