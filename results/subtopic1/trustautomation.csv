Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Trust in Automation A Literature Review,Increasingly complex autonomous systems require the human operator to appropriately calibrate their trust in the automation.,Search,2018,7,"Bronwyn  French, Andreas  Duenser, Andrew  Heathcote",,,,,https://semanticscholar.org/paper/92f07d3d1356307decb6e97382ad884d0f62668d,,"Increasingly complex autonomous systems require the human operator to appropriately calibrate their trust in the automation in order to achieve performance and safety goals. Although humanautomation trust has been shown to be similar in many respects to human-human trust, precisely defining and measuring trust in automation has proved to be one of the greatest challenges facing research in this area. This report reviews literature pertaining to trust in automated systems to provide an integrated summary of the major theoretical and empirical work in the field to date.",,Review
Towards the Development of an Inter-cultural Scale to Measure Trust in Automation,Trust is conceived as an attitude leading to intentions resulting in user actions involving automation.,Search,2014,29,"Shih Yi Chien, Zhaleh  Semnani-Azad, Michael  Lewis, Katia P. Sycara",HCI,,10.1007/978-3-319-07308-8_4,https://doi.org/10.1007/978-3-319-07308-8_4,https://semanticscholar.org/paper/da0bc0aa2ddf0fed58f1fcbc42a2a7ca16b29d13,https://link.springer.com/content/pdf/10.1007%2F978-3-319-07308-8_4.pdf,"Trust is conceived as an attitude leading to intentions resulting in user actions involving automation. It is generally believed that trust is dynamic and that a user’s prior experience with automation affects future behavior indirectly through causing changes in trust. Additionally, individual differences and cultural factors have been frequently cited as the contributors to influencing trust beliefs about using and monitoring automation. The presented research focuses on modeling human’s trust when interacting with automated systems across cultures. The initial trust assessment instrument, comprising 110 items along with 2 perceptions (general vs. specific use of automation), has been empirically validated. Detailed results comparing items and dimensionality with our new pooled measure will be presented.",,
Trust in Automation,Trust dynamics play a role in the various meanings of trust and the various modes of operation in which trust dynamics play a role.,Search,2013,114,"Robert R. Hoffman, Matt  Johnson, Jeffrey M. Bradshaw, Al  Underbrink",IEEE Intelligent Systems,,10.1109/MIS.2013.24,https://doi.org/10.1109/MIS.2013.24,https://semanticscholar.org/paper/8f95451a2b3f844fe244f73c03fbbc1f395a252e,,"This essay focuses on trust in the automation within macrocognitive work systems. The authors emphasize the dynamics of trust. They consider numerous different meanings or kinds of trust, and different modes of operation in which trust dynamics play a role. Their goal is to contribute to the development of a methodology for designing and analyzing collaborative human-centered work systems, a methodology that might promote both trust ""calibration"" and appropriate reliance. The analysis suggests an ontology for what the authors call ""active exploration for trusting"" (AET).",,
Trust in Automation: Designing for Appropriate Reliance,Trust influences reliance on automation because people respond to technology socially.,Search,2004,1655,"John D. Lee, Katrina A. See",Hum. Factors,,10.1518/hfes.46.1.50.30392,https://doi.org/10.1518/hfes.46.1.50.30392,https://semanticscholar.org/paper/7dd86508438657ac7a704a5d952a2a4422808975,http://www.engineering.uiowa.edu/~csl/publications/pdf/leesee04.pdf,"Automation is often problematic because people fail to rely upon it appropriately. Because people respond to technology socially, trust influences reliance on automation. In particular, trust guides reliance when complexity and unanticipated situations make a complete understanding of the automation impractical. This review considers trust from the organizational, sociological, interpersonal, psychological, and neurological perspectives. It considers how the context, automation characteristics, and cognitive processes affect the appropriateness of trust. The context in which the automation is used influences automation performance and provides a goal-oriented perspective to assess automation characteristics along a dimension of attributional abstraction. These characteristics can influence trust through analytic, analogical, and affective processes. The challenges of extrapolating the concept of trust in people to trust in automation are discussed. A conceptual model integrates research regarding trust in automation and describes the dynamics of trust, the role of context, and the influence of display characteristics. Actual or potential applications of this research include improved designs of systems that require people to manage imperfect automation.",,Review
Trust in Automation: Designing for Appropriate Reliance,Trust influences reliance on automation because people respond to technology socially.,Search,2004,1361,"John D. Lee, Katrina A. See",,,10.1518/hfes.46.1.50_30392,https://doi.org/10.1518/hfes.46.1.50_30392,https://semanticscholar.org/paper/da7118242082ff14ee89cac4df938bc470c3bfd9,https://journals.sagepub.com/doi/pdf/10.1518/hfes.46.1.50_30392,"Automation is often problematic because people fail to rely upon it appropriately. Because people respond to technology socially, trust influences reliance on automation. In particular, trust guides reliance when complexity and unanticipated situations make a complete understanding of the automation impractical. This review considers trust from the organizational, sociological, interpersonal, psychological, and neurological perspectives. It considers how the context, automation characteristics, and cognitive processes affect the appropriateness of trust. The context in which the automation is used influences automation performance and provides a goal-oriented perspective to assess automation characteristics along a dimension of attributional abstraction. These characteristics can influence trust through analytic, analogical, and affective processes. The challenges of extrapolating the concept of trust in people to trust in automation are discussed. A conceptual model integrates research regarding trust in automation and describes the dynamics of trust, the role of context, and the influence of display characteristics. Actual or potential applications of this research include improved designs of systems that require people to manage imperfect automation.",,Review
Trust in automation. Part II. Experimental studies of trust and human intervention in a process control simulation.,Trust in an automation is based mainly upon the automation's competence.,Search,1996,780,"B M Muir, N  Moray",Ergonomics,,10.1080/00140139608964474,https://doi.org/10.1080/00140139608964474,https://semanticscholar.org/paper/0fea4371d73960c8fcb17e6d94123aba9905a7ab,,"Two experiments are reported which examined operators' trust in and use of the automation in a simulated supervisory process control task. Tests of the integrated model of human trust in machines proposed by Muir (1994) showed that models of interpersonal trust capture some important aspects of the nature and dynamics of human-machine trust. Results showed that operators' subjective ratings of trust in the automation were based mainly upon their perception of its competence. Trust was significantly reduced by any sign of incompetence in the automation, even one which had no effect on overall system performance. Operators' trust changed very little with experience, with a few notable exceptions. Distrust in one function of an automatic component spread to reduce trust in another function of the same component, but did not generalize to another independent automatic component in the same system, or to other systems. There was high positive correlation between operators' trust in and use of the automation; operators used automation they trusted and rejected automation they distrusted, preferring to do the control task manually. There was an inverse relationship between trust and monitoring of the automation. These results suggest that operators' subjective ratings of trust and the properties of the automation which determine their trust, can be used to predict and optimize the dynamic allocation of functions in automated systems.",,
Trust in Automation,"dispositional trust, situational trust, and learned trust.",Search,2015,751,"Kevin  Hoff, Masooda  Bashir",Human factors,,10.1177/0018720814547570,https://doi.org/10.1177/0018720814547570,https://semanticscholar.org/paper/95d66e12bb9116c3e98c7d422decd6b5c74769dc,,"Objective: We systematically review recent empirical research on factors that influence trust in automation to present a three-layered trust model that synthesizes existing knowledge. Background: Much of the existing research on factors that guide human-automation interaction is centered around trust, a variable that often determines the willingness of human operators to rely on automation. Studies have utilized a variety of different automated systems in diverse experimental paradigms to identify factors that impact operators’ trust. Method: We performed a systematic review of empirical research on trust in automation from January 2002 to June 2013. Papers were deemed eligible only if they reported the results of a human-subjects experiment in which humans interacted with an automated system in order to achieve a goal. Additionally, a relationship between trust (or a trust-related behavior) and another variable had to be measured. All together, 101 total papers, containing 127 eligible studies, were included in the review. Results: Our analysis revealed three layers of variability in human–automation trust (dispositional trust, situational trust, and learned trust), which we organize into a model. We propose design recommendations for creating trustworthy automation and identify environmental conditions that can affect the strength of the relationship between trust and reliance. Future research directions are also discussed for each layer of trust. Conclusion: Our three-layered trust model provides a new lens for conceptualizing the variability of trust in automation. Its structure can be applied to help guide future research and develop training interventions and design procedures that encourage appropriate trust.",,Systematic Review
Individual Differences in the Calibration of Trust in Automation,People with high expectancy that automation is trustworthy are more sensitive to changes in automation reliability.,Search,2015,46,"Vlad L. Pop, Alex  Shrewsbury, Francis T. Durso",Hum. Factors,,10.1177/0018720814564422,https://doi.org/10.1177/0018720814564422,https://semanticscholar.org/paper/79a562551a586957bc9045a45dc1ea545a69d9ce,,"Objective: The objective was to determine whether operators with an expectancy that automation is trustworthy are better at calibrating their trust to changes in the capabilities of automation, and if so, why. Background: Studies suggest that individual differences in automation expectancy may be able to account for why changes in the capabilities of automation lead to a substantial change in trust for some, yet only a small change for others. Method: In a baggage screening task, 225 participants searched for weapons in 200 X-ray images of luggage. Participants were assisted by an automated decision aid exhibiting different levels of reliability. Measures of expectancy that automation is trustworthy were used in conjunction with subjective measures of trust and perceived reliability to identify individual differences in trust calibration. Results: Operators with high expectancy that automation is trustworthy were more sensitive to changes (both increases and decreases) in automation reliability. This difference was eliminated by manipulating the causal attribution of automation errors. Conclusion: Attributing the cause of automation errors to factors external to the automation fosters an understanding of tasks and situations in which automation differs in reliability and may lead to more appropriate trust. Application: The development of interventions can lead to calibrated trust in automation.",,
"Trust in Automation: Comparison of Automobile, Robot, Medical, and Cyber Aid Technologies","The largest contributors to trust index included familiarity with the technology, perceived importance and usefulness of the technology, and propensity to trust automation.",Search,2021,1,"Sarah K. Hopko, Ranjana K. Mehta, Anthony D. McDonald",Proceedings of the Human Factors and Ergonomics Society Annual Meeting,,10.1177/1071181321651179,https://doi.org/10.1177/1071181321651179,https://semanticscholar.org/paper/2f0959d089df88f90697f3aed322504d97c8b5c9,https://journals.sagepub.com/doi/pdf/10.1177/1071181321651179,"The adoption and appropriate utilization of automated subsystems is dependent on the acceptance, trust, and reliance in the automated subsystem and the systems as a whole. The differences in trust attitudes between vehicle, robot, medical devices, and cyber aids, as affected by dispositional and learned factors has not been studied. As such this paper employs an anonymous online survey to evaluate the contribution of these factors to trust by technology. The results indicate automation in medical devices are ranked as the most trusted, and the automation trust index is highest for automation in cyber aids, followed by medical devices. Vehicle automation and robot automation are the least trusted technologies by both measures. Furthermore, the largest contributors to trust index included familiarity with the technology, perceived importance and usefulness of the technology, and propensity to trust automation. This study illustrates the importance of considering demographics, attitudes, and experience in trust studies.",,
Theoretical Considerations and Development of a Questionnaire to Measure Trust in Automation,The increasing number of interactions with automated systems has sparked the interest of researchers in trust in automation.,Search,2018,63,Moritz  Körber,Advances in Intelligent Systems and Computing,,10.1007/978-3-319-96074-6_2,https://doi.org/10.1007/978-3-319-96074-6_2,https://semanticscholar.org/paper/3b30a9296e687a5dc63b6281d54830909351de02,https://psyarxiv.com/nfc45/download,"The increasing number of interactions with automated systems has sparked the interest of researchers in trust in automation because it predicts not only whether but also how an operator interacts with an automation. In this work, a theoretical model of trust in automation is established and the development and evaluation of a corresponding questionnaire (Trust in Automation, TiA) are described.",,
Behavioral Measurement of Trust in Automation,The trust fall experiment measures behaviors that demonstrate trust compared to self-reports of trust.,Search,2016,33,"David  Miller, Mishel  Johns, Brian  Mok, Nikhil  Gowda, David  Sirkin, Key Jung Lee, Wendy  Ju",,,10.1177/1541931213601422,https://doi.org/10.1177/1541931213601422,https://semanticscholar.org/paper/a786a3af40e1fe813b6650217fc4ad7b15677151,,"Stating that one trusts a system is markedly different from demonstrating that trust. To investigate trust in automation, we introduce the trust fall: a two-stage behavioral test of trust. In the trust fall paradigm, first the one learns the capabilities of the system, and in the second phase, the ‘fall,’ one’s choices demonstrate trust or distrust. Our first studies using this method suggest the value of measuring behaviors that demonstrate trust, compared with self-reports of one’s trust. Designing interfaces that encourage appropriate trust in automation will be critical for the safe and successful deployment of partially automated vehicles, and this will rely on a solid understanding of whether these interfaces actually inspire trust and encourage supervision.",,
Trusting Automation: Designing for Responsivity and Resilience.,New technological and organizational contexts that shift human operators to co-operators of automation may require different measures of automation trust.,Search,2021,8,"Erin K Chiou, John D Lee",Human factors,,10.1177/00187208211009995,https://doi.org/10.1177/00187208211009995,https://semanticscholar.org/paper/9f7511fb851d5cdafbfdf61de06acb09e3a4b2c7,,"OBJECTIVE

This paper reviews recent articles related to human trust in automation to guide research and design for increasingly capable automation in complex work environments.

BACKGROUND

Two recent trends-the development of increasingly capable automation and the flattening of organizational hierarchies-suggest a reframing of trust in automation is needed.

METHOD

Many publications related to human trust and human-automation interaction were integrated in this narrative literature review.

RESULTS

Much research has focused on calibrating human trust to promote appropriate reliance on automation. This approach neglects relational aspects of increasingly capable automation and system-level outcomes, such as cooperation and resilience. To address these limitations, we adopt a relational framing of trust based on the decision situation, semiotics, interaction sequence, and strategy. This relational framework stresses that the goal is not to maximize trust, or to even calibrate trust, but to support a process of trusting through automation responsivity.

CONCLUSION

This framing clarifies why future work on trust in automation should consider not just individual characteristics and how automation influences people, but also how people can influence automation and how interdependent interactions affect trusting automation. In these new technological and organizational contexts that shift human operators to co-operators of automation, automation responsivity and the ability to resolve conflicting goals may be more relevant than reliability and reliance for advancing system design.

APPLICATION

A conceptual model comprising four concepts-situation, semiotics, strategy, and sequence-can guide future trust research and design for automation responsivity and more resilient human-automation systems.",,Review
The role of trust in automation reliance,Trust is an important factor in understanding automation reliance decisions.,Search,2003,783,"Mary T. Dzindolet, Scott A. Peterson, Regina A. Pomranky, Linda G. Pierce, Hall P. Beck",Int. J. Hum. Comput. Stud.,,10.1016/S1071-5819(03)00038-7,https://doi.org/10.1016/S1071-5819(03)00038-7,https://semanticscholar.org/paper/b0e89da3eb67767b63ca5c23c233238f70263ae7,,"A recent and dramatic increase in the use of automation has not yielded comparable improvements in performance. Researchers have found human operators often underutilize (disuse) and overly rely on (misuse) automated aids (Parasuraman and Riley, 1997). Three studies were performed with Cameron University students to explore the relationship among automation reliability, trust, and reliance. With the assistance of an automated decision aid, participants viewed slides of Fort Sill terrain and indicated the presence or absence of a camouflaged soldier. Results from the three studies indicate that trust is an important factor in understanding automation reliance decisions. Participants initially considered the automated decision aid trustworthy and reliable. After observing the automated aid make errors, participants distrusted even reliable aids, unless an explanation was provided regarding why the aid might err. Knowing why the aid might err increased trust in the decision aid and increased automation reliance, even when the trust was unwarranted. Our studies suggest a need for future research focused on understanding automation use, examining individual differences in automation reliance, and developing valid and reliable self-report measures of trust in automation.",,
Examining Single- and Multiple-Process Theories of Trust in Automation,Automation errors have different effects on human dependence behaviors.,Search,2009,100,Stephen  Rice,The Journal of general psychology,,10.3200/GENP.136.3.303-322,https://doi.org/10.3200/GENP.136.3.303-322,https://semanticscholar.org/paper/2e7ecfd2f3f79b4d2ec5d27c26c809f09f58e3b0,,"The author examined the effects of human responses to automation alerts and nonalerts. Previous research has shown that automation false alarms and misses have differential effects on human trust (i.e., automation false alarms tend to affect operator compliance, whereas automation misses tend to affect operator reliance). Participants performed a simulated combat task, whereby they examined aerial photographs for the presence of enemy targets. A diagnostic aid provided a recommendation during each trial. The author manipulated the reliability and response bias of the aid to provide appropriate data for state-trace analyses. The analyses provided strong evidence that only a multiple-process theory of operator trust can explain the effects of automation errors on human dependence behaviors. The author discusses the theoretical and practical implications of this finding.",,
Influence of cultural factors in dynamic trust in automation,There are significant cultural differences in human trust attitude in automation.,Search,2016,12,"Shih Yi Chien, Michael  Lewis, Katia P. Sycara, Jyi-Shane  Liu, Asiye  Kumru","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",,10.1109/SMC.2016.7844677,https://doi.org/10.1109/SMC.2016.7844677,https://semanticscholar.org/paper/52900704d030dbd79cb3a1e78bcbd2db90ece8dc,http://d-scholarship.pitt.edu/31539/1/smc-trust.pdf,"The use of autonomous systems has been rapidly increasing in recent decades. To improve human-automation interaction, trust has been closely studied. Research shows trust is critical in the development of appropriate reliance on automation. To examine how trust mediates the human-automation relationships across cultures, the present study investigated the influences of cultural factors on trust in automation. Theoretically guided empirical studies were conducted in the U.S., Taiwan and Turkey to examine how cultural dynamics affect various aspects of trust in automation. The results found significant cultural differences in human trust attitude in automation.",,
Building a framework to manage trust in automation,A behavior and physiology-based predictor can anticipate human interaction behaviors and influence them towards more optimal choices about automation use.,Search,2017,24,"J. S. Metcalfe, A. R. Marathe, B.  Haynes, V. J. Paul, G. M. Gremillion, K.  Drnec, C.  Atwater, J. R. Estepp, J. R. Lukos, E. C. Carter, W. D. Nothwang",Defense + Security,,10.1117/12.2264245,https://doi.org/10.1117/12.2264245,https://semanticscholar.org/paper/56769ed6402988792fae612fcf869b5530fdb4c1,,"All automations must, at some point in their lifecycle, interface with one or more humans. Whether operators, end-users, or bystanders, human responses can determine the perceived utility and acceptance of an automation. It has been long believed that human trust is a primary determinant of human-automation interactions and further presumed that calibrating trust can lead to appropriate choices regarding automation use. However, attempts to improve joint system performance by calibrating trust have not yet provided a generalizable solution. To address this, we identified several factors limiting the direct integration of trust, or metrics thereof, into an active mitigation strategy. The present paper outlines our approach to addressing this important issue, its conceptual underpinnings, and practical challenges encountered in execution. Among the most critical outcomes has been a shift in focus from trust to basic interaction behaviors and their antecedent decisions. This change in focus inspired the development of a testbed and paradigm that was deployed in two experiments of human interactions with driving automation that were executed in an immersive, full-motion simulation environment. Moreover, by integrating a behavior and physiology-based predictor within a novel consequence-based control system, we demonstrated that it is possible to anticipate particular interaction behaviors and influence humans towards more optimal choices about automation use in real time. Importantly, this research provides a fertile foundation for the development and integration of advanced, wearable technologies for sensing and inferring critical state variables for better integration of human elements into otherwise fully autonomous systems.",,