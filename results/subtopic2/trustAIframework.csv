Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
An agile framework for trustworthy AI,The AI High Level Expert Group (AI-HLEG) put forward ethics guidelines for trustworthy AI systems.,Search,2020,2,"Stefan  Leijnen, Huib  Aldewereld, Rudy van Belkom, Roland  Bijvank, Roelant  Ossewaarde",NeHuAI@ECAI,,,,https://semanticscholar.org/paper/880049a16c8fea47dcfe07450668f5507db5e96d,,"From the article: The ethics guidelines put forward by the AI High Level Expert Group (AI-HLEG) present a list of seven key requirements that Human-centered, trustworthy AI systems should meet. These guidelines are useful for the evaluation of AI systems, but can be complemented by applied methods and tools for the development of trustworthy AI systems in practice. In this position paper we propose a framework for translating the AI-HLEG ethics guidelines into the specific context within which an AI system operates. This approach aligns well with a set of Agile principles commonly employed in software engineering. http://ceur-ws.org/Vol-2659/",,
TAII Framework for Trustworthy AI Systems,The Trustworthy Artificial Intelligence Implementation (TAII) Framework identifies ethical dependencies within companies.,Search,2021,,Josef  Baker-Brunnbauer,,,,,https://semanticscholar.org/paper/c0b7aba59db8da49ca2109b4e90244a7fbb2153c,,"Organizations and companies need practical tools and guidelines to kick-off the implementation of Trustworthy Artificial Intelligence (TAI) systems. AI development companies are still in the beginning of this process or have not even started yet. The findings of this article address to decrease the entry level barrier for AI ethics implementation by introducing the Trustworthy Artificial Intelligence Implementation (TAII) Framework. The outcome is comparatively unique given that it considers a meta perspective of implementing TAI within organizations. As such, this research aims to fill a literature gap for management guidance to tackle trustworthy AI implementation while considering ethical dependencies within the company. The TAII Framework takes a holistic approach to identify the systemic relationships of ethics for the company ecosystem and considers corporate values, business models, and common good aspects like the Sustainable Development Goals and the Universal Declaration of Human Rights. The TAII Framework creates guidance to initiate the implementation of AI ethics in organizations without requiring a deep background in philosophy and considers the social impacts outside of a software and data engineering setting. Depending on the legal regulation or area of application, the TAII Framework can be adapted and used with different regulations and ethical principles.",,
Trustworthy AI Implementation (TAII) Framework for AI Systems,Companies and their stakeholder need practical tools and implementation guidelines besides abstract frameworks to kick off the realization of Artificial Intelligence (AI) ethics.,Search,2021,1,Josef  Baker-Brunnbauer,,,10.2139/SSRN.3796799,https://doi.org/10.2139/SSRN.3796799,https://semanticscholar.org/paper/3337ff3dd257ae6cb5d276e34aa06f190ab06265,,"Companies and their stakeholder need practical tools and implementation guidelines besides abstract frameworks to kick off the realization of Artificial Intelligence (AI) ethics. Based on my previous research outcome AI development companies are still in the beginning of this process or have not even started yet. How is it possible to decrease the entry level barrier to kickoff AI ethics implementation? I tackle this question by combining AI ethics research with previous research findings to create the Trustworthy AI Implementation (TAII) framework. A literature review was conducted and that specifies the research and implementation status for each process step. The aim is to kickoff AI ethics and to transfer research and abstract guidelines from academia to business. The TAII process generates a meta perspective on the systemic dependencies of ethics for the company ecosystem. It generates orienteering for the AI ethics kickoff without requiring a deep background in philosophy and considers perspectives of social impact outside the software and data engineering setting. Depending on the legal regulation or area of application, the TAII process can be adapted and used with different regulations and ethical principles.",,Review
"Information Theoretic Evaluation of Privacy-Leakage, Interpretability, and Transferability for a Novel Trustworthy AI Framework",A novel trustworthy AI framework takes into account the ethical AI principles during the development of machine learning and deep learning models.,Search,2021,,"Mohit  Kumar, Bernhard A. Moser, Lukas  Fischer, Bernhard  Freudenthaler",ArXiv,,,,https://semanticscholar.org/paper/4deb5a713cba1b041092bcb923719b22bf8fcc56,,"Guidelines and principles of trustworthy AI should be adhered to in practice during the development of AI systems. This work suggests a novel information theoretic trustworthy AI framework based on the hypothesis that information theory enables taking into account the ethical AI principles during the development of machine learning and deep learning models via providing a way to study and optimize the inherent tradeoffs between trustworthy AI principles. Under the proposed framework, a unified approach to “privacy-preserving interpretable and transferable learning” is considered to introduce the information theoretic measures for privacy-leakage, interpretability, and transferability. A technique based on variational optimization, employing conditionally deep autoencoders, is developed for practically calculating the defined information theoretic measures for privacy-leakage, interpretability, and transferability.",,
Designing Trustworthy AI: A Human-Machine Teaming Framework to Guide Development,"A Human-Machine Teaming Framework for Designing Ethical AI Experiences will guide AI development teams to create AI systems that are accountable, de-risked, respectful, secure, honest, and usable.",Search,2019,4,Carol J. Smith,ArXiv,,10.1184/R1/12119847.V1,https://doi.org/10.1184/R1/12119847.V1,https://semanticscholar.org/paper/e4213c10a8894e9a767633ed12a605fd65beb95b,,"Artificial intelligence (AI) holds great promise to empower us with knowledge and augment our effectiveness. We can -- and must -- ensure that we keep humans safe and in control, particularly with regard to government and public sector applications that affect broad populations. How can AI development teams harness the power of AI systems and design them to be valuable to humans? Diverse teams are needed to build trustworthy artificial intelligent systems, and those teams need to coalesce around a shared set of ethics. There are many discussions in the AI field about ethics and trust, but there are few frameworks available for people to use as guidance when creating these systems. The Human-Machine Teaming (HMT) Framework for Designing Ethical AI Experiences described in this paper, when used with a set of technical ethics, will guide AI development teams to create AI systems that are accountable, de-risked, respectful, secure, honest, and usable. To support the team's efforts, activities to understand people's needs and concerns will be introduced along with the themes to support the team's efforts. For example, usability testing can help determine if the audience understands how the AI system works and complies with the HMT Framework. The HMT Framework is based on reviews of existing ethical codes and best practices in human-computer interaction and software development. Human-machine teams are strongest when human users can trust AI systems to behave as expected, safely, securely, and understandably. Using the HMT Framework to design trustworthy AI systems will provide support to teams in identifying potential issues ahead of time and making great experiences for humans.",,Review
An Explainable Artificial Intelligence (xAI) Framework for Improving Trust in Automated ATM Tools,A novel framework to enable trust on AI-based automated solutions is presented based on current guidelines and end user feedback.,Search,2021,,"Carolina Sanchez Hernandez, Samuel  Ayo, Dimitrios  Panagiotakopoulos",2021 IEEE/AIAA 40th Digital Avionics Systems Conference (DASC),,10.1109/dasc52595.2021.9594341,https://doi.org/10.1109/dasc52595.2021.9594341,https://semanticscholar.org/paper/bf3f1666259b71110cdd5540cac9f0058482272e,http://dspace.lib.cranfield.ac.uk/bitstream/1826/17344/1/improving_trust_in_automated_ATM_tools-2021.pdf,"With the increased use of intelligent Decision Support Tools in Air Traffic Management (ATM) and inclusion of non-traditional entities, regulators and end users need assurance that new technologies such as Artificial Intelligence (AI) and Machine Learning (ML) are trustworthy and safe. Although there is a wide amount of research on the technologies themselves, there seem to be a gap between research projects and practical implementation due to different regulatory and practical challenges including the need for transparency and explainability of solutions. In order to help address these challenges, a novel framework to enable trust on AI-based automated solutions is presented based on current guidelines and end user feedback. Finally, recommendations are provided to bridge the gap between research and implementation of AI and ML-based solutions using our framework as a mechanism to aid advances of AI technology within ATM.",,
The relationship between trust in AI and trustworthy machine learning technologies,"Trustworthiness technologies can support the required qualities of fair, explainable, auditable, and safe machine learning.",Search,2020,50,"Ehsan  Toreini, Mhairi  Aitken, Kovila  Coopamootoo, Karen  Elliott, Carlos Gonzalez Zelaya, Aad van Moorsel",FAT*,,10.1145/3351095.3372834,https://doi.org/10.1145/3351095.3372834,https://semanticscholar.org/paper/bd4cf5a6987ef0f39b7e4e88d59b3a3365abcc70,http://arxiv.org/pdf/1912.00782,"To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.",,
What we talk about when we talk about trust: Theory of trust for AI in healthcare,A sound empirical research and conceptual rigor is required for a comprehensive understanding of the trust construct.,Search,2020,10,"Felix  Gille, Anna  Jobin, Marcello  Ienca",,,10.1016/j.ibmed.2020.100001,https://doi.org/10.1016/j.ibmed.2020.100001,https://semanticscholar.org/paper/7f1fd5e7cc85cbe979f7df0acf8c468a01116496,https://www.research-collection.ethz.ch/bitstream/20.500.11850/430039/7/1-s2.0-S2666521220300016-main.pdf,"Abstract Artificial intelligence (AI) is at the forefront of innovation in medicine. Researchers and AI developers have often claimed that ""trust"" is a critical determinant of the successful adoption of AI in medicine. Despite the pivotal role of trust and the emergence of an array of expert-informed guidelines on how to design and implement ""trustworthy AI"" in medicine, we found little common understanding across these guidelines on what constitutes user trust in AI and what the requirements are for its realization. In this article, we call for a conceptual framework of trust in health-related AI which is based not just on expert opinion, but first and foremost on sound empirical research and conceptual rigor. Only with a well-grounded and comprehensive understanding of the trust construct, we will be able to inform AI design and acceptance in medicine in a meaningful way.",,
Designing Trustworthy AI: A User Experience (UX) Framework at RSA Conference 2020,"A new user experience (UX) framework can guide the creation of AI systems that are accountable, de-risked, respectful, secure, honest and usable.",Search,2020,,Carol  Smith,,,10.1184/R1/12198321.V1,https://doi.org/10.1184/R1/12198321.V1,https://semanticscholar.org/paper/a2a2e1d11f3738113b182d5d4e6366a033d96fab,,"Artificial intelligence (AI) holds great promise to empower us with knowledge and scaled effectiveness. To harness the power of AI systems, we can—and must—ensure that we keep humans safe and in control. This session will introduce a new user experience (UX) framework to guide the creation of AI systems that are accountable, de-risked, respectful, secure, honest and usable.",,
Trustworthy AI: From Principles to Practices,"A comprehensive trustworthy AI framework involves various aspects including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, alignment with human values, and accountability.",Search,2021,1,"Bo  Li, Peng  Qi, Bo  Liu, Shuai  Di, Jingen  Liu, Jiquan  Pei, Jinfeng  Yi, Bowen  Zhou",ArXiv,,,,https://semanticscholar.org/paper/c3689493757f90267908e776aeada9194fce55c7,,"Fast developing artificial intelligence (AI) technology has enabled various applied systems deployed in the real world, impacting people’s everyday lives. However, many current AI systems were found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection, etc., which not only degrades user experience but erodes the society’s trust in all AI systems. In this review, we strive to provide AI practitioners a comprehensive guide towards building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, alignment with human values, and accountability. We then survey leading approaches in these aspects in the industry. To unify the current fragmented approaches towards trustworthy AI, we propose a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items to practitioners and societal stakeholders (e.g., researchers and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges in the future development of trustworthy AI systems, where we identify the need for paradigm shift towards comprehensive trustworthy AI systems.",,Review
Can we trust AI? An empirical investigation of trust requirements and guide to successful AI adoption,The valence framework needs to be elaborated to become applicable to the AI context,Search,2021,2,"Patrick  Bedué, Albrecht  Fritzsche",,,10.1108/JEIM-06-2020-0233,https://doi.org/10.1108/JEIM-06-2020-0233,https://semanticscholar.org/paper/2590e2a851c6c9fc61029377c1e451afdb408004,,"Purpose Artificial intelligence (AI) fosters economic growth and opens up new directions for innovation. However, the diffusion of AI proceeds very slowly and falls behind, especially in comparison to other technologies. An important path leading to better adoption rates identified is trust-building. Particular requirements for trust and their relevance for AI adoption are currently insufficiently addressed.Design/methodology/approachTo close this gap, the authors follow a qualitative approach, drawing on the extended valence framework by assessing semi-structured interviews with experts from various companies.FindingsThe authors contribute to research by finding several subcategories for the three main trust dimensions ability, integrity and benevolence, thereby revealing fundamental differences for building trust in AI compared to more traditional technologies. In particular, the authors find access to knowledge, transparency, explainability, certification, as well as self-imposed standards and guidelines to be important factors that increase overall trust in AI.Originality/valueThe results show how the valence framework needs to be elaborated to become applicable to the AI context and provide further structural orientation to better understand AI adoption intentions. This may help decision-makers to identify further requirements or strategies to increase overall trust in their AI products, creating competitive and operational advantage.",,
Trustworthy AI Development Guidelines for Human System Interaction,AI development guidelines can improve the trust between AI systems and humans.,Search,2020,3,"Chathurika S. Wickramasinghe, Daniel L. Marino, Javier  Grandio, Milos  Manic",2020 13th International Conference on Human System Interaction (HSI),,10.1109/HSI49210.2020.9142644,https://doi.org/10.1109/HSI49210.2020.9142644,https://semanticscholar.org/paper/238fa66062114f39e404c40d0b1abc03b86e54bd,,"Artificial Intelligence (AI) is influencing almost all areas of human life. Even though these AI-based systems frequently provide state-of-the-art performance, humans still hesitate to develop, deploy, and use AI systems. The main reason for this is the lack of trust in AI systems caused by the deficiency of transparency of existing AI systems. As a solution, “Trustworthy AI” research area merged with the goal of defining guidelines and frameworks for improving user trust in AI systems, allowing humans to use them without fear. While trust in AI is an active area of research, very little work exists where the focus is to build human trust to improve the interactions between human and AI systems. In this paper, we provide a concise survey on concepts of trustworthy AI. Further, we present trustworthy AI development guidelines for improving the user trust to enhance the interactions between AI systems and humans, that happen during the AI system life cycle.",,
Establishing the rules for building trustworthy AI,The European Commission’s report ‘Ethics guidelines for trustworthy AI’ facilitates international support for AI solutions that are good for the environment.,Search,2019,67,Luciano  Floridi,Nature Machine Intelligence,,10.1038/S42256-019-0055-Y,https://doi.org/10.1038/S42256-019-0055-Y,https://semanticscholar.org/paper/dc44e2be0f85b6225f05390c570885337a99ef83,https://philpapers.org/archive/FLOETR.pdf,"The European Commission’s report ‘Ethics guidelines for trustworthy AI’ provides a clear benchmark to evaluate the responsible development of AI systems, and facilitates international support for AI solutions that are good for humanity and the environment, says Luciano Floridi.",,
Trust management framework for social networks,"Inspired by the similarities between human trust and physical measurements, a new system of trust metrics is proposed.",Search,2012,25,"Ping  Zhang, Arjan  Durresi",2012 IEEE International Conference on Communications (ICC),,10.1109/ICC.2012.6364031,https://doi.org/10.1109/ICC.2012.6364031,https://semanticscholar.org/paper/849d7b05678289ae1a54669e18935c6ec9a5e758,,"Inspired by the similarities between human trust and physical measurements, we propose a new system of trust metrics, composed by impression and confidence, which captures both human trust level and its uncertainty, while being intuitive and user friendly. Furthermore, based on measurement error propagation theory, we propose an evaluation framework to compute confidence according to impression transitivity and aggregation operations. Our framework can be used in all applications where human trust is involved, including various types of social networks. Experiments on a real social network validate our framework, as well as the enormous potential of our trust framework in various social network applications. For example, in an experiment on real data we increased 2250 times the “trust view,” while keeping the same level of confidence.",,
Opening the software engineering toolbox for the assessment of trustworthy AI,Software engineering and testing practices can be applied to the assessment of trustworthy AI.,Search,2020,2,"Mohit Kumar Ahuja, Mohamed-Bachir  Belaid, Pierre  Bernab'e, Mathieu  Collet, Arnaud  Gotlieb, Chhagan  Lal, Dusica  Marijan, Sagar  Sen, Aizaz  Sharif, Helge  Spieker",NeHuAI@ECAI,,,,https://semanticscholar.org/paper/3b0ff6bd000e9c615d024614343f2c1cf12bf124,,"Trustworthiness is a central requirement for the acceptance and success of human-centered artificial intelligence (AI). To deem an AI system as trustworthy, it is crucial to assess its behaviour and characteristics against a gold standard of Trustworthy AI, consisting of guidelines, requirements, or only expectations. While AI systems are highly complex, their implementations are still based on software. The software engineering community has a long-established toolbox for the assessment of software systems, especially in the context of software testing. In this paper, we argue for the application of software engineering and testing practices for the assessment of trustworthy AI. We make the connection between the seven key requirements as defined by the European Commission's AI high-level expert group and established procedures from software engineering and raise questions for future work.",,
Trust in Distributed Artificial Intelligence,"A discussion of trust focuses on multi-agent systems, from the point of view of one agent in a system.",Search,1992,90,Stephen  Marsh,MAAMAW,,10.1007/3-540-58266-5_6,https://doi.org/10.1007/3-540-58266-5_6,https://semanticscholar.org/paper/6d296cd0ddaccb6d01aa197c9ba5a04afee2d399,,"A discussion of trust is presented which focuses on multiagent systems, from the point of view of one agent in a system. The roles trust plays in various forms of interaction are considered, with the view that trust allows interactions between agents where there may have been no effective interaction possible before trust. Trust allows parties to acknowledge that, whilst there is a risk in relationships with potentially malevolent agents, some form of interaction may produce benefits, where no interaction at all may not. In addition, accepting the risk allows the trusting agent to prepare itself for possibly irresponsible or untrustworthy behaviour, thus minimizing the potential damage caused. A formalism is introduced to clarify these notions, and to permit computer simulations. An important contribution of this work is that the formalism is not allen-compassing: there are some notions of trust that are excluded. What it describes is a specific view of trust.",,