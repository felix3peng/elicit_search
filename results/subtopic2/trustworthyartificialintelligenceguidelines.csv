Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
An agile framework for trustworthy AI,The AI High Level Expert Group (AI-HLEG) ethics guidelines present a list of requirements that trustworthy AI systems should meet.,Search,2020,2,"Stefan  Leijnen, Huib  Aldewereld, Rudy van Belkom, Roland  Bijvank, Roelant  Ossewaarde",NeHuAI@ECAI,,,,https://semanticscholar.org/paper/880049a16c8fea47dcfe07450668f5507db5e96d,,"From the article: The ethics guidelines put forward by the AI High Level Expert Group (AI-HLEG) present a list of seven key requirements that Human-centered, trustworthy AI systems should meet. These guidelines are useful for the evaluation of AI systems, but can be complemented by applied methods and tools for the development of trustworthy AI systems in practice. In this position paper we propose a framework for translating the AI-HLEG ethics guidelines into the specific context within which an AI system operates. This approach aligns well with a set of Agile principles commonly employed in software engineering. http://ceur-ws.org/Vol-2659/",,
Establishing the rules for building trustworthy AI,The European Commission’s report ‘Ethics guidelines for trustworthy AI’ facilitates international support for AI solutions that are good for humanity and the environment.,Search,2019,67,Luciano  Floridi,Nature Machine Intelligence,,10.1038/S42256-019-0055-Y,https://doi.org/10.1038/S42256-019-0055-Y,https://semanticscholar.org/paper/dc44e2be0f85b6225f05390c570885337a99ef83,https://philpapers.org/archive/FLOETR.pdf,"The European Commission’s report ‘Ethics guidelines for trustworthy AI’ provides a clear benchmark to evaluate the responsible development of AI systems, and facilitates international support for AI solutions that are good for humanity and the environment, says Luciano Floridi.",,
Comments on the “Draft Ethics Guidelines for Trustworthy AI” by the High-LevelExpert Group on Artificial Intelligence.,The European Commission appointed the High-Level Expert Group on Artificial Intelligence.,Search,2019,49,Ninja  Marnau,,,,,https://semanticscholar.org/paper/01040c2897ea82bb801931dbfe8a85a3b64d2e7f,,"The European Commission appointed the High-Level Expert Group on Artificial Intelligence (AI HLEG). The AI HLEG has the objective to support the implementation of the European strategy on Artificial Intelligence. This will include the elaboration of recommendations on future-related policy development and on ethical, legal and societal issues related to AI. In January 2019, the Commission asked stakeholders for comments on the AI HLEG’s “Draft Ethics Guidelines for Trustworthy AI”. CISPA submitted the following comments and remarks in the Stakeholders’ Consultation.",,
GSMA Response to the Draft Ethics Guidelines for Trustworthy AI,The development and deployment of artificial intelligence systems should respect fundamental human rights and applicable regulation.,Search,2019,10,,,,,,https://semanticscholar.org/paper/0a87bcc17b0f385ed52cae4b4bfb235256f88eb3,,"Introduction The GSMA supports the European Commission’s endeavour to maximise the benefits of artificial intelligence (AI) while minimising the risks to individuals and communities, and appreciates the opportunity to comment on the Commission’s new draft ethics guidelines. We support the view that the development and deployment of AI systems should respect fundamental human rights and applicable regulation, as well as principles and values ensuring an ‘ethical purpose’. A growing number of GSMA members have already committed to responsible development of AI technologies.",,
The EU Approach to Ethics Guidelines for Trustworthy Artificial Intelligence,The EU published Ethics Guidelines to address ethical questions raised by Artificial Intelligence.,Search,2019,56,Nathalie A. Smuha,Computer Law Review International,,10.9785/cri-2019-200402,https://doi.org/10.9785/cri-2019-200402,https://semanticscholar.org/paper/a00ee7ae4642b986b622e0f48c845e79707b707b,https://lirias.kuleuven.be/bitstream/123456789/640572/2/EU%20Approach%20for%20Trustworthy%20AI%20-%20a%20continuous%20journey.pdf,"As part of its European strategy for Artificial Intelligence (AI), and as a response to the increasing ethical questions raised by this technology, the European Commission established an independent High-Level Expert Group on Artificial Intelligence (AI HLEG) in June 2018. The group was tasked to draft two deliverables: AI Ethics Guidelines and Policy and Investment Recommendations. Nine months later, its first deliverable was published, putting forward a comprehensive framework to achieve “Trustworthy AI” by offering ethical guidance to AI practitioners. This paper dives into the work carried out by the group, focusing in particular on its AI Ethics Guidelines. First, this paper clarifies the context that led to the creation of the AI HLEG and its mandate (I.). Subsequently, it elaborates on the Guidelines’ aim and purpose (II.), and analyses the Guidelines’ drafting process (III.). Particular focus is given to the questions surrounding the respective role played by ethics and law in the AI governance landscape (IV.), as well as some of the challenges that had to be overcome throughout the process (V.). Finally, this paper places the Guidelines in an international context, and sets out the next steps (VI.) ahead on the journey towards an appropriate governance framework for AI (VII.).",,
The Ethical Guidelines for Trustworthy AI – A Procrastination of Effective Law Enforcement,The Ethical Guidelines for Trustworthy Artificial Intelligence by the EU Commission are ineffective.,Search,2019,2,Ramak Molavi Vasse’i,Computer Law Review International,,10.9785/cri-2019-200502,https://doi.org/10.9785/cri-2019-200502,https://semanticscholar.org/paper/64223e4aa1a20a6406cd036417245385b6b33cc0,,"In the august issue of the CRi, Nathalie Smuha, the coordinator of the work of the High-Level Expert Group on AI, outlined the approach and considerations leading to the “The EU Approach to Ethics Guidelines for Trustworthy Artificial Intelligence”. This paper provides a critical assessment of the Ethical Guidelines of the EU Commission and points out why a law enforcement focused approach must be the essential next step towards a beneficial and humane development of AI. Questioning the diversity of the Commssion’s High Level Expert Group on Artificial Intelligence, the dangers of ethics shopping are exposed as well as the UN Universal Declaration of Human Rights explored as already well established alternative reference framework for AI. Having exposed the need for effective red lines, not only the hidden social and ecological cost are assessed, but also the risk of “buying-out” research and other ethical issues neglected in the Ethics Guidelines for Trustworthy Artificial Intelligence. Finally, three key weaknesses concerning the crucial translation of ethical principles into practice (enforcement) are highlighted.",,
Trustworthy AI Development Guidelines for Human System Interaction,AI development guidelines can improve the user trust in AI systems to enhance human-AI interactions.,Search,2020,3,"Chathurika S. Wickramasinghe, Daniel L. Marino, Javier  Grandio, Milos  Manic",2020 13th International Conference on Human System Interaction (HSI),,10.1109/HSI49210.2020.9142644,https://doi.org/10.1109/HSI49210.2020.9142644,https://semanticscholar.org/paper/238fa66062114f39e404c40d0b1abc03b86e54bd,,"Artificial Intelligence (AI) is influencing almost all areas of human life. Even though these AI-based systems frequently provide state-of-the-art performance, humans still hesitate to develop, deploy, and use AI systems. The main reason for this is the lack of trust in AI systems caused by the deficiency of transparency of existing AI systems. As a solution, “Trustworthy AI” research area merged with the goal of defining guidelines and frameworks for improving user trust in AI systems, allowing humans to use them without fear. While trust in AI is an active area of research, very little work exists where the focus is to build human trust to improve the interactions between human and AI systems. In this paper, we provide a concise survey on concepts of trustworthy AI. Further, we present trustworthy AI development guidelines for improving the user trust to enhance the interactions between AI systems and humans, that happen during the AI system life cycle.",,
How to achieve trustworthy artificial intelligence for health,"The EU's guidelines for artificial intelligence leave room for local, contextualized discretion.",Search,2020,15,"Kristine  Bærøe, Ainar  Miyata-Sturm, Edmund  Henden",Bulletin of the World Health Organization,,10.2471/BLT.19.237289,https://doi.org/10.2471/BLT.19.237289,https://semanticscholar.org/paper/0a3c92d6c4fa4670743fb13758916067e772a143,https://europepmc.org/articles/pmc7133476?pdf=render,"Abstract Artificial intelligence holds great promise in terms of beneficial, accurate and effective preventive and curative interventions. At the same time, there is also awareness of potential risks and harm that may be caused by unregulated developments of artificial intelligence. Guiding principles are being developed around the world to foster trustworthy development and application of artificial intelligence systems. These guidelines can support developers and governing authorities when making decisions about the use of artificial intelligence. The High-Level Expert Group on Artificial Intelligence set up by the European Commission launched the report Ethical guidelines for trustworthy artificial intelligence in2019. The report aims to contribute to reflections and the discussion on the ethics of artificial intelligence technologies also beyond the countries of the European Union (EU). In this paper, we use the global health sector as a case and argue that the EU’s guidance leaves too much room for local, contextualized discretion for it to foster trustworthy artificial intelligence globally. We point to the urgency of shared globalized efforts to safeguard against the potential harms of artificial intelligence technologies in health care.",,
Ethical Guidelines for Trustworthy AI Systems,Ethical guidelines are required to develop trustworthy AI systems.,Search,2020,,"Zahoor ul Islam, Andreas  Theodorou, Juan Carlos Nieves, Virginia  Dignum",,,,,https://semanticscholar.org/paper/6346b968624cca5d4e2f3a53ab53abfd97052ef5,,"In order to engineer human-centric Artificial Intelligence (AI) and autonomous systems, comply with societal norms, ethical guidelines, and established standards, a well-established set of the development life cycle is needed. This development life cycle requires a continuous evaluation process for continuously evolving AI systems [9]. Furthermore, a strategy is required to initiate human responsibility in the development of AI systems from the start of the life cycle, address the gaps that emerge from the increased automation of decision, and provide tools to fill the gaps. [25]. A possible way to develop ethical guidelines for trustworthy AI systems from a development perspective is to look into software engineering (SE) domain. Software Development life cycle (SDLC) is defined by IEEE as “the process by which user needs are translated into a software product. The process involves translating user needs into software requirements, transforming the software requirements into design, implementing the design in code, testing the code, and sometimes, installing and checking out the software for operational use.” [13]. Some of the most commonly used SDLC models is the waterfall, incremental, prototype-driven, evolutionary, spiral and agile software development methods [23]. Most SDLCs processes are available in all methodologies but applied and practised differently based on projects, problems and personal needs. The rapid development in machine learning and neural networks has enabled machines and algorithms to effectively manage tasks such as natural language processing, translations, stock market predictions, and route planning and optimisation. AI systems can learn quickly from patterns and propose decisions and in some instances, autonomously take decisions but without paying in particular attention to the implications of those decisions. Therefore, applying SE methodologies is a fundamental prerequisite for delivering high quality, responsible, transparent, trustworthy, accountable, and robust smart software applications. A well-established set of design methodologies has the potential to address many of the challenges of designing high-performing trustworthy intelligent systems. It can provide an explicit process for values elicitation and stakeholder involvement in the development of AI systems [9]. It can provide support for effective communication, re-usability, process enhancement, and process management. This methodology can help to maintain explicit formal links between values, norms, and systems functionalities that enable adaptation of the system to evolve perception and justification of implementation de-",,
Guidelines for Trustworthy AI application in clinical trials,The European Commission established the High-Level Expert Group on Artificial Intelligence (AI HLEG).,Search,2020,,"N  Leventi, A  Vodenitcharova, K  Popova",,,10.1093/eurpub/ckaa165.806,https://doi.org/10.1093/eurpub/ckaa165.806,https://semanticscholar.org/paper/29898969890b3d4d8a5813a6750d308b3fb7117e,https://academic.oup.com/eurpub/article-pdf/30/Supplement_5/ckaa165.806/33817815/ckaa165.806.pdf,"Innovative information technologies (IIT) like artificial intelligence (AI), big data, etc. promise to support individual patient care, and promote public health. Their use raises ethical, social and legal issues. Here we demonstrate how the guidelines for trustworthy AI, can assist to answer those ethical issues in the case of clinical trials (CT).

In 2018 the European Commission established the High-Level Expert Group on Artificial Intelligence (AI HLEG). The group proposed Guidelines to promote Trustworthy AI, with three components, which should be met throughout the system's entire life cycle, as it should be lawful, ethical and robust.

Trustworthiness is a prerequisite for people and societies to develop, and use AI systems. We used a focus group methodology to explore how the guidelines for trustworthy AI can assist to answer the ethical issues that rise by the application of AI in CTs.

The discussion was directed to the seven requirements for trustworthy AI in CTs, by questions like:

Are they relevant in CTs as a whole? Would they be applicable to the use of IIT as AI in CTs? Are you currently applying part, or all, of the proposed list? In the future, would you attach some, or all, of the proposed list? Is the administrative burden of applying the requirements justified by the effect?

It was recommended that:

the guidelines are relevant in the conduct of the CT; planning and implementation of CTs using IIT, should take them into account; ethical aspects and challenges are of the utmost importance; the proposed list is a very comprehensive framework; particular attention should be paid where more vulnerable groups are affected; the administrative burden is acceptable, as the effect exceeds the resources invested.

IIT are becoming increasingly important in medicine, and requirements for trustworthy IIT, and AI are necessary. Appropriate instrument in the case of the CTs are the provided by AI HLEG guidelines.",,
Walking the tightrope of artificial intelligence guidelines in clinical practice.,The EU guidelines for trustworthy AI recommend that AI approaches should augment the actions of humans through transparent decision pathways.,Search,2019,12,The Lancet Digital Health,The Lancet. Digital health,,10.1016/s2589-7500(19)30063-9,https://doi.org/10.1016/s2589-7500(19)30063-9,https://semanticscholar.org/paper/7743594c60840995ca6b137b192b3c079b0bccf1,http://www.thelancet.com/article/S2589750019300639/pdf,"Over the past few months, there has been a wave of digital health guidelines and whitepapers issued by regulators, institutes, and organisations worldwide. In the field of artificial intelligence (AI), EU guidelines, published in April, promote the development of trustworthy AI across all disciplines, while a US Food and Drug Administration (FDA) whitepaper proposes a regulatory framework for constantly developing software in health care. Guidelines from the National Institution of Health and Care Excellence (NICE) tackle the level of evidence required for a new digital health intervention, and NHSX and Public Health England have both reported their intention to produce their own AI guidelines. AI approaches in medical practice needs to be lawful, ethical, and robust. According to the EU guidelines for trustworthy AI, there are seven key requirements for ethical AI: human agency and oversight; technical robustness and safety; privacy and data governance; transparency; diversity, non-discrimination, and fairness; societal and environmental wellbeing; and accountability. They include tiered, risk-based guidance for tool validation for prevention of harm, recommendations to make the model explainable as well as fair and unbiased, and ensure that human autonomy is maintained. The guidelines highlight that AI approaches should augment the actions of humans through transparent decision pathways rather than black box decision making. Assuming that a model has been designed and created ethically, what is the minimum level of evidence needed to use the AI in the clinic? It will, and should, vary depending on the function for which the software has been designed, which is recognised in NICE’s guidelines for digital health interventions. An AI that recommends a dietary programme in people at risk of developing high blood pressure should require a different level of evidence to a program that recommends treatment options for patients in intensive care. Rather than AI-specific guidelines, NICE’s recommendations are across all digital health interventions; as a result, AI-specific guidelines are not yet detailed enough to define which level of evidence is needed in each classification. In other disciplines, AI algorithms have been tested and validated primarily from the same source dataset. Through a random separation into training and test datasets, and cross validation to improve its reliability, the AI tool could be considered sufficiently reliable to be used in real-world settings with the potential to learn and improve with access to more data over time. However, for tools suggesting treatments and diagnosis algorithms, the models must be more robust to ensure patient safety. AI algorithms need to be trained on an independent and diverse validation dataset to confirm the effectiveness and generalisability of the algorithm. Validation of algorithms hinge on the availability of external data from public datasets. By definition, an AI model is constantly evolving. The final consideration, therefore, is how to regulate these inevitable changes to AI models after approval for use in the clinic has been granted. This is addressed by the FDA whitepaper on modifications to software using machine learning models. Though not yet formal guidelines, the framework that has been issued for discussion is thoughtful and identifies three main areas under which the AI can change: performance, input, and intended use of the software. The last of these changes could be grounds for restarting the approval process, whereas other modifications need only be documented and subject to review periodically. Guidelines securing the minimum level of clinical evidence required for different tiers of AI studies are necessary to eliminate variation in the quality of published studies and in the AI tools themselves. The existing guidelines discussed here and those still in development need to be updated frequently to ensure they remain relevant to advancing technologies. We are awaiting the publication of TRIPOD-ML reporting guidelines, which are being developed specifically for some issues that arise in AI studies published in journals such as ours. We will continue to require independent validation for all AI studies that screen, treat, or diagnose disease. Data should be diverse so as minimise bias and be of high quality to ensure veracity of the findings. We believe that the evidence threshold will evolve as technology advances to strive for increasingly accurate AI models in health care. ■ The Lancet Digital Health",,Review
Trustworthy AI: From Principles to Practices,There is a need for a paradigm shift towards comprehensive trustworthy AI systems.,Search,2021,1,"Bo  Li, Peng  Qi, Bo  Liu, Shuai  Di, Jingen  Liu, Jiquan  Pei, Jinfeng  Yi, Bowen  Zhou",ArXiv,,,,https://semanticscholar.org/paper/c3689493757f90267908e776aeada9194fce55c7,,"Fast developing artificial intelligence (AI) technology has enabled various applied systems deployed in the real world, impacting people’s everyday lives. However, many current AI systems were found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection, etc., which not only degrades user experience but erodes the society’s trust in all AI systems. In this review, we strive to provide AI practitioners a comprehensive guide towards building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, alignment with human values, and accountability. We then survey leading approaches in these aspects in the industry. To unify the current fragmented approaches towards trustworthy AI, we propose a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items to practitioners and societal stakeholders (e.g., researchers and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges in the future development of trustworthy AI systems, where we identify the need for paradigm shift towards comprehensive trustworthy AI systems.",,Review
Trustworthy AI and Corporate Governance – The EU’s Ethics Guidelines For Trustworthy Artificial Intelligence from a Company Law Perspective,The EU Ethics Guidelines for Trustworthy AI require businesses to establish trustworthy AI.,Search,2020,5,"Eleanore  Hickman, Martin  Petrin",,,10.2139/ssrn.3607225,https://doi.org/10.2139/ssrn.3607225,https://semanticscholar.org/paper/72b39cf03173e4ced50e54873dac32258bbbfe16,,"AI will change many aspects of the world we live in, including the way corporations are governed. Many efficiencies and improvements are likely, but there are also potential dangers, including the threat of harmful impacts on third parties, discriminatory practices, data and privacy breaches, fraudulent practices and even ‘rogue AI’. To address these dangers, the EU published its 'Ethics Guidelines for Trustworthy AI’. The Guidelines produce seven principles from its four foundational pillars of respect for human autonomy, prevention of harm, fairness and explicability.

If implemented by business, the impact on corporate governance will be substantial. Fundamental questions at the intersection of ethics and law are considered but, because the Guidelines only address the former without much reference to the latter, their practical application is challenging for business. Further, while they promote many positive corporate governance principles, it is clear that the Guidelines' general nature leaves many questions and concerns unanswered.

In this paper we examine the potential significance and impact of the Guidelines on selected corporate law and governance issues. We conclude that more specificity is needed in relation to how the principles therein will harmonise with company law rules and governance practices. However, despite their imperfections, until harder legislative instruments emerge, the Guidelines provide a useful starting point for directing businesses towards establishing trustworthy AI.",,
Trustworthy AI and Corporate Governance: The EU’s Ethics Guidelines for Trustworthy Artificial Intelligence from a Company Law Perspective,The EU Ethics Guidelines for Trustworthy Artificial Intelligence provide a useful starting point for directing businesses towards establishing trustworthy AI.,Search,2021,1,"Eleanore  Hickman, Martin  Petrin",European Business Organization Law Review,,10.1007/s40804-021-00224-0,https://doi.org/10.1007/s40804-021-00224-0,https://semanticscholar.org/paper/89b6823aea0f7306bd2ee9a4dac7206256ecd5cc,https://link.springer.com/content/pdf/10.1007/s40804-021-00224-0.pdf,"AI will change many aspects of the world we live in, including the way corporations are governed. Many efficiencies and improvements are likely, but there are also potential dangers, including the threat of harmful impacts on third parties, discriminatory practices, data and privacy breaches, fraudulent practices and even ‘rogue AI’. To address these dangers, the EU published ‘The Expert Group’s Policy and Investment Recommendations for Trustworthy AI’ (the Guidelines). The Guidelines produce seven principles from its four foundational pillars of respect for human autonomy, prevention of harm, fairness, and explicability. If implemented by business, the impact on corporate governance will be substantial. Fundamental questions at the intersection of ethics and law are considered, but because the Guidelines only address the former without (much) reference to the latter, their practical application is challenging for business. Further, while they promote many positive corporate governance principles—including a stakeholder-oriented (‘human-centric’) corporate purpose and diversity, non-discrimination, and fairness—it is clear that their general nature leaves many questions and concerns unanswered. In this paper we examine the potential significance and impact of the Guidelines on selected corporate law and governance issues. We conclude that more specificity is needed in relation to how the principles therein will harmonise with company law rules and governance principles. However, despite their imperfections, until harder legislative instruments emerge, the Guidelines provide a useful starting point for directing businesses towards establishing trustworthy AI.",,
A critical perspective on guidelines for responsible and trustworthy artificial intelligence,The ethical issues of AI technology vary from privacy and confidentiality of personal data to the ethical status and value of AI entities.,Search,2020,4,"Banu  Buruk, Perihan Elif Ekmekci, Berna  Arda","Medicine, health care, and philosophy",,10.1007/s11019-020-09948-1,https://doi.org/10.1007/s11019-020-09948-1,https://semanticscholar.org/paper/4990df10e6423ad5d44888697dd8bc523287fd14,,"Artificial intelligence (AI) is among the fastest developing areas of advanced technology in medicine. The most important qualia of AI which makes it different from other advanced technology products is its ability to improve its original program and decision-making algorithms via deep learning abilities. This difference is the reason that AI technology stands out from the ethical issues of other advanced technology artifacts. The ethical issues of AI technology vary from privacy and confidentiality of personal data to ethical status and value of AI entities in a wide spectrum, depending on their capability of deep learning and scope of the domains in which they operate. Developing ethical norms and guidelines for planning, development, production, and usage of AI technology has become an important issue to overcome these problems. In this respect three outstanding documents have been produced: 1. The Montréal Declaration for Responsible Development of Artificial Intelligence 2. Ethics Guidelines for Trustworthy AI 3. Asilomar Artificial Intelligence Principles In this study, these three documents will be analyzed with respect to the ethical principles and values they involve, their perspectives for approaching ethical issues, and their prospects for ethical reasoning when one or more of these values and principles are in conflict. Then, the sufficiency of these guidelines for addressing current or prospective ethical issues emerging from the existence of AI technology in medicine will be evaluated. The discussion will be pursued in terms of the ambiguity of interlocutors and efficiency for working out ethical dilemmas occurring in practical life.",,
Opening the software engineering toolbox for the assessment of trustworthy AI,The European Commission's AI high-level expert group defined seven key requirements for Trustworthy AI.,Search,2020,2,"Mohit Kumar Ahuja, Mohamed-Bachir  Belaid, Pierre  Bernab'e, Mathieu  Collet, Arnaud  Gotlieb, Chhagan  Lal, Dusica  Marijan, Sagar  Sen, Aizaz  Sharif, Helge  Spieker",NeHuAI@ECAI,,,,https://semanticscholar.org/paper/3b0ff6bd000e9c615d024614343f2c1cf12bf124,,"Trustworthiness is a central requirement for the acceptance and success of human-centered artificial intelligence (AI). To deem an AI system as trustworthy, it is crucial to assess its behaviour and characteristics against a gold standard of Trustworthy AI, consisting of guidelines, requirements, or only expectations. While AI systems are highly complex, their implementations are still based on software. The software engineering community has a long-established toolbox for the assessment of software systems, especially in the context of software testing. In this paper, we argue for the application of software engineering and testing practices for the assessment of trustworthy AI. We make the connection between the seven key requirements as defined by the European Commission's AI high-level expert group and established procedures from software engineering and raise questions for future work.",,