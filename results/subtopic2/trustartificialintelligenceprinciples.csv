Paper title,Takeaway from abstract,Source,Year,Citations,Authors,Journal,Influential citations,DOI,DOI URL,Semantic Scholar URL,PDF,Abstract,Takeaway suggests yes/no,Study type
Trustworthy artificial intelligence,"Trustworthy AI bases on the idea that trust builds the foundation of societies, economies, and sustainable development.",Search,2021,21,"Scott  Thiebes, Sebastian  Lins, Ali  Sunyaev",Electron. Mark.,,10.1007/S12525-020-00441-4,https://doi.org/10.1007/S12525-020-00441-4,https://semanticscholar.org/paper/9da092d7c7674e96830f8d6713a9a4f8101f984c,https://link.springer.com/content/pdf/10.1007/s12525-020-00441-4.pdf,"Artificial intelligence (AI) brings forth many opportunities to contribute to the wellbeing of individuals and the advancement of economies and societies, but also a variety of novel ethical, legal, social, and technological challenges. Trustworthy AI (TAI) bases on the idea that trust builds the foundation of societies, economies, and sustainable development, and that individuals, organizations, and societies will therefore only ever be able to realize the full potential of AI, if trust can be established in its development, deployment, and use. With this article we aim to introduce the concept of TAI and its five foundational principles (1) beneficence, (2) non-maleficence, (3) autonomy, (4) justice, and (5) explicability. We further draw on these five principles to develop a data-driven research framework for TAI and demonstrate its utility by delineating fruitful avenues for future research, particularly with regard to the distributed ledger technology-based realization of TAI.",,
Trusted Artificial Intelligence: Technique Requirements and Best Practices,"Trusted AI should be based on ethical principles, which can combat information cocoons, algorithmic collusion, algorithmic bias and privacy issues.",Search,2021,,"Tao  Zhang, Yi  Qin, Qiang  Li",2021 International Conference on Cyberworlds (CW),,10.1109/CW52790.2021.00058,https://doi.org/10.1109/CW52790.2021.00058,https://semanticscholar.org/paper/b62d643afc5f3ee16ebf26bf33b1d100685cfe15,,"The development and adoption of artificial intelligence (AI) have brought numerous problems and potential threats, such as information cocoons, algorithmic collusion, algorithmic bias and privacy, issues. To combat these issues, governments all over the world, international organizations and giant tech companies all have taken actions and reached an agreement that AI should be trusted. However, there is no uniform definition on trusted AI. In the paper, we make a survey on related works on artificial intelligence principles and ethical guidelines. Then analyze the ethical foundations of trusted AI and give a definition on trusted AI. We propose specific requirements of trusted AI from a technical perspective and give explanations on these technique requirements. Finally, best practices are recommended to achieve trusted AI and promote the responsible use of AI.",,
"Designing Trust in Artificial Intelligence: A Comparative Study Among Specifications, Principles and Levels of Control",Levels of control are the most reliable option to design trust in Highly Automated Systems.,Search,2020,,"Fernando  Galdon, Ashley  Hall, Laura  Ferrarello",IHIET,,10.1007/978-3-030-44267-5_14,https://doi.org/10.1007/978-3-030-44267-5_14,https://semanticscholar.org/paper/f7a8c301ce98454efa9f0808892d26065606acec,https://researchonline.rca.ac.uk/4382/1/Galdon2020_Chapter_DesigningTrustInArtificialInte.pdf,"This paper presents a comparative study amongst the three main frameworks acknowledged for designing trust in AI; specifications, principles and the levels of control necessary to underpin trust in order to address the rising concerns of Highly Automated Systems (HAS). We will also address trust design in four case studies specifically designed to address the rising concerns of these systems in the area of health and wellbeing. Based on the results, levels of control emerge as at the most reliable option to design trust in Highly Automated Systems, as it provides a more structured focus than specifications and principles. However, principles enhance philosophical inquiry to frame the intended outcome and specifications provide a constructive space for product development. In this context, the authors recommend the integration of all the frameworks into a multi-dimensional cross-disciplinary framework to build and extend robustness throughout the entire interactive lifecycle in the development of future applications.",,
How to achieve trustworthy artificial intelligence for health,"The EU's guidance leaves room for local, contextualized discretion in the health sector.",Search,2020,15,"Kristine  Bærøe, Ainar  Miyata-Sturm, Edmund  Henden",Bulletin of the World Health Organization,,10.2471/BLT.19.237289,https://doi.org/10.2471/BLT.19.237289,https://semanticscholar.org/paper/0a3c92d6c4fa4670743fb13758916067e772a143,https://europepmc.org/articles/pmc7133476?pdf=render,"Abstract Artificial intelligence holds great promise in terms of beneficial, accurate and effective preventive and curative interventions. At the same time, there is also awareness of potential risks and harm that may be caused by unregulated developments of artificial intelligence. Guiding principles are being developed around the world to foster trustworthy development and application of artificial intelligence systems. These guidelines can support developers and governing authorities when making decisions about the use of artificial intelligence. The High-Level Expert Group on Artificial Intelligence set up by the European Commission launched the report Ethical guidelines for trustworthy artificial intelligence in2019. The report aims to contribute to reflections and the discussion on the ethics of artificial intelligence technologies also beyond the countries of the European Union (EU). In this paper, we use the global health sector as a case and argue that the EU’s guidance leaves too much room for local, contextualized discretion for it to foster trustworthy artificial intelligence globally. We point to the urgency of shared globalized efforts to safeguard against the potential harms of artificial intelligence technologies in health care.",,
Requirements for Trustworthy Artificial Intelligence - A Review,"The field of algorithmic decision-making, particularly Artificial Intelligence (AI), has been drastically changing.",Search,2020,8,"Davinder  Kaur, Suleyman  Uslu, Arjan  Durresi",NBiS,,10.1007/978-3-030-57811-4_11,https://doi.org/10.1007/978-3-030-57811-4_11,https://semanticscholar.org/paper/ab7f6628cbbafae1ce994929af2482efbd092d61,https://scholarworks.iupui.edu/bitstream/1805/28055/1/Kaur2020Requirements-AAM.pdf,"The field of algorithmic decision-making, particularly Artificial Intelligence (AI), has been drastically changing. With the availability of a massive amount of data and an increase in the processing power, AI systems have been used in a vast number of high-stake applications. So, it becomes vital to make these systems reliable and trustworthy. Different approaches have been proposed to make theses systems trustworthy. In this paper, we have reviewed these approaches and summarized them based on the principles proposed by the European Union for trustworthy AI. This review provides an overview of different principles that are important to make AI trustworthy.",,Review
Trust in Artificial Intelligence: Australian Insights,Artificial Intelligence has the potential to positively impact many aspects of Australian society and business.,Search,2020,5,"Steve  Lockey, Nicole  Gillespie, Caitlin  Curtis",,,10.14264/b32f129,https://doi.org/10.14264/b32f129,https://semanticscholar.org/paper/6c2714f81bc323851052920d240bc21b98e2a9f8,https://espace.library.uq.edu.au/view/UQ:b32f129/LockeyGillespieCurtis_Trust_In_AI.pdf,"Artificial Intelligence (AI) is the cornerstone technology of the Fourth Industrial Revolution and is enabling rapid innovation with many potential benefits for Australian society (e.g. enhanced healthcare diagnostics, transportation optimisation) and business (e.g. enhanced efficiency and competitiveness). The COVID-19 pandemic has accelerated the uptake of advanced technology, and investment in AI continues to grow exponentially.AI also poses considerable risks and challenges to society which raises concerns about whether AI systems are worthy of trust. These concerns have been fuelled by high profile cases of AI use that were biased, discriminatory, manipulative, unlawful, or violated privacy or other human rights. Without public confidence that AI is being developed and used in an ethical and trustworthy manner, it will not be trusted and its full potential will not be realised. To echo the sentiment of Dr Alan Finkel AO, Australia’s Chief Scientist, acceptance of AI rests on “the essential foundation of trust”. Are we capable of extending our trust to AI? This national survey is the first to take a deep dive into answering this question and understanding community trust and expectations in relation to AI. To do this, we surveyed a nationally representative sample of over 2,500 Australian citizens in June to July 2020. Our findings provide important and timely research insights into the public’s trust and attitudes towards AI and lay out a pathway for strengthening trust and acceptance of AI systems.Key findings include:              - Trust is central to the acceptance of AI, and is influenced by four key drivers;              - Australians have low trust in AI systems but generally ‘accept’ or ‘tolerate’ AI;              - Australians expect AI to be regulated and carefully managed;              - Australians expect organisations to uphold the principles of trustworthy AI;              - Australians feel comfortable with some but not all uses of AI at work;              - Australians want to know more about AI but currently have low awareness and understanding of AI and its uses.We draw out the implications of the findings for government, business and NGOs and provide a roadmap to enhancing public trust in AI highlighting three key actions:              - Live up to Australian’s expectations of trustworthy AI              - Strengthen the regulatory framework for governing AI              - Strengthen Australia’s AI literacy",,
Trust in Artificial Intelligence: What do we know and why is it important?,There is a need for evidence-based research on trust in AI in order to guide policy and practice.,Search,2020,,Steve  Lockey,,,10.37421/JTSM.2020.9.207,https://doi.org/10.37421/JTSM.2020.9.207,https://semanticscholar.org/paper/1a2bf8b563290014a4375cee463f4bd4f34bfa45,,"The rise of Artificial Intelligence (AI) in our society is becoming ubiquitous and undoubtedly holds much promise. However, AI has also been implicated in high profile breaches of trust or ethical standards, and concerns have been raised over the use of AI in initiatives and technologies that could be inimical to society. Public trust and perceptions of AI trustworthiness underpin AI systems’ social licence to operate, and a myriad of company, industry, governmental and intergovernmental reports have set out principles for ethical and trustworthy AI. To guide the responsible stewardship of AI into our society, a firm foundation of research on trust in AI to enable evidence-based policy and practice is required. However, in order to inform and guide future research, it is imperative to first take stock and understand what is already know about human trust in AI. As such, we undertake a review of 100 papers examining the relationship between trust and AI. We found a fragmented, disjointed and siloed literature with an empirical emphasis on experimentation and surveys relating to specific AI technologies. While findings suggest some convergence on the importance of explainability as a determinant of trust in AI technologies, there are still gaps between conceptual arguments and what has been examined empirically. We urge future research to take a more holistic approach and investigate how trust in different referents impacts on attitudinal and behavioural intentions. Doing so will facilitate a more nuanced understanding of what it means to develop trustworthy AI.",,Review
Trust in Artificial Intelligence,"Artificial intelligence is being used for scheduling appointments, powering smart homes, recognizing people’s faces in photos, making health diagnoses, and several other things.",Search,2018,14,Arathi  Sethumadhavan,Ergonomics in Design: The Quarterly of Human Factors Applications,,10.1177/1064804618818592,https://doi.org/10.1177/1064804618818592,https://semanticscholar.org/paper/94a43052c729c8e7e3f28c50c76e46d09ec95f62,https://journals.sagepub.com/doi/pdf/10.1177/1064804618818592,"Trust is fundamental to creating a lasting relationship with another human being. In our daily lives, we encounter several situations where we place trust on other humans such as bus drivers, colleagues, and even strangers. Trust in human-human teams is initially founded on the predictability of the trustee, and as the relationship between the trustor and the trustee progresses, dependability or integrity replaces predictability as the basis of trust (Hoff & Bashir, 2015). As artificial intelligence (AI) gets smarter and smarter, it is becoming an integral part of human lives. For example, AI is being used for scheduling appointments, powering smart homes, recognizing people’s faces in photos, making health diagnoses, providing investment advice, and several other things. As with a humanhuman team, trust is a necessary ingredient for human-AI partnerships. However, trust in human-human teams progresses in the reverse order from human-human teams (e.g., Hoff & Bashir, 2015). This is because humans initially assume that AI is near perfect. Therefore, in the initial stages, faith forms the essential constituent of trust, and as the number of exchanges between the human and the AI increases, faith is replaced by dependability and predictability. Trust in AI is considered a twodimensional construct comprising trust and distrust, where trust is associated with feelings of calmness and security and distrust involves fear and worry (Lyons, Stokes, Eschleman, Alarcon, & Barelka, 2011). Unarguably, trust is a complex social process with a variety of factors determining the extent to which humans trust AI agents (Hoff & Bashir, 2015; Lee & See, 2004). Specifically, in order to calibrate the right level of trust in AI, consider dispositional (i.e., user characteristics such as age, culture, gender, and personality), internal (e.g., user characteristics such as workload, mood, self-confidence, and working memory capacity), environmental (e.g., task difficulty, perceived risks and benefits, organizational setting), and learned factors (e.g., reputation of the AI, reliability, consistency, type and timing of errors made by the AI, and users’ experience with similar agents). Design factors (e.g., appearance, ease of use, communication style, and transparency of the AI) also affect perceptions of trust. For example, anthrophomorphizing is an effective way of establishing a long-term social bond between humans and AI agents, which is driven by the neurotransmitter and hormone, oxytocin (e.g., de Visser et al., 2017). Further, anthropomorphic agents also resist breakdown in trust compared to their counterpart non-anthropomorphic agents, presumably because anthropomorphic agents remind users of humans who are forgiven more easily for being imperfect in comparison to machine-like agents. Interestingly, there is also evidence that humans tend to disclose more information to AI therapists than human therapists. Transparency of the AI also helps to calibrate the right level of trust by enabling users to develop accurate mental models of the AI underpinnings (e.g., Balfe, Sharples, & Wilson, 2018). 818592 ERGXXX10.1177/1064804618818592ergonomics in designergonomics in design research-article2018",,
Trustworthy AI: From Principles to Practices,"AI practitioners need to develop trustworthy AI systems that are robust, generalizable, explainable, transparent, reproducible, fair, private, and aligned with human values.",Search,2021,1,"Bo  Li, Peng  Qi, Bo  Liu, Shuai  Di, Jingen  Liu, Jiquan  Pei, Jinfeng  Yi, Bowen  Zhou",ArXiv,,,,https://semanticscholar.org/paper/c3689493757f90267908e776aeada9194fce55c7,,"Fast developing artificial intelligence (AI) technology has enabled various applied systems deployed in the real world, impacting people’s everyday lives. However, many current AI systems were found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection, etc., which not only degrades user experience but erodes the society’s trust in all AI systems. In this review, we strive to provide AI practitioners a comprehensive guide towards building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, alignment with human values, and accountability. We then survey leading approaches in these aspects in the industry. To unify the current fragmented approaches towards trustworthy AI, we propose a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items to practitioners and societal stakeholders (e.g., researchers and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges in the future development of trustworthy AI systems, where we identify the need for paradigm shift towards comprehensive trustworthy AI systems.",,Review
The relationship between trust in AI and trustworthy machine learning technologies,Trust can be impacted throughout the life cycle of AI-based systems.,Search,2020,50,"Ehsan  Toreini, Mhairi  Aitken, Kovila  Coopamootoo, Karen  Elliott, Carlos Gonzalez Zelaya, Aad van Moorsel",FAT*,,10.1145/3351095.3372834,https://doi.org/10.1145/3351095.3372834,https://semanticscholar.org/paper/bd4cf5a6987ef0f39b7e4e88d59b3a3365abcc70,http://arxiv.org/pdf/1912.00782,"To design and develop AI-based systems that users and the larger public can justifiably trust, one needs to understand how machine learning technologies impact trust. To guide the design and implementation of trusted AI-based systems, this paper provides a systematic approach to relate considerations about trust from the social sciences to trustworthiness technologies proposed for AI-based services and products. We start from the ABI+ (Ability, Benevolence, Integrity, Predictability) framework augmented with a recently proposed mapping of ABI+ on qualities of technologies that support trust. We consider four categories of trustworthiness technologies for machine learning, namely these for Fairness, Explainability, Auditability and Safety (FEAS) and discuss if and how these support the required qualities. Moreover, trust can be impacted throughout the life cycle of AI-based systems, and we therefore introduce the concept of Chain of Trust to discuss trustworthiness technologies in all stages of the life cycle. In so doing we establish the ways in which machine learning technologies support trusted AI-based systems. Finally, FEAS has obvious relations with known frameworks and therefore we relate FEAS to a variety of international 'principled AI' policy and technology frameworks that have emerged in recent years.",,
Trust in Distributed Artificial Intelligence,Trust allows interactions between agents where there may have been no effective interaction possible before trust.,Search,1992,90,Stephen  Marsh,MAAMAW,,10.1007/3-540-58266-5_6,https://doi.org/10.1007/3-540-58266-5_6,https://semanticscholar.org/paper/6d296cd0ddaccb6d01aa197c9ba5a04afee2d399,,"A discussion of trust is presented which focuses on multiagent systems, from the point of view of one agent in a system. The roles trust plays in various forms of interaction are considered, with the view that trust allows interactions between agents where there may have been no effective interaction possible before trust. Trust allows parties to acknowledge that, whilst there is a risk in relationships with potentially malevolent agents, some form of interaction may produce benefits, where no interaction at all may not. In addition, accepting the risk allows the trusting agent to prepare itself for possibly irresponsible or untrustworthy behaviour, thus minimizing the potential damage caused. A formalism is introduced to clarify these notions, and to permit computer simulations. An important contribution of this work is that the formalism is not allen-compassing: there are some notions of trust that are excluded. What it describes is a specific view of trust.",,
Trust in Artificial Intelligence: Meta-Analytic Findings.,"There are several factors that influence trust in AI, including reliability and anthropomorphism.",Search,2021,1,"Alexandra D Kaplan, Theresa T Kessler, J Christopher Brill, P A Hancock",Human factors,,10.1177/00187208211013988,https://doi.org/10.1177/00187208211013988,https://semanticscholar.org/paper/4ad12e5c49202187f2a85351d417f81dc86b8524,,"OBJECTIVE

The present meta-analysis sought to determine significant factors that predict trust in artificial intelligence (AI). Such factors were divided into those relating to (a) the human trustor, (b) the AI trustee, and (c) the shared context of their interaction.

BACKGROUND

There are many factors influencing trust in robots, automation, and technology in general, and there have been several meta-analytic attempts to understand the antecedents of trust in these areas. However, no targeted meta-analysis has been performed examining the antecedents of trust in AI.

METHOD

Data from 65 articles examined the three predicted categories, as well as the subcategories of human characteristics and abilities, AI performance and attributes, and contextual tasking. Lastly, four common uses for AI (i.e., chatbots, robots, automated vehicles, and nonembodied, plain algorithms) were examined as further potential moderating factors.

RESULTS

Results showed that all of the examined categories were significant predictors of trust in AI as well as many individual antecedents such as AI reliability and anthropomorphism, among many others.

CONCLUSION

Overall, the results of this meta-analysis determined several factors that influence trust, including some that have no bearing on AI performance. Additionally, we highlight the areas where there is currently no empirical research.

APPLICATION

Findings from this analysis will allow designers to build systems that elicit higher or lower levels of trust, as they require.",,Meta-Analysis
"Artificial Intelligence, Trust, and Perceptions of Agency","The benevolence of the AI increases if the AI is seen as more agentic, but so does the anticipated psychological cost if it violates the trust.",Search,2021,,"Phanish  Puranam, Bart  Vanneste",,,10.2139/ssrn.3897704,https://doi.org/10.2139/ssrn.3897704,https://semanticscholar.org/paper/f0eb55a172546ca542767068ced5532af0e23524,,"Extant theories of trust assume the trustee has agency (i.e. intentionality and free will). We propose that a crucial qualitative distinction between placing trust in Artificial Intelligence (AI) vs. trust in a human lies in the degree to which attributions of agency are made to the trustee by the trustor (human). We specify two mechanisms through which the extent of agency attributions can affect human trust in AI. First, the importance of the benevolence of the trustee—the AI—increases if the AI is seen as more agentic, but so does the anticipated psychological cost if it violates the trust (because of betrayal aversion, see Bohnet & Zeckhauser, 2004). Second, attributions of benevolence and competence become less relevant for placing confidence in a non-agentic seeming AI system, and instead benevolence and competence attributions to the designer of the system become important. Both mechanisms imply that making an AI appear more agentic may increase or decrease the trust that humans place in it. While designers of AI technology often strive to endow their creations with features that convey its benevolent nature (e.g. through anthropomorphizing or transparency), this may also change agency perceptions in a manner that results in making it less trustworthy in human eyes.",,
Attachment and trust in artificial intelligence,People with a secure attachment style have a greater trust in artificial intelligence.,Search,2021,21,"Omri  Gillath, Ting  Ai, Michael  Branicky, Shawn  Keshmiri, Rob  Davison, Ryan  Spaulding",Comput. Hum. Behav.,,10.1016/j.chb.2020.106607,https://doi.org/10.1016/j.chb.2020.106607,https://semanticscholar.org/paper/d7e73699049a73cf90c82a7b5aef589d84dbe794,,"Abstract Lack of trust is one of the main obstacles standing in the way of taking full advantage of the benefits artificial intelligence (AI) has to offer. Most research on trust in AI focuses on cognitive ways to boost trust. Here, instead, we focus on boosting trust in AI via affective means. Specifically, we tested and found associations between one's attachment style—an individual difference representing the way people feel, think, and behave in relationships—and trust in AI. In Study 1 we found that attachment anxiety predicted less trust. In Study 2, we found that enhancing attachment anxiety reduced trust, whereas enhancing attachment security increased trust in AI. In Study 3, we found that exposure to attachment security cues (but not positive affect cues) resulted in increased trust as compared with exposure to neutral cues. Overall, our findings demonstrate an association between attachment security and trust in AI, and support the ability to increase trust in AI via attachment security priming.",,
Transparency and trust in artificial intelligence systems,Transparency can have a negative impact on trust in machine learning-based decision support tools.,Search,2020,20,"Philipp  Schmidt, Felix  Bießmann, Timm  Teubner",J. Decis. Syst.,,10.1080/12460125.2020.1819094,https://doi.org/10.1080/12460125.2020.1819094,https://semanticscholar.org/paper/7a19f30e02c34c4eb7b197d3bcd4fbcb8a4e1602,,"ABSTRACT Assistive technology featuring artificial intelligence (AI) to support human decision-making has become ubiquitous. Assistive AI achieves accuracy comparable to or even surpassing that of human experts. However, often the adoption of assistive AI systems is limited by a lack of trust of humans into an AI’s prediction. This is why the AI research community has been focusing on rendering AI decisions more transparent by providing explanations of an AIs decision. To what extent these explanations really help to foster trust into an AI system remains an open question. In this paper, we report the results of a behavioural experiment in which subjects were able to draw on the support of an ML-based decision support tool for text classification. We experimentally varied the information subjects received and show that transparency can actually have a negative impact on trust. We discuss implications for decision makers employing assistive AI technology.",,
The challenges and opportunities of artificial intelligence in implementing trustworthy robotics and autonomous systems,Artificial intelligence will benefit from being trustworthy in robotics and autonomous systems.,Search,2020,3,"Hongmei  He, J.  Gray, Angelo  Cangelosi, Q.  Meng, T. M. McGinnity, J.  Mehnen",,,,,https://semanticscholar.org/paper/3e130750bc9d95e12b4194b67137d89da0f87a07,,"Effective Robots and Autonomous Systems (RAS) must be trustworthy. Trust is essential in designing autonomous and semi-autonomous technologies, because “No trust, no use”. RAS should provide high quality of services, with the four key properties that make it trust, i.e. they must be (i) robust for any health issues, (ii) safe for any matters in their surrounding environments, (iii) secure for any threats from cyber spaces, and (iv) trusted for human-machine interaction. We have thoroughly analysed the challenges in implementing the trustworthy RAS in respects of the four properties, and addressed the power of AI in improving the trustworthiness of RAS. While we put our eyes on the beneﬁts that AI brings to human, we should realise the potential risks that could be caused by AI. The new concept of human-centred AI will be the core in implementing the trustworthy RAS. This review could provide a brief reference for the research on AI for trustworthy RAS.",,Review